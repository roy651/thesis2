{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# RUN Main import block and TODO list\n",
    "\n",
    "# TODO: see how uri calculated the ridges\n",
    "\n",
    "# TODO: Perform Histogram equalization - start with it\n",
    "# TODO: \n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph \n",
    "# This will be the peakness measure for the PSD ==> The desired ridge index\n",
    "# TODO:\n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph - it's the peakness measure for the PSD\n",
    "# must select a peak above a min threshold in order to ignore noisy frequency\n",
    "# must ignore peaks above a certain threshold in order to detect meaningful frequency\n",
    "# run the PSD in moving windows every 200 px (deduced from the below PSD pointing to a freq of 1/0.02=50-> times 4= 200px)\n",
    "# and medianf the result of the windows\n",
    "# TODO:\n",
    "# Another alternative: (with Yariv)\n",
    "# Run PSD column by column - get the phase, freq, peakness and reconstruct an artificial ridge slice\n",
    "# from this - reconstruct a \"clean\" artificial ridge image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "#import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import mahotas as mh\n",
    "from mahotas import polygon\n",
    "# import pymorph as pm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "import skimage.transform as transform\n",
    "import skimage.morphology as mp\n",
    "import skimage.io as sio\n",
    "import scipy.misc as sm\n",
    "from skimage.filters import threshold_otsu, threshold_adaptive\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "from skimage import exposure\n",
    "from skimage import data, img_as_float\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from bisect import bisect_left\n",
    "import math\n",
    "import warnings\n",
    "import csv\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0,
     23,
     27,
     30,
     48,
     56,
     70,
     76,
     82,
     96,
     105,
     120,
     138,
     155,
     173,
     253,
     259,
     264,
     269,
     276,
     281,
     286,
     293,
     315,
     322,
     331,
     368,
     583,
     602,
     606,
     621
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN Utility functions\n",
    "\n",
    "# One time init\n",
    "# with open('results.csv', 'w') as csvfile:\n",
    "#     csvout = csv.writer(csvfile)\n",
    "#     csvout.writerow([\"File\", \"Model\", \"Gap\", \"Slice_size\", \"Count\", \"Precision\", \"Recall\", \"F-score\", \"True Count\", \"Error Rate\"])\n",
    "\n",
    "#BASIC CROP FRAME\n",
    "X_START = 1000\n",
    "X_END = 6000\n",
    "Y_START = 800\n",
    "Y_END = 4300\n",
    "BG_2_OBJ_RATIO = 0.91\n",
    "CUBE_SIZE = 250\n",
    "EDGE_GAP = 50\n",
    "ROOT_FOLDER = \"/home/il239838/files/\"\n",
    "# ROOT_FOLDER = \"/Users/il239838/Downloads/private/Thesis/Papyrus/PX303/files/\"\n",
    "LEARNING_RATE = 0.001\n",
    "BATCHES = 800\n",
    "BATCH_SIZE = 50\n",
    "BREAK_VAL = 1000\n",
    "\n",
    "# Simple crop by x/y ranges\n",
    "def crop(image, ymin, ymax, xmin, xmax):\n",
    "    return image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "# returns a logical matrix of values beyond a threshld\n",
    "def thresholded(image, val): \n",
    "    return np.logical_and(*[image[...] > val  for t in enumerate([0, 0])])\n",
    "\n",
    "def find_min_max_without_orphand_pixels(nonzero_dimension, crop_filter=20):\n",
    "    sorted = np.sort(nonzero_dimension)\n",
    "    prev=-1\n",
    "    min_val = sorted[0]\n",
    "    for i, x in enumerate(sorted[:100]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            min_val = x\n",
    "        prev = x\n",
    "    prev=-1\n",
    "    max_val = sorted[-1]\n",
    "    for i, x in enumerate(sorted[-100:]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            max_val = prev\n",
    "            break\n",
    "        prev = x\n",
    "    \n",
    "    return min_val, max_val\n",
    "\n",
    "def calc_min_max_coordinates(image, crop_val=50):\n",
    "    temp = thresholded(image, crop_val)\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "def calc_min_max_coordinates_dynamic(image, cutoff=1):\n",
    "    temp = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    flat = np.sort(np.matrix.getA1(temp))\n",
    "    sum_all = np.sum(flat)\n",
    "    index = np.argmin(flat.cumsum() < (sum_all * cutoff))\n",
    "\n",
    "    temp = thresholded(temp, flat[index])\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "# initial static crop and a seondary dynamic crop based on signal2noise ratio\n",
    "def crop_full_scan(image, x_start, x_end, y_start, y_end):\n",
    "    temp = crop(image, y_start, y_end, x_start, x_end)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(temp, cutoff=BG_2_OBJ_RATIO)\n",
    "    temp = crop(image, y_start+ymin, y_start+ymax, x_start+xmin, x_start+xmax)\n",
    "    return temp\n",
    "\n",
    "def crop_thresholded(image):\n",
    "    temp = crop(image, 0, image.shape[0]-1, 0, image.shape[1]-1)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates(temp)\n",
    "    temp = crop(image, ymin, ymax, xmin, xmax)\n",
    "    return temp\n",
    "\n",
    "def read_and_crop(image_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END):\n",
    "    if \"il239838\" in os.getcwd():\n",
    "        image = img.imread(ROOT_FOLDER + image_name)\n",
    "    else:\n",
    "        f = urllib.request.urlopen(\"https://dl.dropboxusercontent.com/s/31b96942qdcn73k/\" + image_name)\n",
    "        image = img.imread(f, format='jpeg')\n",
    "\n",
    "    # Smart-crop the image to get rid of all the noise and redundant area\n",
    "    # return crop_full_scan(image)\n",
    "    cropped = crop_full_scan(image, x_start, x_end, y_start, y_end)\n",
    "    return exposure.equalize_adapthist(cropped, clip_limit=0.03)\n",
    "\n",
    "\n",
    "# TODO: fix performance!!! http://scikit-image.org/docs/dev/user_guide/tutorial_parallelization.html\n",
    "def combine_3_images_to_RGB(red, green, blue):\n",
    "    new_image = np.empty((blue.shape[0],blue.shape[1],3))\n",
    "    for x in range(0, blue.shape[0]):\n",
    "        for y in range(0, blue.shape[1]):\n",
    "            new_image[x,y,0] = red[x,y]\n",
    "            new_image[x,y,1] = green[x,y]\n",
    "            new_image[x,y,2] = blue[x,y]\n",
    "    return new_image\n",
    "\n",
    "def slice_image_left_edge(original, width=200, rotate=0):\n",
    "    rot = ndimage.rotate(original, rotate)\n",
    "    # Slice the left slice of the so-called \"blue\" image\n",
    "    left_edge_orig = crop(rot, 1, 1400, 1, width)\n",
    "    left_edge_orig = crop_thresholded(left_edge_orig)\n",
    "\n",
    "    # Copy to a new array so we don't thrash the origin\n",
    "    left_edge = np.empty_like (left_edge_orig)\n",
    "    np.copyto(left_edge, left_edge_orig)\n",
    "\n",
    "    # Zero down low level \"noise\" values\n",
    "    low_values_indices = left_edge < 30  # Where values are low\n",
    "    left_edge[low_values_indices] = 0  # All low values set to 0\n",
    "    return left_edge\n",
    "\n",
    "def get_best_angle_rotation(original, crop=True, width=200):\n",
    "    min_var = 99999999999\n",
    "    best_angle = -10\n",
    "    for x in range(-5,5):\n",
    "        if crop:            \n",
    "            rot_edge = slice_image_left_edge(original, width, x)\n",
    "        else:\n",
    "            rot_edge = ndimage.rotate(original, x)\n",
    "        left_var = np.var(rot_edge, axis=1)\n",
    "        # left_var = np.apply_along_axis(lambda v: np.var(v[np.nonzero(v)]), 1, rot_edge)\n",
    "        var_sum = np.sum(left_var)\n",
    "        if (var_sum < min_var):\n",
    "            min_var = var_sum\n",
    "            best_angle = x\n",
    "    print (\"best_angle=\"+str(best_angle))\n",
    "    return best_angle\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "def calc_neighbors(slice_map, col, row):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if ((col-1, row) in slice_map and slice_map[(col-1, row)] != None):\n",
    "        slice_map[(col, row)][\"left\"] = slice_map[(col-1, row)]\n",
    "        slice_map[(col-1, row)][\"right\"] = slice_map[(col, row)]\n",
    "    if ((col+1, row) in slice_map and slice_map[(col+1, row)] != None):\n",
    "        slice_map[(col, row)][\"right\"] = slice_map[(col+1, row)]\n",
    "        slice_map[(col+1, row)][\"left\"] = slice_map[(col, row)]\n",
    "    if ((col, row-1) in slice_map and slice_map[(col, row-1)] != None):\n",
    "        slice_map[(col, row)][\"top\"] = slice_map[(col, row-1)]\n",
    "        slice_map[(col, row-1)][\"bottom\"] = slice_map[(col, row)]\n",
    "    if ((col, row+1) in slice_map and slice_map[(col, row+1)] != None):\n",
    "        slice_map[(col, row)][\"bottom\"] = slice_map[(col, row+1)]\n",
    "        slice_map[(col, row+1)][\"top\"] = slice_map[(col, row)]\n",
    "    \n",
    "\n",
    "\n",
    "def VAL_create_cube(name, raw, x, y):\n",
    "    cube = {}\n",
    "    cube[\"cube\"] = raw\n",
    "    cube[\"file\"] = name\n",
    "    #     if name.find('P') == 0:\n",
    "    #         cube[\"index\"] = int(name[name.find('P')+1:name.find('P')+4]) * 1000 + int(name[name.find('Fg')+2:name.find('Fg')+5])\n",
    "    #     else:\n",
    "    # print(\"Found a ZERO index cube with the name:\"+name)\n",
    "    cube[\"index\"] = 0\n",
    "    cube[\"top_row\"] = x\n",
    "    cube[\"left_col\"] = y\n",
    "    cube[\"right_col\"] = y + CUBE_SIZE\n",
    "    return cube\n",
    "    \n",
    "\n",
    "ZERO_CUBE = VAL_create_cube(\"ZERO\", np.zeros((CUBE_SIZE, CUBE_SIZE), dtype=np.int), -1, -2)\n",
    "\n",
    "# slice an image to cubes with 250X250 pixel size\n",
    "def VAL_slice_TEAR_to_static_slices(name, cropped_original):\n",
    "    structure = {}\n",
    "    # cropped_original = cropped_original / 256 # divide by 256 to \"normalize\" between 0 and 1\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    x, y = cropped_original[\"cut\"].shape\n",
    "    # print (x,y)\n",
    "    n = 0\n",
    "    # every 250 pixels on the x axis == rows\n",
    "    while ((n + 1) * CUBE_SIZE < x):\n",
    "        # mark the piece as narrow so the first would be counted as lastt too\n",
    "        narrow = True if ((CUBE_SIZE + (4 * EDGE_GAP)) > y) else False\n",
    "        # cut a cube of 250X250 at the FIRST column\n",
    "        cube = (crop(cropped_original[\"cut\"], n * CUBE_SIZE, (n + 1) * CUBE_SIZE, EDGE_GAP, CUBE_SIZE + EDGE_GAP))\n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, n * CUBE_SIZE, EDGE_GAP)\n",
    "            new_cube[\"col\"] = 0 # marks that the cube is on the first col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = narrow # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + CUBE_SIZE + EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + n * CUBE_SIZE\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + (n + 1) * CUBE_SIZE\n",
    "            structure[(0, n)] = new_cube\n",
    "\n",
    "        # cut a cube of 250X250 at the LAST column\n",
    "        cube = (crop(cropped_original[\"cut\"], n * CUBE_SIZE, (n + 1) * CUBE_SIZE, y - CUBE_SIZE - EDGE_GAP, y - EDGE_GAP))\n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        # aligned with the normalization 0.2 correlates to 50\n",
    "        if np.median(cube) > 0.2:\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, n * CUBE_SIZE, y - CUBE_SIZE - EDGE_GAP)\n",
    "            new_cube[\"col\"] = 1 # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = not narrow # like col - marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + y - CUBE_SIZE - EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + y - EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + n * CUBE_SIZE\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + (n + 1) * CUBE_SIZE\n",
    "            structure[(1, n)] = new_cube\n",
    "\n",
    "    #         m = 0\n",
    "    #         # every 250 pixels on the y axis == cols\n",
    "    #         while ((m + 1) * CUBE_SIZE < y):            \n",
    "    #             if ((m == 0) or ((m + 2) * CUBE_SIZE >= y)): # Only keep the left and right edges of the piece for matching!!\n",
    "    #                 # cut a cube of 250X250\n",
    "    #                 cube = crop(cropped_original[\"cut\"], n * CUBE_SIZE, (n + 1) * CUBE_SIZE, m * CUBE_SIZE, (m + 1) * CUBE_SIZE)\n",
    "    #                 # keep only cubes for which half of the pixels have some \"color\"\n",
    "    #                 # print(np.median(cube))\n",
    "    #                 if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "    #                     # keep the cube\n",
    "    #                     new_cube = VAL_create_cube(name, cube, n * CUBE_SIZE, m * CUBE_SIZE)\n",
    "    #                     new_cube[\"col\"] = m\n",
    "    #                     new_cube[\"row\"] = n\n",
    "    #                     new_cube[\"orig\"] = cropped_original\n",
    "    #                     new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + m * CUBE_SIZE\n",
    "    #                     new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + (m + 1) * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + n * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + (n + 1) * CUBE_SIZE\n",
    "    #                     if ((m + 2) * CUBE_SIZE >= y):\n",
    "    #                         new_cube[\"last\"] = True\n",
    "    #                     else:\n",
    "    #                         new_cube[\"last\"] = False\n",
    "    #                     structure[(m, n)] = new_cube\n",
    "    #             m += 1\n",
    "        n += 1\n",
    "            \n",
    "    # this loop has to be performed only after we've established all the None cubes\n",
    "    for cube in structure.values():\n",
    "        # set the reference to neighbor cubes\n",
    "        if cube != None:\n",
    "            calc_neighbors(structure, cube[\"col\"], cube[\"row\"])\n",
    "\n",
    "    # return the data structure with all the cubes and the counters of the rows and columns\n",
    "    return structure.values()\n",
    "\n",
    "def pad_above(original, above, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount), above[\"cube\"][-amount:], axis=0)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE,CUBE_SIZE+amount), axis=0)\n",
    "    return VAL_create_cube(original[\"file\"], res, original[\"top_row\"] - amount, original[\"left_col\"])\n",
    "  \n",
    "\n",
    "def pad_below(original, below, amount):\n",
    "    res = np.insert(original[\"cube\"], np.full(amount, CUBE_SIZE), below[\"cube\"][:amount], axis=0)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=0)\n",
    "    return VAL_create_cube(original[\"file\"], res, original[\"top_row\"] + amount, original[\"left_col\"])\n",
    "\n",
    "def pad_left(original, left, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount, dtype=int), left[\"cube\"][:,-amount:], axis=1)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE, CUBE_SIZE+amount), axis=1)\n",
    "    return VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] - amount)\n",
    "\n",
    "def pad_right(original, right, amount):\n",
    "    res = np.insert(original[\"cube\"], [CUBE_SIZE], right[\"cube\"][:,:amount], axis=1)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=1)\n",
    "    return VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] + amount)\n",
    "    \n",
    "\n",
    "# \"Shave\" the right edge of the cube with <gap> pixels and pad with zeros on the left\n",
    "def shave_right(original, amount):\n",
    "    return pad_left(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# \"Shave\" the left edge of the cube with <gap> pixels and pad with zeros on the right    \n",
    "def shave_left(original, amount):\n",
    "    return pad_right(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# concatenate cubes \n",
    "def concatenate_cubes(left, right, slice_size):\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right[\"cube\"][:,:slice_size]), axis=1)\n",
    "    x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "    return con, x_delta, y_delta\n",
    "\n",
    "# concatenate cubes \n",
    "def VAL_concatenate_cubes(left, right, slice_size):\n",
    "    right_img = right[\"cube\"]\n",
    "    # next block is not relevant for training ...\n",
    "    #     # if the left cube is matched to another left cube (or right cube to another right cube) then rotate the right\n",
    "    #     # cube by 180 so we try to match it upside down, covering the option that the cube was pictured rotated\n",
    "    #     if ((left[\"col\"] == 0 and right[\"col\"] == 0) or (left[\"col\"] != 0 and right[\"col\"] != 0)):\n",
    "    #         right_img = np.rot90(right[\"cube\"], 2);\n",
    "\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right_img[:,:slice_size]), axis=1)\n",
    "\n",
    "    # next block calculates distance based on the distance between left's right-top corner and right's left-top corner    \n",
    "    #     x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    #     y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "\n",
    "    # next block calculates the distance between the centers of cubes, accounting for test set's possibility of reverse slices (left instead of right and vice versa)\n",
    "    x_delta = right[\"top_row\"] - left[\"top_row\"] # equivalent to distance between vertical centers\n",
    "    y_delta = (right[\"left_col\"] + (slice_size / 2)) - (left[\"right_col\"] - (slice_size / 2)) # measuring the distance between horizontal centers of the slices\n",
    "\n",
    "    return con, x_delta, y_delta, left[\"file\"], right[\"file\"]\n",
    "    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cube with zeros\n",
    "def concatenate_cubes_zero_pad_gaps(left_orig, right_orig, gap):\n",
    "    left = left_orig if gap == 0 else shave_right(left_orig, gap)\n",
    "    right = right_orig if gap == 0 else shave_left(right_orig, gap)\n",
    "    return concatenate_cubes(left, right)    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cobe with the nearby\n",
    "# continuation of the cubes\n",
    "def concatenate_cubes_with_gap(left_orig, right_orig, gap, left_pad, right_pad, slice_size):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    left = left_orig if gap == 0 else pad_left(left_orig, left_pad, gap)\n",
    "    right = right_orig if gap == 0 else pad_right(right_orig, right_pad, gap)\n",
    "    return concatenate_cubes(left, right, slice_size)        \n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def VAL_build_train_set_for_euclidean_distance(cubes, slice_size, folder):\n",
    "    # clean folder before starting\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    train_left_obj = []\n",
    "    train_right_obj = []\n",
    "    # iterate over all cubes   \n",
    "    for curr in cubes:\n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in cubes:\n",
    "            if (adj[\"file\"] != curr[\"file\"]): # no need to test against self CURRENTLY checking from directions!!!\n",
    "                #import pdb; pdb.set_trace()\n",
    "                # append the adjacent image to the current image\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "                output = folder+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                np.save(output, conc)\n",
    "                train_imgs.append(output)\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                train_left_obj.append(curr)\n",
    "                train_right_obj.append(adj)\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_x_delta, train_y_delta, train_left_obj, train_right_obj\n",
    "\n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def ORIG_build_train_set(cubes, gap):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    # iterate over the rows and cols, essentially going over the grid of sliced cubes\n",
    "    for row in range(0, rows):\n",
    "        for col in range(0, cols):\n",
    "            # if this cube exists (could have been removed previously due to lack of data)\n",
    "            if (cubes[(col, row)] != None):\n",
    "                # for each \"current\" image in the iteration\n",
    "                curr = cubes[(col, row)]\n",
    "                # iterate over all the cubes to find all the \"other\" (adjacent) cubes\n",
    "                for adj_row in range(0, rows):\n",
    "                    for adj_col in range(0, cols):\n",
    "                        if (adj_row != row or adj_col != col):\n",
    "                            if (cubes[(adj_col, adj_row)] != None):\n",
    "                                adj = cubes[(adj_col, adj_row)]\n",
    "                                # append the adjacent image to the current image\n",
    "                                # pass the filling cubes on the right and left to pad against the gap\n",
    "                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                    if (gap == 0):\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes(curr, adj, slice_size)\n",
    "                                    else:\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr, adj, gap, curr[\"left\"], adj[\"right\"], slice_size)\n",
    "                                    train_imgs.append(conc)\n",
    "                                    train_x_delta.append(x_delta)\n",
    "                                    train_y_delta.append(y_delta)\n",
    "                                    # if the adj image is on the same row and on the right of the curr image - it will be marked as match    \n",
    "                                    if (adj_row == row and adj_col == (col + 1)):\n",
    "                                        # mark the image as matched\n",
    "                                        train_lbls.append([0,1])\n",
    "                                        # need to enrich the set with a few more tru positive samples - so we offset \n",
    "                                        # the matched images up ad down a few times and create more matches\n",
    "                                        if (\"top\" in curr.keys() and \"top\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                                                adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"top\" in curr[\"left\"].keys() and \"top\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_above(curr[\"left\"], curr[\"left\"][\"top\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_above(adj[\"right\"], curr[\"right\"][\"top\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                                                adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"bottom\" in curr[\"left\"].keys() and \"bottom\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_below(curr[\"left\"], curr[\"left\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_below(adj[\"right\"], curr[\"right\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"left\" in curr.keys()): # enough to check only the curr as the left of the adj is the curr\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_left(curr, curr[\"left\"],i)\n",
    "                                                adj1 = pad_left(adj, adj[\"left\"],i) # essentially the curr\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_left(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_left(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"right\" in adj.keys()): # enough to check only the adj as the right of the curr is the adj\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_right(curr, curr[\"right\"],i) # essentially the adj\n",
    "                                                adj1 = pad_right(adj, adj[\"right\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_right(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_right(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                    else:\n",
    "                                        # mark the image as not matched\n",
    "                                        train_lbls.append([1,0])\n",
    "                                \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta\n",
    "\n",
    "# IMPORTANT: enrich_factor determines how many \"duplications\" of TRUE values will we have in the train set\n",
    "# This allows for a more balanced train set however, it reduces the strictness of the matches \n",
    "# i.e. (not sure why) when we have multiple nearby \"duplicates\" matches we get much more matches in the validation\n",
    "def NEW_build_train_set_for_binary_labeling(cubes, slice_size, folder, enrich_factor=1): # 1 means no enrich\n",
    "    # enrich_factor is split by 2 because it is dual-sided and 1 means actually no enrichment - i.e. 0.5\n",
    "    enrich_factor = enrich_factor / 2 \n",
    "    # clean folder before starting\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in cubes:\n",
    "            # 1 - not of the same fragment (file==fragment)\n",
    "            # 2 - they ARE of the same tear - don't want to confuse the learning with false data coming from different tears\n",
    "            # 3 - no need to test against self and avoid checking from both directions\n",
    "            if  adj[\"file\"] != curr[\"file\"] and \\\n",
    "                adj[\"tear\"] == curr[\"tear\"] and \\\n",
    "                curr[\"piece_col\"] < adj[\"piece_col\"]: \n",
    "                # last condition above - actually ignores pieces of the same col but different rows\n",
    "                # the assumption is that they are either \"not-match\" and then will tilt the balance further to not-match\n",
    "                # or they are \"somewhat-matching\" but in a way that might confuse the algorithm\n",
    "                \n",
    "                # print(\">>> >>>\"+curr[\"file\"])\n",
    "                # append the adjacent image to the current image\n",
    "                # import pdb; pdb.set_trace()\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                \n",
    "                # Condition for marking as match:\n",
    "                # 1 - the adj piece is on the same row as the curr\n",
    "                # 2 - the adj piece is just to the right of the curr\n",
    "                # 3 - the curr cube is on the right edge of the piece\n",
    "                # 4 - the adj cube is on the left edge of the piece\n",
    "                # import pdb; pdb.set_trace()\n",
    "                if  curr[\"piece_row\"] == adj[\"piece_row\"] and \\\n",
    "                    curr[\"piece_col\"] + 1 == adj[\"piece_col\"] and \\\n",
    "                    (curr[\"col\"] != 0 or curr[\"last\"]) and \\\n",
    "                    (adj[\"col\"] == 0 or not adj[\"last\"]): \n",
    "                    \n",
    "                    # mark the image as matched\n",
    "                    output = folder+\"1=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\">>> MATCH >>>\"+output)\n",
    "                    np.save(output, conc)\n",
    "                    # print(\">>> >>> >>> SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([0,1])\n",
    "\n",
    "                    # need to enrich the set with a few more true positive samples - so we offset \n",
    "                    # the matched images up and down a few times and create more matches\n",
    "                    if (\"top\" in curr.keys() and \"top\" in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                            adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            np.save(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    "                    if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                            adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            np.save(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    " \n",
    "                else:\n",
    "                    # mark the image as not matched\n",
    "                    output = folder+\"0=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\"<<< nonmatch <<<\"+output)\n",
    "                    np.save(output, conc)\n",
    "                    # print(\"<<< <<< <<< SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([1,0]) # not matched\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta\n",
    "\n",
    "\n",
    "def frame_to_n_by_m(orig, start_vector, end_vector, is_col):\n",
    "    max_val = np.amax(end_vector)\n",
    "    min_val = np.amin(start_vector)\n",
    "    width = max_val - min_val\n",
    "    if (is_col):\n",
    "        result = np.zeros((start_vector.size, width))\n",
    "    else:\n",
    "        result = np.zeros((width, start_vector.size))\n",
    "    \n",
    "    for i in range(0, start_vector.size):\n",
    "        if (is_col):\n",
    "            row_vec = orig[i, start_vector[i]:end_vector[i]]\n",
    "        else:\n",
    "            row_vec = orig[start_vector[i]:end_vector[i],i]\n",
    "        temp = np.lib.pad(row_vec, (start_vector[i]-min_val, max_val-end_vector[i]), 'constant', constant_values=(0.09, 0.09))\n",
    "        if (is_col):\n",
    "            if (result[i].size != width):\n",
    "                import pdb; pdb.set_trace()\n",
    "            result[i] = temp[0:width]\n",
    "        else:\n",
    "            result[:,i] = temp[0:width]\n",
    "    return min_val, result\n",
    "\n",
    "def rough_tear_line(orig, start_vector, cut_mean, is_col, chew_factor):\n",
    "    end_vector = np.empty(start_vector.size).astype(int)\n",
    "    if (is_col and np.absolute(cut_mean-orig.shape[1]) < 10):\n",
    "        end_vector.fill(orig.shape[1])\n",
    "    elif (not is_col and np.absolute(cut_mean-orig.shape[0]) < 10):\n",
    "        end_vector.fill(orig.shape[0])\n",
    "    else:\n",
    "        deviation_vector = np.random.normal(0, chew_factor, start_vector.size).astype(int)\n",
    "        end_vector[0] = cut_mean + deviation_vector[0]\n",
    "        for i in range(1, end_vector.size):\n",
    "            end_vector[i] = end_vector[i - 1] + deviation_vector[i]\n",
    "    \n",
    "    start_px, cut_piece = frame_to_n_by_m(orig, start_vector, end_vector, is_col)    \n",
    "    return start_px, cut_piece, end_vector\n",
    "\n",
    "def rough_tear_image(image, cols, rows):\n",
    "    pieces = []\n",
    "    col_width = int(image.shape[1] / cols)\n",
    "    row_height = int(image.shape[0] / rows)\n",
    "    # print(col_width, row_height)\n",
    "    next_col_start_vec = np.zeros((image.shape[0],), dtype=int)\n",
    "    for col_idx in range(0, cols):\n",
    "    #         import pdb; pdb.set_trace()\n",
    "        start_col_px, cut_column, next_col_start_vec =  rough_tear_line(image, next_col_start_vec, col_width * (col_idx + 1), True, 5)\n",
    "        next_row_start_vec = np.zeros((cut_column.shape[1],), dtype=int)\n",
    "        for row_idx in range(0, rows):\n",
    "            start_row_px, cut_piece, next_row_start_vec = rough_tear_line(cut_column, next_row_start_vec, row_height * (row_idx + 1), False, 1)\n",
    "\n",
    "            ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(cut_piece, cutoff=BG_2_OBJ_RATIO)\n",
    "            temp = crop(cut_piece, ymin, ymax, xmin, xmax)\n",
    "            \n",
    "            #import pdb; pdb.set_trace()\n",
    "            piece = {}\n",
    "            piece[\"orig\"] = cut_piece\n",
    "            piece[\"cut\"] = temp\n",
    "            piece[\"col\"] = col_idx\n",
    "            piece[\"row\"] = row_idx\n",
    "            piece[\"col_px\"] = start_col_px + xmin\n",
    "            piece[\"row_px\"] = start_row_px + ymin\n",
    "            pieces.append(piece)\n",
    "            \n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Define model util functions\n",
    "\n",
    "# initialize a shaped matrix of weights with random values\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# initialize a shaped matrix of bias with random values\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 2, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 1, 1],\n",
    "                        strides=[1, 2, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x5(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 5, 1],\n",
    "                        strides=[1, 5, 5, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 2, 1],\n",
    "                        strides=[1, 5, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Image utility functions (external source)\n",
    "def branchedPoints(skel):\n",
    "    branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])\n",
    "    branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])\n",
    "    branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])\n",
    "    branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])\n",
    "    branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])\n",
    "    branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])\n",
    "    branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])\n",
    "    br1=mh.morph.hitmiss(skel,branch1)\n",
    "    br2=mh.morph.hitmiss(skel,branch2)\n",
    "    br3=mh.morph.hitmiss(skel,branch3)\n",
    "    br4=mh.morph.hitmiss(skel,branch4)\n",
    "    br5=mh.morph.hitmiss(skel,branch5)\n",
    "    br6=mh.morph.hitmiss(skel,branch6)\n",
    "    br7=mh.morph.hitmiss(skel,branch7)\n",
    "    br8=mh.morph.hitmiss(skel,branch8)\n",
    "    br9=mh.morph.hitmiss(skel,branch9)\n",
    "    return br1+br2+br3+br4+br5+br6+br7+br8+br9\n",
    "\n",
    "def endPoints(skel):\n",
    "    endpoint1=np.array([[0, 0, 0],\n",
    "                        [0, 1, 0],\n",
    "                        [2, 1, 2]])\n",
    "    \n",
    "    endpoint2=np.array([[0, 0, 0],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 2, 1]])\n",
    "    \n",
    "    endpoint3=np.array([[0, 0, 2],\n",
    "                        [0, 1, 1],\n",
    "                        [0, 0, 2]])\n",
    "    \n",
    "    endpoint4=np.array([[0, 2, 1],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint5=np.array([[2, 1, 2],\n",
    "                        [0, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint6=np.array([[1, 2, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint7=np.array([[2, 0, 0],\n",
    "                        [1, 1, 0],\n",
    "                        [2, 0, 0]])\n",
    "    \n",
    "    endpoint8=np.array([[0, 0, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [1, 2, 0]])\n",
    "    \n",
    "    ep1=mh.morph.hitmiss(skel,endpoint1)\n",
    "    ep2=mh.morph.hitmiss(skel,endpoint2)\n",
    "    ep3=mh.morph.hitmiss(skel,endpoint3)\n",
    "    ep4=mh.morph.hitmiss(skel,endpoint4)\n",
    "    ep5=mh.morph.hitmiss(skel,endpoint5)\n",
    "    ep6=mh.morph.hitmiss(skel,endpoint6)\n",
    "    ep7=mh.morph.hitmiss(skel,endpoint7)\n",
    "    ep8=mh.morph.hitmiss(skel,endpoint8)\n",
    "    ep = ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8\n",
    "    return ep\n",
    "\n",
    "def pruning(skeleton, size):\n",
    "    '''remove iteratively end points \"size\" \n",
    "       times from the skeleton\n",
    "    '''\n",
    "    for i in range(0, size):\n",
    "        endpoints = endPoints(skeleton)\n",
    "        endpoints = np.logical_not(endpoints)\n",
    "        skeleton = np.logical_and(skeleton,endpoints)\n",
    "    return skeleton\n",
    "\n",
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_adjustable('box-forced')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_deep - Define the model - 250, 125, 62, 25\n",
    "def model_tf_deep(input_width, forced_bias=0): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob, probability, probabilities #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv0 = bias_variable([32])\n",
    "    h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "    h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "#     W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "#     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    c_fc2 = tf.constant([0, forced_bias], dtype=tf.float32)\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 + c_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    probability = tf.nn.softmax(y_conv,1)\n",
    "    \n",
    "    probabilities=tf.reduce_sum(probability,1)\n",
    "    \n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_orig - Define the model - 250, 125, 62, 25\n",
    "def model_tf_orig(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "#     b_conv0 = bias_variable([32])\n",
    "#     h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "#     h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "#     h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_wide - Define the model - 250, 125, 62, 25\n",
    "def model_tf_wide(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 1, 64])\n",
    "    b_conv1 = bias_variable([64])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*64]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*64, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([2048, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN train\n",
    "def train(train_imgs, train_lbls, output_model, input_model=\"\"):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING:\")\n",
    "    print(\"MODEL:\"+output_model)\n",
    "    print(\"#####################################################################\")\n",
    "\n",
    "    from random import randrange\n",
    "    \n",
    "    # TRAIN Prepare the session\n",
    "\n",
    "    # create a saver object\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "    if input_model != \"\":\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, input_model)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # TRAIN Train the model\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # run the train batches\n",
    "    for i in range(BATCHES):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(BATCH_SIZE):\n",
    "            random_index = randrange(0,len(train_imgs))\n",
    "            image = np.load(train_imgs[random_index]+\".npy\")\n",
    "            x_batch.append(image)\n",
    "            y_batch.append(train_lbls[random_index])\n",
    "\n",
    "        # train\n",
    "        # print(\"step %d\"%(i))\n",
    "        train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "        # print the accuracy thus far\n",
    "        if (i+1)%50 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, output_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    # Close the Session when we're done. If un-commented - need to run next bock of restore...\n",
    "    sess.close()   \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process - OLD?\n",
    "def pre_process(folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+folder)\n",
    "    print(\"#####################################################################\")\n",
    "    result = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            # Read the image\n",
    "            # image = img.imread(os.path.join(root, file_))\n",
    "            image = np.load(os.path.join(root, file_))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            cubes = VAL_slice_to_static_slices(file_, image)\n",
    "            print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "            result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process_training - crop image, then tear it randomly to various tears, then per tear create cubes out of the edges, return cube set\n",
    "def pre_process_training(img_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+img_name)\n",
    "    print(\"#####################################################################\")\n",
    "    short_name = img_name[:img_name.rfind('-D')]\n",
    "    image = read_and_crop(img_name, x_start, x_end, y_start, y_end)\n",
    "    result = []\n",
    "    \n",
    "    for col_cut in range(4, 7): # 3...10\n",
    "        for row_cut in range(3, 5): # 2...5\n",
    "            print(\"PRE_PROCESS:::\"+\"TEAR_\"+str(col_cut)+\"X\"+str(row_cut))\n",
    "            pieces = rough_tear_image(image, col_cut, row_cut)\n",
    "            \n",
    "            for piece in pieces:\n",
    "                # print(\"PRE_PROCESS:::\"+\"PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"]))\n",
    "                file_ = short_name + \"_TEAR_\"+str(col_cut)+\"X\"+str(row_cut)+\"_PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "                cubes = VAL_slice_TEAR_to_static_slices(file_, piece)\n",
    "                for cube in cubes:\n",
    "                    cube[\"tear\"] = str(col_cut)+\"X\"+str(row_cut)\n",
    "                    cube[\"piece_col\"] = piece[\"col\"]\n",
    "                    cube[\"piece_row\"] = piece[\"row\"]\n",
    "                # print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "                result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate1(cubes, model, slice_size, folder, curr_cube):    \n",
    "    # VALIDATE prepare the data sets\n",
    "    test_imgs, test_x_delta, test_y_delta, test_x_file, test_y_file = VAL_build_train_set(cubes, slice_size, folder, curr_cube)\n",
    "    print(\"loaded %d images\"%(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate2(folder, model, slice_size):\n",
    "    test_imgs = []\n",
    "    test_x_file = []\n",
    "    test_y_file = []\n",
    "    the_root = \"\"\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        the_root = root\n",
    "        for file_ in files:\n",
    "            test_imgs.append( os.path.join(root, file_) )\n",
    "            test_x_file.append(file_[:file_.rfind('---P')])\n",
    "            test_y_file.append(file_[file_.rfind('---P')+3:])\n",
    "            \n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    v1t = []\n",
    "    count = 0\n",
    "    length = len(test_imgs)\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i])\n",
    "                x_batch.append(image)\n",
    "                count += 1\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        x_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "\n",
    "\n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    A = np.array(test_x_file)\n",
    "    B = np.array(test_y_file)\n",
    "    C = np.array(test_imgs)\n",
    "    match_x_files = A[match_indexes]\n",
    "    match_y_files = B[match_indexes]\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    for matched_img in match_images:\n",
    "        load_img = np.load(matched_img)\n",
    "        plt.imsave(os.path.join(\"/Volumes/250GB/matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            os.remove( os.path.join(root, file_) ) # delete it from the FS\n",
    "                \n",
    "    with open('matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            print(\"MATCH %s === %s\"%(test_x_file[match_index], test_y_file[match_index]))\n",
    "            # print(\"MATCH %s === %s\"%(A[match_index], B[match_index]))\n",
    "            # csvout.writerow([A[match_index], B[match_index]])\n",
    "            csvout.writerow([test_x_file[match_index], test_y_file[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate2_for_cross_validation(test_imgs, test_lbls, model, max_samples=0):\n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    count = 0\n",
    "    se = 0\n",
    "    st = 0\n",
    "    v1t = []\n",
    "    v2t = []\n",
    "    v1tt = []\n",
    "    v2tt = []\n",
    "    length = len(test_imgs)\n",
    "    if max_samples != 0:\n",
    "        length = max_samples\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i]+\".npy\")\n",
    "                x_batch.append(image)\n",
    "                y_batch.append(train_lbls[i])                \n",
    "                \n",
    "        # print the accuracy thus far\n",
    "    #         train_accuracy = accuracy.eval(feed_dict={\n",
    "    #             x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    #         print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "    #         my_prediction=tf.argmax(y_conv,1)\n",
    "    #         v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "    #         v1t = np.concatenate((v1t, v1), axis=0)\n",
    "\n",
    "        ######## printing the predictions and their normalized values\n",
    "        # print(\"y_conv=\"+str(y_conv.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probability=\"+str(probability.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probabilities=\"+str(probabilities.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        \n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        my_target=tf.argmax(y_,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v2 = my_target.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        v2t = np.concatenate((v2t, v2), axis=0)\n",
    "\n",
    "        c1 = np.sum(np.absolute(np.subtract(v2, v1)))\n",
    "        c2 = np.sum(np.absolute(v2))\n",
    "        se += c1\n",
    "        st += c2\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "        \n",
    "        count += ((i+1) - start)\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "        print(\"step %d-%d, precision %f, recall %f, f_score %f\"%(start, i, precision, recall, f_score))\n",
    "        # print(\"Accumulated total true = %d\"%(st));\n",
    "        # print(\"Accumulated total error rate = %f\"%(se/count));\n",
    "        # v1tt = np.concatenate((v1tt, v1t), axis=0)\n",
    "        # v2tt = np.concatenate((v2tt, v2t), axis=0)\n",
    "        print(\"=== total %d match %d\"%(count, len(np.nonzero(v1t)[0])))\n",
    "\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "    print(\"TOTAL %d, precision %f, recall %f, f_score %f\"%(count, precision, recall, f_score))\n",
    "    print(\"TOTAL true = %d\"%(st));\n",
    "    print(\"TOTAL error rate = %f\"%(se/count));\n",
    "\n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    C = np.array(test_imgs)\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    # for matched_img in match_images:\n",
    "    #    load_img = np.load(matched_img+\".npy\")\n",
    "    #    plt.imsave(os.path.join(ROOT_FOLDER+\"synt_matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "                    \n",
    "    with open('synt_matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            # print(\"MATCH %s === %s\"%(test_imgs[match_index], train_lbls[match_index]))\n",
    "            csvout.writerow([test_imgs[match_index], train_lbls[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def iter_validate(cubes, model, slice_size, folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATING\")\n",
    "    print(\"#####################################################################\")\n",
    "    cubes_len = len(cubes)\n",
    "    batch_size = 100\n",
    "    count = 0\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        count += 1\n",
    "        if count < batch_size: ### TEMP LIMITATION\n",
    "            print(\"CUBE:%s\"%(curr[\"file\"]+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])))\n",
    "            validate1(cubes, model, slice_size, folder, curr)\n",
    "            validate2(folder, model, slice_size)\n",
    "        \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATION ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_all(folder, model, slice_size):\n",
    "    model_tf(slice_size)\n",
    "    cubes_set = pre_process(folder)\n",
    "    validate(cubes_set, model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# HELPER block\n",
    "# image = read_and_crop(\"PX303/FG001/PX303-Fg001-V-C01-R01-D05032015-T112602-ML924__012.jpg\")\n",
    "## image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R01-D08032015-T110900-ML924__012.jpg\", 100, -1, 400, -1)\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R02-D08032015-T105147-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R01-D08032015-T110025-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R02-D08032015-T105553-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R01-D08032015-T120605-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R02-D08032015-T115230-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R01-D08032015-T120158-ML924__012.jpg\")\n",
    "##image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R02-D08032015-T115704-ML924__012.jpg\", 0, 6200, 0, 4400)\n",
    "##plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_train_from_disk(path):\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_ in files:\n",
    "            file_name = os.path.join(root, file_)\n",
    "            file_name = file_name[:file_name.rfind(\".\")]\n",
    "            train_imgs.append(file_name)\n",
    "            train_lbls.append([1,0] if file_.startswith(\"0=\") else [0,1])\n",
    "    return train_imgs, train_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg001-V-C01-R01-D05032015-T112520-ML638__006.jpg\n",
      "#####################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/matplotlib/image.py:1318: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  x = np.fromstring(x_str, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X4\n",
      "PRE_PROCESS:::TEAR_5X3\n",
      "PRE_PROCESS:::TEAR_5X4\n",
      "PRE_PROCESS:::TEAR_6X3\n",
      "PRE_PROCESS:::TEAR_6X4\n",
      "WARNING:tensorflow:From <ipython-input-5-898022341402>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "TRAINING:\n",
      "MODEL:/home/il239838/files/model_binary/tear_model1.ckpt\n",
      "#####################################################################\n",
      "WARNING:tensorflow:From /home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 49, training accuracy 0.84\n",
      "step 99, training accuracy 0.86\n",
      "step 149, training accuracy 0.94\n",
      "step 199, training accuracy 0.94\n",
      "step 249, training accuracy 1\n",
      "step 299, training accuracy 0.96\n",
      "step 349, training accuracy 0.98\n",
      "step 399, training accuracy 0.92\n",
      "step 449, training accuracy 1\n",
      "step 499, training accuracy 0.92\n",
      "step 549, training accuracy 0.92\n",
      "step 599, training accuracy 0.96\n",
      "step 649, training accuracy 0.94\n",
      "step 699, training accuracy 0.98\n",
      "step 749, training accuracy 0.98\n",
      "step 799, training accuracy 0.98\n",
      "Optimization Finished!\n",
      "step 799, training accuracy 0.98\n",
      "Model saved in file: /home/il239838/files/model_binary/tear_model1.ckpt\n",
      "#####################################################################\n",
      "TRAINING ENDED\n",
      "#####################################################################\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# RUN1 - take 1st large pieces and train on it\n",
    "# cubes_set = pre_process_training(\"PX303-Fg001-V-C01-R01-D05032015-T112520-ML638__006.jpg\")\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats/\", 20)\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RE-RUN1 - take 1st large pieces and train on it\n",
    "# train_imgs, train_lbls = \\\n",
    "#     load_train_from_disk(ROOT_FOLDER + \"train_concats/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\n",
      "#####################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/matplotlib/image.py:1318: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  x = np.fromstring(x_str, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X4\n",
      "PRE_PROCESS:::TEAR_5X3\n",
      "PRE_PROCESS:::TEAR_5X4\n",
      "PRE_PROCESS:::TEAR_6X3\n",
      "PRE_PROCESS:::TEAR_6X4\n"
     ]
    }
   ],
   "source": [
    "# RUN2 - take 2nd large pieces and train on it\n",
    "# cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", 100, -1, 400, -1)\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats2/\", 20)\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\", ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RE-RUN2 - take 2nd large pieces and train on it\n",
    "# train_imgs, train_lbls = \\\n",
    "#     load_train_from_disk(ROOT_FOLDER + \"train_concats2/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\", ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL RUN3 - take 3rd large pieces and train on it OR TEST in next block\n",
    "\n",
    "# cubes_set = pre_process_training(\"PX303/FG006/PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf(250)    \n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model3.ckpt\", ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\n",
      "#####################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/matplotlib/image.py:1318: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  x = np.fromstring(x_str, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_4X4\n",
      "PRE_PROCESS:::TEAR_5X3\n",
      "PRE_PROCESS:::TEAR_5X4\n",
      "PRE_PROCESS:::TEAR_6X3\n",
      "PRE_PROCESS:::TEAR_6X4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37952\n",
      "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model2.ckpt\n",
      "Model restored.\n",
      ">>> step 100\n",
      "step 0-99, precision 0.500000, recall 0.681818, f_score 0.576923\n",
      "=== total 100 match 30\n",
      ">>> step 200\n",
      "step 100-199, precision 0.524590, recall 0.695652, f_score 0.598131\n",
      "=== total 200 match 61\n",
      ">>> step 300\n",
      "step 200-299, precision 0.344086, recall 0.695652, f_score 0.460432\n",
      "=== total 300 match 93\n",
      ">>> step 400\n",
      "step 300-399, precision 0.353383, recall 0.746032, f_score 0.479592\n",
      "=== total 400 match 133\n",
      ">>> step 500\n",
      "step 400-499, precision 0.394444, recall 0.771739, f_score 0.522059\n",
      "=== total 500 match 180\n",
      ">>> step 600\n",
      "step 500-599, precision 0.365979, recall 0.771739, f_score 0.496503\n",
      "=== total 600 match 194\n",
      ">>> step 700\n",
      "step 600-699, precision 0.327189, recall 0.739583, f_score 0.453674\n",
      "=== total 700 match 217\n",
      ">>> step 800\n",
      "step 700-799, precision 0.319672, recall 0.696429, f_score 0.438202\n",
      "=== total 800 match 244\n",
      ">>> step 900\n",
      "step 800-899, precision 0.301158, recall 0.696429, f_score 0.420485\n",
      "=== total 900 match 259\n",
      ">>> step 1000\n",
      "step 900-999, precision 0.289199, recall 0.648438, f_score 0.400000\n",
      "=== total 1000 match 287\n",
      ">>> step 1100\n",
      "step 1000-1099, precision 0.258567, recall 0.648438, f_score 0.369710\n",
      "=== total 1100 match 321\n",
      ">>> step 1200\n",
      "step 1100-1199, precision 0.247813, recall 0.625000, f_score 0.354906\n",
      "=== total 1200 match 343\n",
      ">>> step 1300\n",
      "step 1200-1299, precision 0.256831, recall 0.626667, f_score 0.364341\n",
      "=== total 1300 match 366\n",
      ">>> step 1400\n",
      "step 1300-1399, precision 0.267677, recall 0.650307, f_score 0.379249\n",
      "=== total 1400 match 396\n",
      ">>> step 1500\n",
      "step 1400-1499, precision 0.245370, recall 0.650307, f_score 0.356303\n",
      "=== total 1500 match 432\n",
      ">>> step 1600\n",
      "step 1500-1599, precision 0.256842, recall 0.659459, f_score 0.369697\n",
      "=== total 1600 match 475\n",
      ">>> step 1700\n",
      "step 1600-1699, precision 0.261905, recall 0.666667, f_score 0.376068\n",
      "=== total 1700 match 504\n",
      ">>> step 1800\n",
      "step 1700-1799, precision 0.269027, recall 0.690909, f_score 0.387261\n",
      "=== total 1800 match 565\n",
      ">>> step 1900\n",
      "step 1800-1899, precision 0.261168, recall 0.690909, f_score 0.379052\n",
      "=== total 1900 match 582\n",
      ">>> step 2000\n",
      "step 1900-1999, precision 0.258883, recall 0.692308, f_score 0.376847\n",
      "=== total 2000 match 591\n",
      ">>> step 2100\n",
      "step 2000-2099, precision 0.255426, recall 0.692308, f_score 0.373171\n",
      "=== total 2100 match 599\n",
      ">>> step 2200\n",
      "step 2100-2199, precision 0.254576, recall 0.692308, f_score 0.372263\n",
      "=== total 2200 match 601\n",
      ">>> step 2300\n",
      "step 2200-2299, precision 0.252046, recall 0.693694, f_score 0.369748\n",
      "=== total 2300 match 611\n",
      ">>> step 2400\n",
      "step 2300-2399, precision 0.255663, recall 0.683983, f_score 0.372203\n",
      "=== total 2400 match 618\n",
      ">>> step 2500\n",
      "step 2400-2499, precision 0.252396, recall 0.683983, f_score 0.368728\n",
      "=== total 2500 match 626\n",
      ">>> step 2600\n",
      "step 2500-2599, precision 0.251149, recall 0.677686, f_score 0.366480\n",
      "=== total 2600 match 653\n",
      ">>> step 2700\n",
      "step 2600-2699, precision 0.253991, recall 0.678295, f_score 0.369588\n",
      "=== total 2700 match 689\n",
      ">>> step 2800\n",
      "step 2700-2799, precision 0.251076, recall 0.678295, f_score 0.366492\n",
      "=== total 2800 match 697\n",
      ">>> step 2900\n",
      "step 2800-2899, precision 0.267586, recall 0.675958, f_score 0.383399\n",
      "=== total 2900 match 725\n",
      ">>> step 3000\n",
      "step 2900-2999, precision 0.288512, recall 0.665663, f_score 0.402550\n",
      "=== total 3000 match 766\n",
      ">>> step 3100\n",
      "step 3000-3099, precision 0.284251, recall 0.660714, f_score 0.397493\n",
      "=== total 3100 match 781\n",
      ">>> step 3200\n",
      "step 3100-3199, precision 0.310219, recall 0.672823, f_score 0.424646\n",
      "=== total 3200 match 822\n",
      ">>> step 3300\n",
      "step 3200-3299, precision 0.303571, recall 0.672823, f_score 0.418376\n",
      "=== total 3300 match 840\n",
      ">>> step 3400\n",
      "step 3300-3399, precision 0.310461, recall 0.688279, f_score 0.427907\n",
      "=== total 3400 match 889\n",
      ">>> step 3500\n",
      "step 3400-3499, precision 0.327292, recall 0.696145, f_score 0.445250\n",
      "=== total 3500 match 938\n",
      ">>> step 3600\n",
      "step 3500-3599, precision 0.333994, recall 0.715499, f_score 0.455405\n",
      "=== total 3600 match 1009\n",
      ">>> step 3700\n",
      "step 3600-3699, precision 0.344569, recall 0.731610, f_score 0.468491\n",
      "=== total 3700 match 1068\n",
      ">>> step 3800\n",
      "step 3700-3799, precision 0.357208, recall 0.711191, f_score 0.475558\n",
      "=== total 3800 match 1103\n",
      ">>> step 3900\n",
      "step 3800-3899, precision 0.364996, recall 0.701014, f_score 0.480046\n",
      "=== total 3900 match 1137\n",
      ">>> step 4000\n",
      "step 3900-3999, precision 0.368146, recall 0.681159, f_score 0.477966\n",
      "=== total 4000 match 1149\n",
      ">>> step 4100\n",
      "step 4000-4099, precision 0.370653, recall 0.684953, f_score 0.481013\n",
      "=== total 4100 match 1179\n",
      ">>> step 4200\n",
      "step 4100-4199, precision 0.382186, recall 0.689051, f_score 0.491667\n",
      "=== total 4200 match 1235\n",
      ">>> step 4300\n",
      "step 4200-4299, precision 0.385783, recall 0.677419, f_score 0.491603\n",
      "=== total 4300 match 1252\n",
      ">>> step 4400\n",
      "step 4300-4399, precision 0.389752, recall 0.682993, f_score 0.496293\n",
      "=== total 4400 match 1288\n",
      ">>> step 4500\n",
      "step 4400-4499, precision 0.382550, recall 0.685829, f_score 0.491144\n",
      "=== total 4500 match 1341\n",
      ">>> step 4600\n",
      "step 4500-4599, precision 0.374111, recall 0.691196, f_score 0.485464\n",
      "=== total 4600 match 1406\n",
      ">>> step 4700\n",
      "step 4600-4699, precision 0.379021, recall 0.692209, f_score 0.489833\n",
      "=== total 4700 match 1430\n",
      ">>> step 4800\n",
      "step 4700-4799, precision 0.374568, recall 0.692209, f_score 0.486099\n",
      "=== total 4800 match 1447\n",
      ">>> step 4900\n",
      "step 4800-4899, precision 0.371233, recall 0.692209, f_score 0.483281\n",
      "=== total 4900 match 1460\n",
      ">>> step 5000\n",
      "step 4900-4999, precision 0.371542, recall 0.700621, f_score 0.485579\n",
      "=== total 5000 match 1518\n",
      ">>> step 5100\n",
      "step 5000-5099, precision 0.371373, recall 0.704156, f_score 0.486281\n",
      "=== total 5100 match 1551\n",
      ">>> step 5200\n",
      "step 5100-5199, precision 0.365947, recall 0.704156, f_score 0.481605\n",
      "=== total 5200 match 1574\n",
      ">>> step 5300\n",
      "step 5200-5299, precision 0.367128, recall 0.701564, f_score 0.482017\n",
      "=== total 5300 match 1588\n",
      ">>> step 5400\n",
      "step 5300-5399, precision 0.365397, recall 0.699761, f_score 0.480098\n",
      "=== total 5400 match 1601\n",
      ">>> step 5500\n",
      "step 5400-5499, precision 0.361557, recall 0.699761, f_score 0.476773\n",
      "=== total 5500 match 1618\n",
      ">>> step 5600\n",
      "step 5500-5599, precision 0.353438, recall 0.696790, f_score 0.468988\n",
      "=== total 5600 match 1658\n",
      ">>> step 5700\n",
      "step 5600-5699, precision 0.349017, recall 0.696790, f_score 0.465079\n",
      "=== total 5700 match 1679\n",
      ">>> step 5800\n",
      "step 5700-5799, precision 0.343494, recall 0.695136, f_score 0.459788\n",
      "=== total 5800 match 1706\n",
      ">>> step 5900\n",
      "step 5800-5899, precision 0.339110, recall 0.694675, f_score 0.455745\n",
      "=== total 5900 match 1731\n",
      ">>> step 6000\n",
      "step 5900-5999, precision 0.335237, recall 0.693034, f_score 0.451886\n",
      "=== total 6000 match 1751\n",
      ">>> step 6100\n",
      "step 6000-6099, precision 0.330332, recall 0.691402, f_score 0.447068\n",
      "=== total 6100 match 1777\n",
      ">>> step 6200\n",
      "step 6100-6199, precision 0.328483, recall 0.691402, f_score 0.445372\n",
      "=== total 6200 match 1787\n",
      ">>> step 6300\n",
      "step 6200-6299, precision 0.332963, recall 0.688863, f_score 0.448934\n",
      "=== total 6300 match 1802\n",
      ">>> step 6400\n",
      "step 6300-6399, precision 0.334983, recall 0.688914, f_score 0.450777\n",
      "=== total 6400 match 1818\n",
      ">>> step 6500\n",
      "step 6400-6499, precision 0.340957, recall 0.697497, f_score 0.458021\n",
      "=== total 6500 match 1880\n",
      ">>> step 6600\n",
      "step 6500-6599, precision 0.337724, recall 0.697497, f_score 0.455094\n",
      "=== total 6600 match 1898\n",
      ">>> step 6700\n",
      "step 6600-6699, precision 0.340625, recall 0.695005, f_score 0.457183\n",
      "=== total 6700 match 1920\n",
      ">>> step 6800\n",
      "step 6700-6799, precision 0.338652, recall 0.689727, f_score 0.454263\n",
      "=== total 6800 match 1943\n",
      ">>> step 6900\n",
      "step 6800-6899, precision 0.339095, recall 0.689762, f_score 0.454669\n",
      "=== total 6900 match 1967\n",
      ">>> step 7000\n",
      "step 6900-6999, precision 0.337180, recall 0.691358, f_score 0.453288\n",
      "=== total 7000 match 1993\n",
      ">>> step 7100\n",
      "step 7000-7099, precision 0.334653, recall 0.691914, f_score 0.451118\n",
      "=== total 7100 match 2020\n",
      ">>> step 7200\n",
      "step 7100-7199, precision 0.338875, recall 0.693694, f_score 0.455322\n",
      "=== total 7200 match 2045\n",
      ">>> step 7300\n",
      "step 7200-7299, precision 0.339971, recall 0.691700, f_score 0.455878\n",
      "=== total 7300 match 2059\n",
      ">>> step 7400\n",
      "step 7300-7399, precision 0.342761, recall 0.690821, f_score 0.458186\n",
      "=== total 7400 match 2086\n",
      ">>> step 7500\n",
      "step 7400-7499, precision 0.344614, recall 0.690544, f_score 0.459777\n",
      "=== total 7500 match 2098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 7600\n",
      "step 7500-7599, precision 0.344991, recall 0.688679, f_score 0.459698\n",
      "=== total 7600 match 2116\n",
      ">>> step 7700\n",
      "step 7600-7699, precision 0.344424, recall 0.688202, f_score 0.459088\n",
      "=== total 7700 match 2134\n",
      ">>> step 7800\n",
      "step 7700-7799, precision 0.340751, recall 0.687558, f_score 0.455673\n",
      "=== total 7800 match 2157\n",
      ">>> step 7900\n",
      "step 7800-7899, precision 0.340278, recall 0.686916, f_score 0.455108\n",
      "=== total 7900 match 2160\n",
      ">>> step 8000\n",
      "step 7900-7999, precision 0.339006, recall 0.687500, f_score 0.454097\n",
      "=== total 8000 match 2174\n",
      ">>> step 8100\n",
      "step 8000-8099, precision 0.335305, recall 0.687500, f_score 0.450765\n",
      "=== total 8100 match 2198\n",
      ">>> step 8200\n",
      "step 8100-8199, precision 0.332586, recall 0.688951, f_score 0.448609\n",
      "=== total 8200 match 2231\n",
      ">>> step 8300\n",
      "step 8200-8299, precision 0.325642, recall 0.688766, f_score 0.442211\n",
      "=== total 8300 match 2297\n",
      ">>> step 8400\n",
      "step 8300-8399, precision 0.321687, recall 0.687614, f_score 0.438316\n",
      "=== total 8400 match 2347\n",
      ">>> step 8500\n",
      "step 8400-8499, precision 0.319509, recall 0.687614, f_score 0.436290\n",
      "=== total 8500 match 2363\n",
      ">>> step 8600\n",
      "step 8500-8599, precision 0.316164, recall 0.687614, f_score 0.433161\n",
      "=== total 8600 match 2388\n",
      ">>> step 8700\n",
      "step 8600-8699, precision 0.314060, recall 0.686988, f_score 0.431059\n",
      "=== total 8700 match 2404\n",
      ">>> step 8800\n",
      "step 8700-8799, precision 0.312112, recall 0.686364, f_score 0.429099\n",
      "=== total 8800 match 2419\n",
      ">>> step 8900\n",
      "step 8800-8899, precision 0.311855, recall 0.686364, f_score 0.428855\n",
      "=== total 8900 match 2421\n",
      ">>> step 9000\n",
      "step 8900-8999, precision 0.311597, recall 0.685740, f_score 0.428490\n",
      "=== total 9000 match 2423\n",
      ">>> step 9100\n",
      "step 9000-9099, precision 0.308667, recall 0.685740, f_score 0.425712\n",
      "=== total 9100 match 2446\n",
      ">>> step 9200\n",
      "step 9100-9199, precision 0.306667, recall 0.686878, f_score 0.424022\n",
      "=== total 9200 match 2475\n",
      ">>> step 9300\n",
      "step 9200-9299, precision 0.308632, recall 0.692920, f_score 0.427052\n",
      "=== total 9300 match 2537\n",
      ">>> step 9400\n",
      "step 9300-9399, precision 0.308025, recall 0.692920, f_score 0.426471\n",
      "=== total 9400 match 2542\n",
      ">>> step 9500\n",
      "step 9400-9499, precision 0.307811, recall 0.697472, f_score 0.427122\n",
      "=== total 9500 match 2599\n",
      ">>> step 9600\n",
      "step 9500-9599, precision 0.304762, recall 0.697472, f_score 0.424178\n",
      "=== total 9600 match 2625\n",
      ">>> step 9700\n",
      "step 9600-9699, precision 0.308408, recall 0.704932, f_score 0.429089\n",
      "=== total 9700 match 2688\n",
      ">>> step 9800\n",
      "step 9700-9799, precision 0.307525, recall 0.709137, f_score 0.429006\n",
      "=== total 9800 match 2751\n",
      ">>> step 9900\n",
      "step 9800-9899, precision 0.309929, recall 0.715221, f_score 0.432459\n",
      "=== total 9900 match 2820\n",
      ">>> step 10000\n",
      "step 9900-9999, precision 0.304424, recall 0.715221, f_score 0.427071\n",
      "=== total 10000 match 2871\n",
      ">>> step 10100\n",
      "step 10000-10099, precision 0.302213, recall 0.715221, f_score 0.424891\n",
      "=== total 10100 match 2892\n",
      ">>> step 10200\n",
      "step 10100-10199, precision 0.301587, recall 0.715221, f_score 0.424272\n",
      "=== total 10200 match 2898\n",
      ">>> step 10300\n",
      "step 10200-10299, precision 0.303858, recall 0.715434, f_score 0.426552\n",
      "=== total 10300 match 2929\n",
      ">>> step 10400\n",
      "step 10300-10399, precision 0.304965, recall 0.713270, f_score 0.427253\n",
      "=== total 10400 match 2961\n",
      ">>> step 10500\n",
      "step 10400-10499, precision 0.305125, recall 0.707584, f_score 0.426384\n",
      "=== total 10500 match 2966\n",
      ">>> step 10600\n",
      "step 10500-10599, precision 0.305260, recall 0.709752, f_score 0.426909\n",
      "=== total 10600 match 3004\n",
      ">>> step 10700\n",
      "step 10600-10699, precision 0.305975, recall 0.709285, f_score 0.427523\n",
      "=== total 10700 match 3046\n",
      ">>> step 10800\n",
      "step 10700-10799, precision 0.303979, recall 0.709285, f_score 0.425571\n",
      "=== total 10800 match 3066\n",
      ">>> step 10900\n",
      "step 10800-10899, precision 0.305619, recall 0.710189, f_score 0.427339\n",
      "=== total 10900 match 3079\n",
      ">>> step 11000\n",
      "step 10900-10999, precision 0.304278, recall 0.712257, f_score 0.426398\n",
      "=== total 11000 match 3132\n",
      ">>> step 11100\n",
      "step 11000-11099, precision 0.303116, recall 0.711752, f_score 0.425166\n",
      "=== total 11100 match 3177\n",
      ">>> step 11200\n",
      "step 11100-11199, precision 0.301881, recall 0.711752, f_score 0.423949\n",
      "=== total 11200 match 3190\n",
      ">>> step 11300\n",
      "step 11200-11299, precision 0.304646, recall 0.712099, f_score 0.426731\n",
      "=== total 11300 match 3207\n",
      ">>> step 11400\n",
      "step 11300-11399, precision 0.306407, recall 0.711207, f_score 0.428293\n",
      "=== total 11400 match 3231\n",
      ">>> step 11500\n",
      "step 11400-11499, precision 0.305367, recall 0.711207, f_score 0.427277\n",
      "=== total 11500 match 3242\n",
      ">>> step 11600\n",
      "step 11500-11599, precision 0.304897, recall 0.711207, f_score 0.426816\n",
      "=== total 11600 match 3247\n",
      ">>> step 11700\n",
      "step 11600-11699, precision 0.306066, recall 0.712046, f_score 0.428112\n",
      "=== total 11700 match 3264\n",
      ">>> step 11800\n",
      "step 11700-11799, precision 0.304150, recall 0.712562, f_score 0.426327\n",
      "=== total 11800 match 3301\n",
      ">>> step 11900\n",
      "step 11800-11899, precision 0.300925, recall 0.713579, f_score 0.423327\n",
      "=== total 11900 match 3353\n",
      ">>> step 12000\n",
      "step 11900-11999, precision 0.300527, recall 0.716981, f_score 0.423529\n",
      "=== total 12000 match 3414\n",
      ">>> step 12100\n",
      "step 12000-12099, precision 0.300527, recall 0.716981, f_score 0.423529\n",
      "=== total 12100 match 3414\n",
      ">>> step 12200\n",
      "step 12100-12199, precision 0.303772, recall 0.722603, f_score 0.427732\n",
      "=== total 12200 match 3473\n",
      ">>> step 12300\n",
      "step 12200-12299, precision 0.307824, recall 0.723746, f_score 0.431936\n",
      "=== total 12300 match 3515\n",
      ">>> step 12400\n",
      "step 12300-12399, precision 0.312832, recall 0.728990, f_score 0.437793\n",
      "=== total 12400 match 3577\n",
      ">>> step 12500\n",
      "step 12400-12499, precision 0.315080, recall 0.729764, f_score 0.440131\n",
      "=== total 12500 match 3634\n",
      ">>> step 12600\n",
      "step 12500-12599, precision 0.314512, recall 0.729285, f_score 0.439489\n",
      "=== total 12600 match 3666\n",
      ">>> step 12700\n",
      "step 12600-12699, precision 0.313998, recall 0.729285, f_score 0.438987\n",
      "=== total 12700 match 3672\n",
      ">>> step 12800\n",
      "step 12700-12799, precision 0.312229, recall 0.728996, f_score 0.437204\n",
      "=== total 12800 match 3696\n",
      ">>> step 12900\n",
      "step 12800-12899, precision 0.310716, recall 0.728535, f_score 0.435636\n",
      "=== total 12900 match 3714\n",
      ">>> step 13000\n",
      "step 12900-12999, precision 0.311159, recall 0.728643, f_score 0.436090\n",
      "=== total 13000 match 3728\n",
      ">>> step 13100\n",
      "step 13000-13099, precision 0.310088, recall 0.727216, f_score 0.434783\n",
      "=== total 13100 match 3757\n",
      ">>> step 13200\n",
      "step 13100-13199, precision 0.310976, recall 0.727667, f_score 0.435736\n",
      "=== total 13200 match 3772\n",
      ">>> step 13300\n",
      "step 13200-13299, precision 0.311842, recall 0.726103, f_score 0.436303\n",
      "=== total 13300 match 3800\n",
      ">>> step 13400\n",
      "step 13300-13399, precision 0.312516, recall 0.726169, f_score 0.436975\n",
      "=== total 13400 match 3827\n",
      ">>> step 13500\n",
      "step 13400-13499, precision 0.314820, recall 0.728966, f_score 0.439732\n",
      "=== total 13500 match 3853\n",
      ">>> step 13600\n",
      "step 13500-13599, precision 0.314973, recall 0.729778, f_score 0.440029\n",
      "=== total 13600 match 3867\n",
      ">>> step 13700\n",
      "step 13600-13699, precision 0.316045, recall 0.732386, f_score 0.441549\n",
      "=== total 13700 match 3914\n",
      ">>> step 13800\n",
      "step 13700-13799, precision 0.320532, recall 0.733487, f_score 0.446114\n",
      "=== total 13800 match 3984\n",
      ">>> step 13900\n",
      "step 13800-13899, precision 0.322123, recall 0.735913, f_score 0.448103\n",
      "=== total 13900 match 4014\n",
      ">>> step 14000\n",
      "step 13900-13999, precision 0.321358, recall 0.737853, f_score 0.447720\n",
      "=== total 14000 match 4064\n",
      ">>> step 14100\n",
      "step 14000-14099, precision 0.321315, recall 0.739764, f_score 0.448030\n",
      "=== total 14100 match 4105\n",
      ">>> step 14200\n",
      "step 14100-14199, precision 0.321065, recall 0.738307, f_score 0.447519\n",
      "=== total 14200 match 4130\n",
      ">>> step 14300\n",
      "step 14200-14299, precision 0.320646, recall 0.737396, f_score 0.446944\n",
      "=== total 14300 match 4151\n",
      ">>> step 14400\n",
      "step 14300-14399, precision 0.319190, recall 0.736524, f_score 0.445368\n",
      "=== total 14400 match 4195\n",
      ">>> step 14500\n",
      "step 14400-14499, precision 0.317539, recall 0.735664, f_score 0.443603\n",
      "=== total 14500 match 4242\n",
      ">>> step 14600\n",
      "step 14500-14599, precision 0.317683, recall 0.733945, f_score 0.443430\n",
      "=== total 14600 match 4281\n",
      ">>> step 14700\n",
      "step 14600-14699, precision 0.317090, recall 0.733945, f_score 0.442852\n",
      "=== total 14700 match 4289\n",
      ">>> step 14800\n",
      "step 14700-14799, precision 0.316353, recall 0.733945, f_score 0.442133\n",
      "=== total 14800 match 4299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 14900\n",
      "step 14800-14899, precision 0.315034, recall 0.733945, f_score 0.440843\n",
      "=== total 14900 match 4317\n",
      ">>> step 15000\n",
      "step 14900-14999, precision 0.316359, recall 0.735011, f_score 0.442332\n",
      "=== total 15000 match 4340\n",
      ">>> step 15100\n",
      "step 15000-15099, precision 0.314475, recall 0.735011, f_score 0.440488\n",
      "=== total 15100 match 4366\n",
      ">>> step 15200\n",
      "step 15100-15199, precision 0.314520, recall 0.736281, f_score 0.440759\n",
      "=== total 15200 match 4394\n",
      ">>> step 15300\n",
      "step 15200-15299, precision 0.310701, recall 0.736281, f_score 0.436996\n",
      "=== total 15300 match 4448\n",
      ">>> step 15400\n",
      "step 15300-15399, precision 0.310817, recall 0.736310, f_score 0.437115\n",
      "=== total 15400 match 4456\n",
      ">>> step 15500\n",
      "step 15400-15499, precision 0.310237, recall 0.735949, f_score 0.436478\n",
      "=== total 15500 match 4474\n",
      ">>> step 15600\n",
      "step 15500-15599, precision 0.310098, recall 0.735949, f_score 0.436341\n",
      "=== total 15600 match 4476\n",
      ">>> step 15700\n",
      "step 15600-15699, precision 0.307624, recall 0.735949, f_score 0.433886\n",
      "=== total 15700 match 4512\n",
      ">>> step 15800\n",
      "step 15700-15799, precision 0.306537, recall 0.735949, f_score 0.432803\n",
      "=== total 15800 match 4528\n",
      ">>> step 15900\n",
      "step 15800-15899, precision 0.303454, recall 0.735949, f_score 0.429721\n",
      "=== total 15900 match 4574\n",
      ">>> step 16000\n",
      "step 15900-15999, precision 0.301542, recall 0.735949, f_score 0.427801\n",
      "=== total 16000 match 4603\n",
      ">>> step 16100\n",
      "step 16000-16099, precision 0.300758, recall 0.735949, f_score 0.427011\n",
      "=== total 16100 match 4615\n",
      ">>> step 16200\n",
      "step 16100-16199, precision 0.300563, recall 0.735949, f_score 0.426814\n",
      "=== total 16200 match 4618\n",
      ">>> step 16300\n",
      "step 16200-16299, precision 0.300992, recall 0.736675, f_score 0.427369\n",
      "=== total 16300 match 4638\n",
      ">>> step 16400\n",
      "step 16300-16399, precision 0.299808, recall 0.737395, f_score 0.426294\n",
      "=== total 16400 match 4683\n",
      ">>> step 16500\n",
      "step 16400-16499, precision 0.297836, recall 0.737395, f_score 0.424297\n",
      "=== total 16500 match 4714\n",
      ">>> step 16600\n",
      "step 16500-16599, precision 0.297017, recall 0.737395, f_score 0.423466\n",
      "=== total 16600 match 4727\n",
      ">>> step 16700\n",
      "step 16600-16699, precision 0.296515, recall 0.737395, f_score 0.422955\n",
      "=== total 16700 match 4735\n",
      ">>> step 16800\n",
      "step 16700-16799, precision 0.294969, recall 0.737808, f_score 0.421447\n",
      "=== total 16800 match 4770\n",
      ">>> step 16900\n",
      "step 16800-16899, precision 0.293125, recall 0.737808, f_score 0.419562\n",
      "=== total 16900 match 4800\n",
      ">>> step 17000\n",
      "step 16900-16999, precision 0.292576, recall 0.737808, f_score 0.418999\n",
      "=== total 17000 match 4809\n",
      ">>> step 17100\n",
      "step 17000-17099, precision 0.291606, recall 0.737808, f_score 0.418004\n",
      "=== total 17100 match 4825\n",
      ">>> step 17200\n",
      "step 17100-17199, precision 0.291846, recall 0.736651, f_score 0.418064\n",
      "=== total 17200 match 4869\n",
      ">>> step 17300\n",
      "step 17200-17299, precision 0.290061, recall 0.736354, f_score 0.416182\n",
      "=== total 17300 match 4930\n",
      ">>> step 17400\n",
      "step 17300-17399, precision 0.288423, recall 0.736354, f_score 0.414493\n",
      "=== total 17400 match 4958\n",
      ">>> step 17500\n",
      "step 17400-17499, precision 0.287658, recall 0.735550, f_score 0.413575\n",
      "=== total 17500 match 4999\n",
      ">>> step 17600\n",
      "step 17500-17599, precision 0.287909, recall 0.735099, f_score 0.413763\n",
      "=== total 17600 match 5012\n",
      ">>> step 17700\n",
      "step 17600-17699, precision 0.285460, recall 0.734248, f_score 0.411095\n",
      "=== total 17700 match 5062\n",
      ">>> step 17800\n",
      "step 17700-17799, precision 0.284285, recall 0.734415, f_score 0.409901\n",
      "=== total 17800 match 5097\n",
      ">>> step 17900\n",
      "step 17800-17899, precision 0.283258, recall 0.731954, f_score 0.408451\n",
      "=== total 17900 match 5119\n",
      ">>> step 18000\n",
      "step 17900-17999, precision 0.283093, recall 0.731954, f_score 0.408278\n",
      "=== total 18000 match 5122\n",
      ">>> step 18100\n",
      "step 18000-18099, precision 0.283234, recall 0.732901, f_score 0.408572\n",
      "=== total 18100 match 5183\n",
      ">>> step 18200\n",
      "step 18100-18199, precision 0.282621, recall 0.731647, f_score 0.407740\n",
      "=== total 18200 match 5219\n",
      ">>> step 18300\n",
      "step 18200-18299, precision 0.284271, recall 0.732318, f_score 0.409559\n",
      "=== total 18300 match 5245\n",
      ">>> step 18400\n",
      "step 18300-18399, precision 0.284091, recall 0.731351, f_score 0.409221\n",
      "=== total 18400 match 5280\n",
      ">>> step 18500\n",
      "step 18400-18499, precision 0.283607, recall 0.731351, f_score 0.408719\n",
      "=== total 18500 match 5289\n",
      ">>> step 18600\n",
      "step 18500-18599, precision 0.284072, recall 0.732264, f_score 0.409344\n",
      "=== total 18600 match 5305\n",
      ">>> step 18700\n",
      "step 18600-18699, precision 0.285554, recall 0.733461, f_score 0.411069\n",
      "=== total 18700 match 5358\n",
      ">>> step 18800\n",
      "step 18700-18799, precision 0.284870, recall 0.734286, f_score 0.410488\n",
      "=== total 18800 match 5413\n",
      ">>> step 18900\n",
      "step 18800-18899, precision 0.285846, recall 0.733145, f_score 0.411321\n",
      "=== total 18900 match 5440\n",
      ">>> step 19000\n",
      "step 18900-18999, precision 0.285216, recall 0.733145, f_score 0.410669\n",
      "=== total 19000 match 5452\n",
      ">>> step 19100\n",
      "step 19000-19099, precision 0.286625, recall 0.734018, f_score 0.412266\n",
      "=== total 19100 match 5488\n",
      ">>> step 19200\n",
      "step 19100-19199, precision 0.287166, recall 0.734448, f_score 0.412893\n",
      "=== total 19200 match 5509\n",
      ">>> step 19300\n",
      "step 19200-19299, precision 0.286899, recall 0.733979, f_score 0.412542\n",
      "=== total 19300 match 5549\n",
      ">>> step 19400\n",
      "step 19300-19399, precision 0.287525, recall 0.735281, f_score 0.413395\n",
      "=== total 19400 match 5603\n",
      ">>> step 19500\n",
      "step 19400-19499, precision 0.287115, recall 0.735281, f_score 0.412971\n",
      "=== total 19500 match 5611\n",
      ">>> step 19600\n",
      "step 19500-19599, precision 0.285942, recall 0.734275, f_score 0.411599\n",
      "=== total 19600 match 5634\n",
      ">>> step 19700\n",
      "step 19600-19699, precision 0.285638, recall 0.734275, f_score 0.411284\n",
      "=== total 19700 match 5640\n",
      ">>> step 19800\n",
      "step 19700-19799, precision 0.285538, recall 0.734481, f_score 0.411213\n",
      "=== total 19800 match 5677\n",
      ">>> step 19900\n",
      "step 19800-19899, precision 0.284686, recall 0.734481, f_score 0.410328\n",
      "=== total 19900 match 5694\n",
      ">>> step 20000\n",
      "step 19900-19999, precision 0.284715, recall 0.733333, f_score 0.410179\n",
      "=== total 20000 match 5718\n",
      ">>> step 20100\n",
      "step 20000-20099, precision 0.287056, recall 0.735950, f_score 0.413016\n",
      "=== total 20100 match 5748\n",
      ">>> step 20200\n",
      "step 20100-20199, precision 0.288717, recall 0.738977, f_score 0.415211\n",
      "=== total 20200 match 5805\n",
      ">>> step 20300\n",
      "step 20200-20299, precision 0.289492, recall 0.741048, f_score 0.416340\n",
      "=== total 20300 match 5862\n",
      ">>> step 20400\n",
      "step 20300-20399, precision 0.290995, recall 0.742215, f_score 0.418078\n",
      "=== total 20400 match 5897\n",
      ">>> step 20500\n",
      "step 20400-20499, precision 0.293016, recall 0.742943, f_score 0.420276\n",
      "=== total 20500 match 5928\n",
      ">>> step 20600\n",
      "step 20500-20599, precision 0.294809, recall 0.743644, f_score 0.422230\n",
      "=== total 20600 match 5953\n",
      ">>> step 20700\n",
      "step 20600-20699, precision 0.294226, recall 0.743970, f_score 0.421684\n",
      "=== total 20700 match 5975\n",
      ">>> step 20800\n",
      "step 20700-20799, precision 0.294029, recall 0.743970, f_score 0.421482\n",
      "=== total 20800 match 5979\n",
      ">>> step 20900\n",
      "step 20800-20899, precision 0.293688, recall 0.741611, f_score 0.420752\n",
      "=== total 20900 match 6020\n",
      ">>> step 21000\n",
      "step 20900-20999, precision 0.293249, recall 0.741611, f_score 0.420302\n",
      "=== total 21000 match 6029\n",
      ">>> step 21100\n",
      "step 21000-21099, precision 0.294884, recall 0.742727, f_score 0.422159\n",
      "=== total 21100 match 6060\n",
      ">>> step 21200\n",
      "step 21100-21199, precision 0.292328, recall 0.742727, f_score 0.419533\n",
      "=== total 21200 match 6113\n",
      ">>> step 21300\n",
      "step 21200-21299, precision 0.293210, recall 0.743410, f_score 0.420550\n",
      "=== total 21300 match 6156\n",
      ">>> step 21400\n",
      "step 21300-21399, precision 0.291882, recall 0.743410, f_score 0.419183\n",
      "=== total 21400 match 6184\n",
      ">>> step 21500\n",
      "step 21400-21499, precision 0.291176, recall 0.743410, f_score 0.418454\n",
      "=== total 21500 match 6199\n",
      ">>> step 21600\n",
      "step 21500-21599, precision 0.291586, recall 0.743957, f_score 0.418964\n",
      "=== total 21600 match 6228\n",
      ">>> step 21700\n",
      "step 21600-21699, precision 0.289072, recall 0.743160, f_score 0.416238\n",
      "=== total 21700 match 6296\n",
      ">>> step 21800\n",
      "step 21700-21799, precision 0.288766, recall 0.743684, f_score 0.416002\n",
      "=== total 21800 match 6320\n",
      ">>> step 21900\n",
      "step 21800-21899, precision 0.287855, recall 0.743684, f_score 0.415056\n",
      "=== total 21900 match 6340\n",
      ">>> step 22000\n",
      "step 21900-21999, precision 0.287421, recall 0.743392, f_score 0.414559\n",
      "=== total 22000 match 6360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 22100\n",
      "step 22000-22099, precision 0.287060, recall 0.742787, f_score 0.414090\n",
      "=== total 22100 match 6368\n",
      ">>> step 22200\n",
      "step 22100-22199, precision 0.286745, recall 0.742787, f_score 0.413762\n",
      "=== total 22200 match 6375\n",
      ">>> step 22300\n",
      "step 22200-22299, precision 0.286341, recall 0.742787, f_score 0.413341\n",
      "=== total 22300 match 6384\n",
      ">>> step 22400\n",
      "step 22300-22399, precision 0.286362, recall 0.742498, f_score 0.413318\n",
      "=== total 22400 match 6394\n",
      ">>> step 22500\n",
      "step 22400-22499, precision 0.286273, recall 0.742498, f_score 0.413225\n",
      "=== total 22500 match 6396\n",
      ">>> step 22600\n",
      "step 22500-22599, precision 0.285714, recall 0.742926, f_score 0.412709\n",
      "=== total 22600 match 6433\n",
      ">>> step 22700\n",
      "step 22600-22699, precision 0.284696, recall 0.742926, f_score 0.411646\n",
      "=== total 22700 match 6456\n",
      ">>> step 22800\n",
      "step 22700-22799, precision 0.283905, recall 0.742926, f_score 0.410818\n",
      "=== total 22800 match 6474\n",
      ">>> step 22900\n",
      "step 22800-22899, precision 0.285451, recall 0.745192, f_score 0.412783\n",
      "=== total 22900 match 6516\n",
      ">>> step 23000\n",
      "step 22900-22999, precision 0.283839, recall 0.745192, f_score 0.411095\n",
      "=== total 23000 match 6553\n",
      ">>> step 23100\n",
      "step 23000-23099, precision 0.283666, recall 0.745192, f_score 0.410914\n",
      "=== total 23100 match 6557\n",
      ">>> step 23200\n",
      "step 23100-23199, precision 0.284067, recall 0.746114, f_score 0.411474\n",
      "=== total 23200 match 6590\n",
      ">>> step 23300\n",
      "step 23200-23299, precision 0.283122, recall 0.746114, f_score 0.410481\n",
      "=== total 23300 match 6612\n",
      ">>> step 23400\n",
      "step 23300-23399, precision 0.284469, recall 0.747531, f_score 0.412111\n",
      "=== total 23400 match 6651\n",
      ">>> step 23500\n",
      "step 23400-23499, precision 0.284412, recall 0.748035, f_score 0.412128\n",
      "=== total 23500 match 6691\n",
      ">>> step 23600\n",
      "step 23500-23599, precision 0.283185, recall 0.748035, f_score 0.410838\n",
      "=== total 23600 match 6720\n",
      ">>> step 23700\n",
      "step 23600-23699, precision 0.284552, recall 0.749415, f_score 0.412484\n",
      "=== total 23700 match 6758\n",
      ">>> step 23800\n",
      "step 23700-23799, precision 0.283670, recall 0.749415, f_score 0.411557\n",
      "=== total 23800 match 6779\n",
      ">>> step 23900\n",
      "step 23800-23899, precision 0.283002, recall 0.749415, f_score 0.410854\n",
      "=== total 23900 match 6795\n",
      ">>> step 24000\n",
      "step 23900-23999, precision 0.282628, recall 0.749415, f_score 0.410459\n",
      "=== total 24000 match 6804\n",
      ">>> step 24100\n",
      "step 24000-24099, precision 0.282040, recall 0.749515, f_score 0.409853\n",
      "=== total 24100 match 6843\n",
      ">>> step 24200\n",
      "step 24100-24199, precision 0.280238, recall 0.749515, f_score 0.407948\n",
      "=== total 24200 match 6887\n",
      ">>> step 24300\n",
      "step 24200-24299, precision 0.279087, recall 0.749030, f_score 0.406655\n",
      "=== total 24300 match 6919\n",
      ">>> step 24400\n",
      "step 24300-24399, precision 0.278698, recall 0.747971, f_score 0.406086\n",
      "=== total 24400 match 6943\n",
      ">>> step 24500\n",
      "step 24400-24499, precision 0.278438, recall 0.747886, f_score 0.405797\n",
      "=== total 24500 match 6989\n",
      ">>> step 24600\n",
      "step 24500-24599, precision 0.277841, recall 0.747886, f_score 0.405163\n",
      "=== total 24600 match 7004\n",
      ">>> step 24700\n",
      "step 24600-24699, precision 0.277762, recall 0.747886, f_score 0.405079\n",
      "=== total 24700 match 7006\n",
      ">>> step 24800\n",
      "step 24700-24799, precision 0.277643, recall 0.747886, f_score 0.404953\n",
      "=== total 24800 match 7009\n",
      ">>> step 24900\n",
      "step 24800-24899, precision 0.277280, recall 0.747316, f_score 0.404483\n",
      "=== total 24900 match 7029\n",
      ">>> step 25000\n",
      "step 24900-24999, precision 0.276831, recall 0.746840, f_score 0.403936\n",
      "=== total 25000 match 7044\n",
      ">>> step 25100\n",
      "step 25000-25099, precision 0.276958, recall 0.745892, f_score 0.403932\n",
      "=== total 25100 match 7048\n",
      ">>> step 25200\n",
      "step 25100-25199, precision 0.276644, recall 0.745892, f_score 0.403598\n",
      "=== total 25200 match 7056\n",
      ">>> step 25300\n",
      "step 25200-25299, precision 0.277778, recall 0.746297, f_score 0.404862\n",
      "=== total 25300 match 7074\n",
      ">>> step 25400\n",
      "step 25300-25399, precision 0.277356, recall 0.745276, f_score 0.404264\n",
      "=== total 25400 match 7110\n",
      ">>> step 25500\n",
      "step 25400-25499, precision 0.277505, recall 0.744921, f_score 0.404370\n",
      "=== total 25500 match 7135\n",
      ">>> step 25600\n",
      "step 25500-25599, precision 0.276367, recall 0.745017, f_score 0.403175\n",
      "=== total 25600 match 7168\n",
      ">>> step 25700\n",
      "step 25600-25699, precision 0.276981, recall 0.745223, f_score 0.403858\n",
      "=== total 25700 match 7181\n",
      ">>> step 25800\n",
      "step 25700-25799, precision 0.277624, recall 0.746741, f_score 0.404764\n",
      "=== total 25800 match 7222\n",
      ">>> step 25900\n",
      "step 25800-25899, precision 0.276247, recall 0.746741, f_score 0.403299\n",
      "=== total 25900 match 7258\n",
      ">>> step 26000\n",
      "step 25900-25999, precision 0.276209, recall 0.746741, f_score 0.403258\n",
      "=== total 26000 match 7259\n",
      ">>> step 26100\n",
      "step 26000-26099, precision 0.276209, recall 0.746741, f_score 0.403258\n",
      "=== total 26100 match 7259\n",
      ">>> step 26200\n",
      "step 26100-26199, precision 0.277075, recall 0.747497, f_score 0.404292\n",
      "=== total 26200 match 7276\n",
      ">>> step 26300\n",
      "step 26200-26299, precision 0.276112, recall 0.747591, f_score 0.403279\n",
      "=== total 26300 match 7305\n",
      ">>> step 26400\n",
      "step 26300-26399, precision 0.275439, recall 0.748058, f_score 0.402628\n",
      "=== total 26400 match 7341\n",
      ">>> step 26500\n",
      "step 26400-26499, precision 0.275102, recall 0.748058, f_score 0.402268\n",
      "=== total 26500 match 7350\n",
      ">>> step 26600\n",
      "step 26500-26599, precision 0.273910, recall 0.747781, f_score 0.400952\n",
      "=== total 26600 match 7382\n",
      ">>> step 26700\n",
      "step 26600-26699, precision 0.273428, recall 0.747505, f_score 0.400396\n",
      "=== total 26700 match 7395\n",
      ">>> step 26800\n",
      "step 26700-26799, precision 0.273022, recall 0.747228, f_score 0.399921\n",
      "=== total 26800 match 7406\n",
      ">>> step 26900\n",
      "step 26800-26899, precision 0.271471, recall 0.747322, f_score 0.398268\n",
      "=== total 26900 match 7452\n",
      ">>> step 27000\n",
      "step 26900-26999, precision 0.270998, recall 0.747046, f_score 0.397719\n",
      "=== total 27000 match 7465\n",
      ">>> step 27100\n",
      "step 27000-27099, precision 0.270998, recall 0.747046, f_score 0.397719\n",
      "=== total 27100 match 7465\n",
      ">>> step 27200\n",
      "step 27100-27199, precision 0.271139, recall 0.746603, f_score 0.397808\n",
      "=== total 27200 match 7498\n",
      ">>> step 27300\n",
      "step 27200-27299, precision 0.270670, recall 0.746603, f_score 0.397303\n",
      "=== total 27300 match 7511\n",
      ">>> step 27400\n",
      "step 27300-27399, precision 0.271101, recall 0.747261, f_score 0.397861\n",
      "=== total 27400 match 7547\n",
      ">>> step 27500\n",
      "step 27400-27499, precision 0.270132, recall 0.747361, f_score 0.396830\n",
      "=== total 27500 match 7600\n",
      ">>> step 27600\n",
      "step 27500-27599, precision 0.269795, recall 0.746734, f_score 0.396379\n",
      "=== total 27600 match 7628\n",
      ">>> step 27700\n",
      "step 27600-27699, precision 0.271038, recall 0.747923, f_score 0.397887\n",
      "=== total 27700 match 7641\n",
      ">>> step 27800\n",
      "step 27700-27799, precision 0.271403, recall 0.749013, f_score 0.398434\n",
      "=== total 27800 match 7686\n",
      ">>> step 27900\n",
      "step 27800-27899, precision 0.274441, recall 0.751149, f_score 0.402005\n",
      "=== total 27900 match 7743\n",
      ">>> step 28000\n",
      "step 27900-27999, precision 0.276034, recall 0.753497, f_score 0.404050\n",
      "=== total 28000 match 7807\n",
      ">>> step 28100\n",
      "step 28000-28099, precision 0.277029, recall 0.753894, f_score 0.405172\n",
      "=== total 28100 match 7862\n",
      ">>> step 28200\n",
      "step 28100-28199, precision 0.277239, recall 0.755319, f_score 0.405602\n",
      "=== total 28200 match 7939\n",
      ">>> step 28300\n",
      "step 28200-28299, precision 0.277896, recall 0.753830, f_score 0.406090\n",
      "=== total 28300 match 7967\n",
      ">>> step 28400\n",
      "step 28300-28399, precision 0.278519, recall 0.753711, f_score 0.406736\n",
      "=== total 28400 match 8021\n",
      ">>> step 28500\n",
      "step 28400-28499, precision 0.279654, recall 0.754421, f_score 0.408049\n",
      "=== total 28500 match 8085\n",
      ">>> step 28600\n",
      "step 28500-28599, precision 0.280375, recall 0.754398, f_score 0.408813\n",
      "=== total 28600 match 8107\n",
      ">>> step 28700\n",
      "step 28600-28699, precision 0.280270, recall 0.754218, f_score 0.408675\n",
      "=== total 28700 match 8135\n",
      ">>> step 28800\n",
      "step 28700-28799, precision 0.280171, recall 0.753953, f_score 0.408531\n",
      "=== total 28800 match 8170\n",
      ">>> step 28900\n",
      "step 28800-28899, precision 0.280616, recall 0.753034, f_score 0.408868\n",
      "=== total 28900 match 8182\n",
      ">>> step 29000\n",
      "step 28900-28999, precision 0.281193, recall 0.753670, f_score 0.409574\n",
      "=== total 29000 match 8215\n",
      ">>> step 29100\n",
      "step 29000-29099, precision 0.282427, recall 0.754448, f_score 0.410998\n",
      "=== total 29100 match 8257\n",
      ">>> step 29200\n",
      "step 29100-29199, precision 0.283267, recall 0.754014, f_score 0.411821\n",
      "=== total 29200 match 8289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 29300\n",
      "step 29200-29299, precision 0.285629, recall 0.755478, f_score 0.414532\n",
      "=== total 29300 match 8329\n",
      ">>> step 29400\n",
      "step 29300-29399, precision 0.285560, recall 0.755478, f_score 0.414460\n",
      "=== total 29400 match 8331\n",
      ">>> step 29500\n",
      "step 29400-29499, precision 0.284637, recall 0.753086, f_score 0.413128\n",
      "=== total 29500 match 8358\n",
      ">>> step 29600\n",
      "step 29500-29599, precision 0.284060, recall 0.753086, f_score 0.412520\n",
      "=== total 29600 match 8375\n",
      ">>> step 29700\n",
      "step 29600-29699, precision 0.283573, recall 0.753477, f_score 0.412065\n",
      "=== total 29700 match 8407\n",
      ">>> step 29800\n",
      "step 29700-29799, precision 0.281364, recall 0.753477, f_score 0.409728\n",
      "=== total 29800 match 8473\n",
      ">>> step 29900\n",
      "step 29800-29899, precision 0.280202, recall 0.753866, f_score 0.408551\n",
      "=== total 29900 match 8526\n",
      ">>> step 30000\n",
      "step 29900-29999, precision 0.279644, recall 0.753866, f_score 0.407958\n",
      "=== total 30000 match 8543\n",
      ">>> step 30100\n",
      "step 30000-30099, precision 0.278991, recall 0.753866, f_score 0.407262\n",
      "=== total 30100 match 8563\n",
      ">>> step 30200\n",
      "step 30100-30199, precision 0.278536, recall 0.753866, f_score 0.406777\n",
      "=== total 30200 match 8577\n",
      ">>> step 30300\n",
      "step 30200-30299, precision 0.278049, recall 0.753866, f_score 0.406258\n",
      "=== total 30300 match 8592\n",
      ">>> step 30400\n",
      "step 30300-30399, precision 0.277436, recall 0.753153, f_score 0.405499\n",
      "=== total 30400 match 8611\n",
      ">>> step 30500\n",
      "step 30400-30499, precision 0.276921, recall 0.753153, f_score 0.404950\n",
      "=== total 30500 match 8627\n",
      ">>> step 30600\n",
      "step 30500-30599, precision 0.276475, recall 0.752594, f_score 0.404392\n",
      "=== total 30600 match 8659\n",
      ">>> step 30700\n",
      "step 30600-30699, precision 0.276160, recall 0.752038, f_score 0.403974\n",
      "=== total 30700 match 8687\n",
      ">>> step 30800\n",
      "step 30700-30799, precision 0.275842, recall 0.752038, f_score 0.403634\n",
      "=== total 30800 match 8697\n",
      ">>> step 30900\n",
      "step 30800-30899, precision 0.275522, recall 0.751800, f_score 0.403257\n",
      "=== total 30900 match 8718\n",
      ">>> step 31000\n",
      "step 30900-30999, precision 0.274608, recall 0.751800, f_score 0.402278\n",
      "=== total 31000 match 8747\n",
      ">>> step 31100\n",
      "step 31000-31099, precision 0.274170, recall 0.751800, f_score 0.401807\n",
      "=== total 31100 match 8761\n",
      ">>> step 31200\n",
      "step 31100-31199, precision 0.272459, recall 0.751563, f_score 0.399933\n",
      "=== total 31200 match 8827\n",
      ">>> step 31300\n",
      "step 31200-31299, precision 0.271782, recall 0.751563, f_score 0.399203\n",
      "=== total 31300 match 8849\n",
      ">>> step 31400\n",
      "step 31300-31399, precision 0.271475, recall 0.751563, f_score 0.398872\n",
      "=== total 31400 match 8859\n",
      ">>> step 31500\n",
      "step 31400-31499, precision 0.271744, recall 0.751636, f_score 0.399173\n",
      "=== total 31500 match 8876\n",
      ">>> step 31600\n",
      "step 31500-31599, precision 0.271286, recall 0.751636, f_score 0.398678\n",
      "=== total 31600 match 8891\n",
      ">>> step 31700\n",
      "step 31600-31699, precision 0.270728, recall 0.751713, f_score 0.398086\n",
      "=== total 31700 match 8913\n",
      ">>> step 31800\n",
      "step 31700-31799, precision 0.270358, recall 0.751788, f_score 0.397696\n",
      "=== total 31800 match 8940\n",
      ">>> step 31900\n",
      "step 31800-31899, precision 0.270171, recall 0.751630, f_score 0.397472\n",
      "=== total 31900 match 8961\n",
      ">>> step 32000\n",
      "step 31900-31999, precision 0.270388, recall 0.751393, f_score 0.397673\n",
      "=== total 32000 match 8976\n",
      ">>> step 32100\n",
      "step 32000-32099, precision 0.270087, recall 0.751393, f_score 0.397348\n",
      "=== total 32100 match 8986\n",
      ">>> step 32200\n",
      "step 32100-32199, precision 0.269727, recall 0.751393, f_score 0.396958\n",
      "=== total 32200 match 8998\n",
      ">>> step 32300\n",
      "step 32200-32299, precision 0.270291, recall 0.752234, f_score 0.397687\n",
      "=== total 32300 match 9031\n",
      ">>> step 32400\n",
      "step 32300-32399, precision 0.269345, recall 0.751998, f_score 0.396629\n",
      "=== total 32400 match 9085\n",
      ">>> step 32500\n",
      "step 32400-32499, precision 0.268885, recall 0.752682, f_score 0.396225\n",
      "=== total 32500 match 9134\n",
      ">>> step 32600\n",
      "step 32500-32599, precision 0.268529, recall 0.753813, f_score 0.395994\n",
      "=== total 32600 match 9202\n",
      ">>> step 32700\n",
      "step 32600-32699, precision 0.266501, recall 0.753813, f_score 0.393785\n",
      "=== total 32700 match 9272\n",
      ">>> step 32800\n",
      "step 32700-32799, precision 0.265727, recall 0.753813, f_score 0.392939\n",
      "=== total 32800 match 9299\n",
      ">>> step 32900\n",
      "step 32800-32899, precision 0.265642, recall 0.753813, f_score 0.392846\n",
      "=== total 32900 match 9302\n",
      ">>> step 33000\n",
      "step 32900-32999, precision 0.266395, recall 0.754935, f_score 0.393822\n",
      "=== total 33000 match 9332\n",
      ">>> step 33100\n",
      "step 33000-33099, precision 0.265567, recall 0.755603, f_score 0.393006\n",
      "=== total 33100 match 9395\n",
      ">>> step 33200\n",
      "step 33100-33199, precision 0.264700, recall 0.755965, f_score 0.392105\n",
      "=== total 33200 match 9456\n",
      ">>> step 33300\n",
      "step 33200-33299, precision 0.264734, recall 0.756325, f_score 0.392191\n",
      "=== total 33300 match 9485\n",
      ">>> step 33400\n",
      "step 33300-33399, precision 0.264086, recall 0.756765, f_score 0.391538\n",
      "=== total 33400 match 9531\n",
      ">>> step 33500\n",
      "step 33400-33499, precision 0.263698, recall 0.756765, f_score 0.391112\n",
      "=== total 33500 match 9545\n",
      ">>> step 33600\n",
      "step 33500-33599, precision 0.263687, recall 0.756002, f_score 0.390997\n",
      "=== total 33600 match 9553\n",
      ">>> step 33700\n",
      "step 33600-33699, precision 0.263246, recall 0.756002, f_score 0.390512\n",
      "=== total 33700 match 9569\n",
      ">>> step 33800\n",
      "step 33700-33799, precision 0.263125, recall 0.755243, f_score 0.390278\n",
      "=== total 33800 match 9581\n",
      ">>> step 33900\n",
      "step 33800-33899, precision 0.262714, recall 0.755243, f_score 0.389825\n",
      "=== total 33900 match 9596\n",
      ">>> step 34000\n",
      "step 33900-33999, precision 0.262632, recall 0.755243, f_score 0.389735\n",
      "=== total 34000 match 9599\n",
      ">>> step 34100\n",
      "step 34000-34099, precision 0.262599, recall 0.754864, f_score 0.389649\n",
      "=== total 34100 match 9604\n",
      ">>> step 34200\n",
      "step 34100-34199, precision 0.262380, recall 0.754864, f_score 0.389408\n",
      "=== total 34200 match 9612\n",
      ">>> step 34300\n",
      "step 34200-34299, precision 0.262380, recall 0.754864, f_score 0.389408\n",
      "=== total 34300 match 9612\n",
      ">>> step 34400\n",
      "step 34300-34399, precision 0.262108, recall 0.754864, f_score 0.389107\n",
      "=== total 34400 match 9622\n",
      ">>> step 34500\n",
      "step 34400-34499, precision 0.261972, recall 0.754864, f_score 0.388957\n",
      "=== total 34500 match 9627\n",
      ">>> step 34600\n",
      "step 34500-34599, precision 0.261821, recall 0.755084, f_score 0.388820\n",
      "=== total 34600 match 9644\n",
      ">>> step 34700\n",
      "step 34600-34699, precision 0.261090, recall 0.755084, f_score 0.388014\n",
      "=== total 34700 match 9671\n",
      ">>> step 34800\n",
      "step 34700-34799, precision 0.261424, recall 0.754605, f_score 0.388320\n",
      "=== total 34800 match 9716\n",
      ">>> step 34900\n",
      "step 34800-34899, precision 0.261146, recall 0.754069, f_score 0.387942\n",
      "=== total 34900 match 9757\n",
      ">>> step 35000\n",
      "step 34900-34999, precision 0.261390, recall 0.752653, f_score 0.388023\n",
      "=== total 35000 match 9767\n",
      ">>> step 35100\n",
      "step 35000-35099, precision 0.261070, recall 0.752653, f_score 0.387670\n",
      "=== total 35100 match 9779\n",
      ">>> step 35200\n",
      "step 35100-35199, precision 0.260701, recall 0.752796, f_score 0.387282\n",
      "=== total 35200 match 9812\n",
      ">>> step 35300\n",
      "step 35200-35299, precision 0.260542, recall 0.752796, f_score 0.387107\n",
      "=== total 35300 match 9818\n",
      ">>> step 35400\n",
      "step 35300-35399, precision 0.260515, recall 0.752796, f_score 0.387077\n",
      "=== total 35400 match 9819\n",
      ">>> step 35500\n",
      "step 35400-35499, precision 0.260207, recall 0.752644, f_score 0.386717\n",
      "=== total 35500 match 9846\n",
      ">>> step 35600\n",
      "step 35500-35599, precision 0.260022, recall 0.752423, f_score 0.386484\n",
      "=== total 35600 match 9853\n",
      ">>> step 35700\n",
      "step 35600-35699, precision 0.259023, recall 0.752202, f_score 0.385350\n",
      "=== total 35700 match 9891\n",
      ">>> step 35800\n",
      "step 35700-35799, precision 0.258657, recall 0.752202, f_score 0.384945\n",
      "=== total 35800 match 9905\n",
      ">>> step 35900\n",
      "step 35800-35899, precision 0.257980, recall 0.751981, f_score 0.384166\n",
      "=== total 35900 match 9931\n",
      ">>> step 36000\n",
      "step 35900-35999, precision 0.258172, recall 0.752345, f_score 0.384425\n",
      "=== total 36000 match 9943\n",
      ">>> step 36100\n",
      "step 36000-36099, precision 0.258216, recall 0.753068, f_score 0.384569\n",
      "=== total 36100 match 9980\n",
      ">>> step 36200\n",
      "step 36100-36199, precision 0.258415, recall 0.754360, f_score 0.384958\n",
      "=== total 36200 match 10042\n",
      ">>> step 36300\n",
      "step 36200-36299, precision 0.257799, recall 0.754360, f_score 0.384274\n",
      "=== total 36300 match 10066\n",
      ">>> step 36400\n",
      "step 36300-36399, precision 0.258396, recall 0.755633, f_score 0.385102\n",
      "=== total 36400 match 10124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 36500\n",
      "step 36400-36499, precision 0.258811, recall 0.756547, f_score 0.385682\n",
      "=== total 36500 match 10158\n",
      ">>> step 36600\n",
      "step 36500-36599, precision 0.259894, recall 0.757135, f_score 0.386960\n",
      "=== total 36600 match 10208\n",
      ">>> step 36700\n",
      "step 36600-36699, precision 0.259570, recall 0.757265, f_score 0.386618\n",
      "=== total 36700 match 10240\n",
      ">>> step 36800\n",
      "step 36700-36799, precision 0.259443, recall 0.757317, f_score 0.386484\n",
      "=== total 36800 match 10272\n",
      ">>> step 36900\n",
      "step 36800-36899, precision 0.259342, recall 0.757317, f_score 0.386372\n",
      "=== total 36900 match 10276\n",
      ">>> step 37000\n",
      "step 36900-36999, precision 0.259497, recall 0.757086, f_score 0.386513\n",
      "=== total 37000 match 10293\n",
      ">>> step 37100\n",
      "step 37000-37099, precision 0.258626, recall 0.757430, f_score 0.385591\n",
      "=== total 37100 match 10347\n",
      ">>> step 37200\n",
      "step 37100-37199, precision 0.258217, recall 0.757207, f_score 0.385107\n",
      "=== total 37200 match 10375\n",
      ">>> step 37300\n",
      "step 37200-37299, precision 0.259391, recall 0.757576, f_score 0.386460\n",
      "=== total 37300 match 10409\n",
      ">>> step 37400\n",
      "step 37300-37399, precision 0.260245, recall 0.757948, f_score 0.387455\n",
      "=== total 37400 match 10444\n",
      ">>> step 37500\n",
      "step 37400-37499, precision 0.260113, recall 0.755765, f_score 0.387023\n",
      "=== total 37500 match 10457\n",
      ">>> step 37600\n",
      "step 37500-37599, precision 0.259903, recall 0.755549, f_score 0.386762\n",
      "=== total 37600 match 10477\n",
      ">>> step 37700\n",
      "step 37600-37699, precision 0.260903, recall 0.755654, f_score 0.387882\n",
      "=== total 37700 match 10502\n",
      ">>> step 37800\n",
      "step 37700-37799, precision 0.261607, recall 0.756024, f_score 0.388709\n",
      "=== total 37800 match 10554\n",
      ">>> step 37900\n",
      "step 37800-37899, precision 0.262760, recall 0.756668, f_score 0.390066\n",
      "=== total 37900 match 10580\n",
      ">>> step 38000\n",
      "step 37900-37999, precision 0.262701, recall 0.756595, f_score 0.389991\n",
      "=== total 38000 match 10590\n",
      "TOTAL 38000, precision 0.262701, recall 0.756595, f_score 0.389991\n",
      "TOTAL true = 3677\n",
      "TOTAL error rate = 0.229026\n"
     ]
    }
   ],
   "source": [
    "# TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "cubes_set = pre_process_training(\"PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 6)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37952\n",
      "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model2.ckpt\n",
      "Model restored.\n",
      ">>> step 100\n",
      "step 0-99, precision 0.210526, recall 0.666667, f_score 0.320000\n",
      "=== total 100 match 38\n",
      ">>> step 200\n",
      "step 100-199, precision 0.219178, recall 0.761905, f_score 0.340426\n",
      "=== total 200 match 73\n",
      ">>> step 300\n",
      "step 200-299, precision 0.234234, recall 0.812500, f_score 0.363636\n",
      "=== total 300 match 111\n",
      ">>> step 400\n",
      "step 300-399, precision 0.228188, recall 0.850000, f_score 0.359788\n",
      "=== total 400 match 149\n",
      ">>> step 500\n",
      "step 400-499, precision 0.227979, recall 0.880000, f_score 0.362140\n",
      "=== total 500 match 193\n",
      ">>> step 600\n",
      "step 500-599, precision 0.239316, recall 0.875000, f_score 0.375839\n",
      "=== total 600 match 234\n",
      ">>> step 700\n",
      "step 600-699, precision 0.210145, recall 0.878788, f_score 0.339181\n",
      "=== total 700 match 276\n",
      ">>> step 800\n",
      "step 700-799, precision 0.202572, recall 0.840000, f_score 0.326425\n",
      "=== total 800 match 311\n",
      ">>> step 900\n",
      "step 800-899, precision 0.205634, recall 0.848837, f_score 0.331066\n",
      "=== total 900 match 355\n",
      ">>> step 1000\n",
      "step 900-999, precision 0.210000, recall 0.848485, f_score 0.336673\n",
      "=== total 1000 match 400\n",
      ">>> step 1100\n",
      "step 1000-1099, precision 0.208520, recall 0.853211, f_score 0.335135\n",
      "=== total 1100 match 446\n",
      ">>> step 1200\n",
      "step 1100-1199, precision 0.197505, recall 0.848214, f_score 0.320405\n",
      "=== total 1200 match 481\n",
      ">>> step 1300\n",
      "step 1200-1299, precision 0.193858, recall 0.848739, f_score 0.315625\n",
      "=== total 1300 match 521\n",
      ">>> step 1400\n",
      "step 1300-1399, precision 0.185965, recall 0.854839, f_score 0.305476\n",
      "=== total 1400 match 570\n",
      ">>> step 1500\n",
      "step 1400-1499, precision 0.192496, recall 0.867647, f_score 0.315087\n",
      "=== total 1500 match 613\n",
      ">>> step 1600\n",
      "step 1500-1599, precision 0.198496, recall 0.880000, f_score 0.323926\n",
      "=== total 1600 match 665\n",
      ">>> step 1700\n",
      "step 1600-1699, precision 0.199153, recall 0.875776, f_score 0.324511\n",
      "=== total 1700 match 708\n",
      ">>> step 1800\n",
      "step 1700-1799, precision 0.197861, recall 0.870588, f_score 0.322440\n",
      "=== total 1800 match 748\n",
      ">>> step 1900\n",
      "step 1800-1899, precision 0.197970, recall 0.871508, f_score 0.322647\n",
      "=== total 1900 match 788\n",
      ">>> step 2000\n",
      "step 1900-1999, precision 0.194511, recall 0.876344, f_score 0.318359\n",
      "=== total 2000 match 838\n",
      ">>> step 2100\n",
      "step 2000-2099, precision 0.202503, recall 0.885572, f_score 0.329630\n",
      "=== total 2100 match 879\n",
      ">>> step 2200\n",
      "step 2100-2199, precision 0.199129, recall 0.888350, f_score 0.325333\n",
      "=== total 2200 match 919\n",
      ">>> step 2300\n",
      "step 2200-2299, precision 0.198958, recall 0.892523, f_score 0.325383\n",
      "=== total 2300 match 960\n",
      ">>> step 2400\n",
      "step 2300-2399, precision 0.196640, recall 0.888393, f_score 0.322006\n",
      "=== total 2400 match 1012\n",
      ">>> step 2500\n",
      "step 2400-2499, precision 0.198292, recall 0.889362, f_score 0.324282\n",
      "=== total 2500 match 1054\n",
      ">>> step 2600\n",
      "step 2500-2599, precision 0.200183, recall 0.893878, f_score 0.327110\n",
      "=== total 2600 match 1094\n",
      ">>> step 2700\n",
      "step 2600-2699, precision 0.198948, recall 0.897233, f_score 0.325681\n",
      "=== total 2700 match 1141\n",
      ">>> step 2800\n",
      "step 2700-2799, precision 0.200169, recall 0.897727, f_score 0.327348\n",
      "=== total 2800 match 1184\n",
      ">>> step 2900\n",
      "step 2800-2899, precision 0.197883, recall 0.896679, f_score 0.324216\n",
      "=== total 2900 match 1228\n",
      ">>> step 3000\n",
      "step 2900-2999, precision 0.201263, recall 0.888502, f_score 0.328185\n",
      "=== total 3000 match 1267\n",
      ">>> step 3100\n",
      "step 3000-3099, precision 0.198767, recall 0.889655, f_score 0.324937\n",
      "=== total 3100 match 1298\n",
      ">>> step 3200\n",
      "step 3100-3199, precision 0.197319, recall 0.892256, f_score 0.323171\n",
      "=== total 3200 match 1343\n",
      ">>> step 3300\n",
      "step 3200-3299, precision 0.196674, recall 0.891803, f_score 0.322275\n",
      "=== total 3300 match 1383\n",
      ">>> step 3400\n",
      "step 3300-3399, precision 0.200565, recall 0.893082, f_score 0.327566\n",
      "=== total 3400 match 1416\n",
      ">>> step 3500\n",
      "step 3400-3499, precision 0.203285, recall 0.897281, f_score 0.331473\n",
      "=== total 3500 match 1461\n",
      ">>> step 3600\n",
      "step 3500-3599, precision 0.206000, recall 0.895652, f_score 0.334959\n",
      "=== total 3600 match 1500\n",
      ">>> step 3700\n",
      "step 3600-3699, precision 0.207166, recall 0.898305, f_score 0.336686\n",
      "=== total 3700 match 1535\n",
      ">>> step 3800\n",
      "step 3700-3799, precision 0.207093, recall 0.900826, f_score 0.336766\n",
      "=== total 3800 match 1579\n",
      ">>> step 3900\n",
      "step 3800-3899, precision 0.207489, recall 0.903743, f_score 0.337494\n",
      "=== total 3900 match 1629\n",
      ">>> step 4000\n",
      "step 3900-3999, precision 0.205669, recall 0.899736, f_score 0.334806\n",
      "=== total 4000 match 1658\n",
      ">>> step 4100\n",
      "step 4000-4099, precision 0.208333, recall 0.903308, f_score 0.338579\n",
      "=== total 4100 match 1704\n",
      ">>> step 4200\n",
      "step 4100-4199, precision 0.208666, recall 0.901478, f_score 0.338889\n",
      "=== total 4200 match 1754\n",
      ">>> step 4300\n",
      "step 4200-4299, precision 0.205932, recall 0.899756, f_score 0.335155\n",
      "=== total 4300 match 1787\n",
      ">>> step 4400\n",
      "step 4300-4399, precision 0.209290, recall 0.899061, f_score 0.339539\n",
      "=== total 4400 match 1830\n",
      ">>> step 4500\n",
      "step 4400-4499, precision 0.211538, recall 0.902050, f_score 0.342709\n",
      "=== total 4500 match 1872\n",
      ">>> step 4600\n",
      "step 4500-4599, precision 0.212311, recall 0.904444, f_score 0.343895\n",
      "=== total 4600 match 1917\n",
      ">>> step 4700\n",
      "step 4600-4699, precision 0.212972, recall 0.904555, f_score 0.344771\n",
      "=== total 4700 match 1958\n",
      ">>> step 4800\n",
      "step 4700-4799, precision 0.213893, recall 0.901053, f_score 0.345719\n",
      "=== total 4800 match 2001\n",
      ">>> step 4900\n",
      "step 4800-4899, precision 0.213123, recall 0.898129, f_score 0.344498\n",
      "=== total 4900 match 2027\n",
      ">>> step 5000\n",
      "step 4900-4999, precision 0.213630, recall 0.900204, f_score 0.345312\n",
      "=== total 5000 match 2069\n",
      ">>> step 5100\n",
      "step 5000-5099, precision 0.213130, recall 0.896000, f_score 0.344350\n",
      "=== total 5100 match 2102\n",
      ">>> step 5200\n",
      "step 5100-5199, precision 0.212816, recall 0.897436, f_score 0.344045\n",
      "=== total 5200 match 2138\n",
      ">>> step 5300\n",
      "step 5200-5299, precision 0.212454, recall 0.890595, f_score 0.343068\n",
      "=== total 5300 match 2184\n",
      ">>> step 5400\n",
      "step 5300-5399, precision 0.211876, recall 0.888679, f_score 0.342172\n",
      "=== total 5400 match 2223\n",
      ">>> step 5500\n",
      "step 5400-5499, precision 0.212616, recall 0.890943, f_score 0.343305\n",
      "=== total 5500 match 2267\n",
      ">>> step 5600\n",
      "step 5500-5599, precision 0.214006, recall 0.892922, f_score 0.345263\n",
      "=== total 5600 match 2299\n",
      ">>> step 5700\n",
      "step 5600-5699, precision 0.212031, recall 0.893885, f_score 0.342759\n",
      "=== total 5700 match 2344\n",
      ">>> step 5800\n",
      "step 5700-5799, precision 0.210924, recall 0.893238, f_score 0.341264\n",
      "=== total 5800 match 2380\n",
      ">>> step 5900\n",
      "step 5800-5899, precision 0.210744, recall 0.893170, f_score 0.341023\n",
      "=== total 5900 match 2420\n",
      ">>> step 6000\n",
      "step 5900-5999, precision 0.210462, recall 0.893287, f_score 0.340663\n",
      "=== total 6000 match 2466\n",
      ">>> step 6100\n",
      "step 6000-6099, precision 0.207517, recall 0.893287, f_score 0.336794\n",
      "=== total 6100 match 2501\n",
      ">>> step 6200\n",
      "step 6100-6199, precision 0.209658, recall 0.894472, f_score 0.339695\n",
      "=== total 6200 match 2547\n",
      ">>> step 6300\n",
      "step 6200-6299, precision 0.208414, recall 0.892562, f_score 0.337922\n",
      "=== total 6300 match 2591\n",
      ">>> step 6400\n",
      "step 6300-6399, precision 0.208254, recall 0.889968, f_score 0.337527\n",
      "=== total 6400 match 2641\n",
      ">>> step 6500\n",
      "step 6400-6499, precision 0.206794, recall 0.887821, f_score 0.335453\n",
      "=== total 6500 match 2679\n",
      ">>> step 6600\n",
      "step 6500-6599, precision 0.205580, recall 0.887480, f_score 0.333830\n",
      "=== total 6600 match 2724\n",
      ">>> step 6700\n",
      "step 6600-6699, precision 0.207957, recall 0.890093, f_score 0.337145\n",
      "=== total 6700 match 2765\n",
      ">>> step 6800\n",
      "step 6700-6799, precision 0.207635, recall 0.891271, f_score 0.336806\n",
      "=== total 6800 match 2803\n",
      ">>> step 6900\n",
      "step 6800-6899, precision 0.208671, recall 0.891566, f_score 0.338189\n",
      "=== total 6900 match 2837\n",
      ">>> step 7000\n",
      "step 6900-6999, precision 0.209722, recall 0.892171, f_score 0.339612\n",
      "=== total 7000 match 2880\n",
      ">>> step 7100\n",
      "step 7000-7099, precision 0.210598, recall 0.894049, f_score 0.340897\n",
      "=== total 7100 match 2925\n",
      ">>> step 7200\n",
      "step 7100-7199, precision 0.213996, recall 0.895332, f_score 0.345430\n",
      "=== total 7200 match 2958\n",
      ">>> step 7300\n",
      "step 7200-7299, precision 0.214428, recall 0.895833, f_score 0.346030\n",
      "=== total 7300 match 3008\n",
      ">>> step 7400\n",
      "step 7300-7399, precision 0.214919, recall 0.897119, f_score 0.346766\n",
      "=== total 7400 match 3043\n",
      ">>> step 7500\n",
      "step 7400-7499, precision 0.214008, recall 0.896739, f_score 0.345550\n",
      "=== total 7500 match 3084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 7600\n",
      "step 7500-7599, precision 0.213828, recall 0.896644, f_score 0.345309\n",
      "=== total 7600 match 3124\n",
      ">>> step 7700\n",
      "step 7600-7699, precision 0.212437, recall 0.897333, f_score 0.343543\n",
      "=== total 7700 match 3168\n",
      ">>> step 7800\n",
      "step 7700-7799, precision 0.211923, recall 0.895778, f_score 0.342756\n",
      "=== total 7800 match 3204\n",
      ">>> step 7900\n",
      "step 7800-7899, precision 0.211503, recall 0.895288, f_score 0.342171\n",
      "=== total 7900 match 3234\n",
      ">>> step 8000\n",
      "step 7900-7999, precision 0.212603, recall 0.895619, f_score 0.343634\n",
      "=== total 8000 match 3269\n",
      ">>> step 8100\n",
      "step 8000-8099, precision 0.213422, recall 0.894804, f_score 0.344642\n",
      "=== total 8100 match 3308\n",
      ">>> step 8200\n",
      "step 8100-8199, precision 0.213836, recall 0.895859, f_score 0.345261\n",
      "=== total 8200 match 3339\n",
      ">>> step 8300\n",
      "step 8200-8299, precision 0.214539, recall 0.897404, f_score 0.346291\n",
      "=== total 8300 match 3384\n",
      ">>> step 8400\n",
      "step 8300-8399, precision 0.215492, recall 0.899149, f_score 0.347663\n",
      "=== total 8400 match 3434\n",
      ">>> step 8500\n",
      "step 8400-8499, precision 0.215952, recall 0.898204, f_score 0.348189\n",
      "=== total 8500 match 3473\n",
      ">>> step 8600\n",
      "step 8500-8599, precision 0.216548, recall 0.896104, f_score 0.348805\n",
      "=== total 8600 match 3505\n",
      ">>> step 8700\n",
      "step 8600-8699, precision 0.216939, recall 0.897555, f_score 0.349422\n",
      "=== total 8700 match 3554\n",
      ">>> step 8800\n",
      "step 8700-8799, precision 0.217270, recall 0.898618, f_score 0.349933\n",
      "=== total 8800 match 3590\n",
      ">>> step 8900\n",
      "step 8800-8899, precision 0.217952, recall 0.898190, f_score 0.350784\n",
      "=== total 8900 match 3643\n",
      ">>> step 9000\n",
      "step 8900-8999, precision 0.217214, recall 0.897868, f_score 0.349803\n",
      "=== total 9000 match 3683\n",
      ">>> step 9100\n",
      "step 9000-9099, precision 0.216528, recall 0.897664, f_score 0.348898\n",
      "=== total 9100 match 3727\n",
      ">>> step 9200\n",
      "step 9100-9199, precision 0.216733, recall 0.896703, f_score 0.349091\n",
      "=== total 9200 match 3765\n",
      ">>> step 9300\n",
      "step 9200-9299, precision 0.217334, recall 0.897297, f_score 0.349916\n",
      "=== total 9300 match 3819\n",
      ">>> step 9400\n",
      "step 9300-9399, precision 0.217020, recall 0.897326, f_score 0.349511\n",
      "=== total 9400 match 3866\n",
      ">>> step 9500\n",
      "step 9400-9499, precision 0.215932, recall 0.896809, f_score 0.348059\n",
      "=== total 9500 match 3904\n",
      ">>> step 9600\n",
      "step 9500-9599, precision 0.216168, recall 0.896008, f_score 0.348305\n",
      "=== total 9600 match 3946\n",
      ">>> step 9700\n",
      "step 9600-9699, precision 0.216487, recall 0.896266, f_score 0.348739\n",
      "=== total 9700 match 3991\n",
      ">>> step 9800\n",
      "step 9700-9799, precision 0.217456, recall 0.897646, f_score 0.350100\n",
      "=== total 9800 match 4033\n",
      ">>> step 9900\n",
      "step 9800-9899, precision 0.217370, recall 0.896761, f_score 0.349921\n",
      "=== total 9900 match 4076\n",
      ">>> step 10000\n",
      "step 9900-9999, precision 0.218014, recall 0.897103, f_score 0.350781\n",
      "=== total 10000 match 4119\n",
      ">>> step 10100\n",
      "step 10000-10099, precision 0.218750, recall 0.897436, f_score 0.351759\n",
      "=== total 10100 match 4160\n",
      ">>> step 10200\n",
      "step 10100-10199, precision 0.218950, recall 0.898635, f_score 0.352110\n",
      "=== total 10200 match 4211\n",
      ">>> step 10300\n",
      "step 10200-10299, precision 0.217871, recall 0.898453, f_score 0.350698\n",
      "=== total 10300 match 4264\n",
      ">>> step 10400\n",
      "step 10300-10399, precision 0.217038, recall 0.898175, f_score 0.349598\n",
      "=== total 10400 match 4308\n",
      ">>> step 10500\n",
      "step 10400-10499, precision 0.216912, recall 0.898192, f_score 0.349435\n",
      "=== total 10500 match 4352\n",
      ">>> step 10600\n",
      "step 10500-10599, precision 0.216579, recall 0.898866, f_score 0.349055\n",
      "=== total 10600 match 4391\n",
      ">>> step 10700\n",
      "step 10600-10699, precision 0.217264, recall 0.898416, f_score 0.349909\n",
      "=== total 10700 match 4437\n",
      ">>> step 10800\n",
      "step 10700-10799, precision 0.218499, recall 0.897248, f_score 0.351419\n",
      "=== total 10800 match 4476\n",
      ">>> step 10900\n",
      "step 10800-10899, precision 0.217892, recall 0.897810, f_score 0.350677\n",
      "=== total 10900 match 4516\n",
      ">>> step 11000\n",
      "step 10900-10999, precision 0.217974, recall 0.897738, f_score 0.350778\n",
      "=== total 11000 match 4551\n",
      ">>> step 11100\n",
      "step 11000-11099, precision 0.218491, recall 0.897046, f_score 0.351394\n",
      "=== total 11100 match 4586\n",
      ">>> step 11200\n",
      "step 11100-11199, precision 0.218264, recall 0.897072, f_score 0.351103\n",
      "=== total 11200 match 4632\n",
      ">>> step 11300\n",
      "step 11200-11299, precision 0.218583, recall 0.896400, f_score 0.351463\n",
      "=== total 11300 match 4671\n",
      ">>> step 11400\n",
      "step 11300-11399, precision 0.219108, recall 0.896612, f_score 0.352158\n",
      "=== total 11400 match 4710\n",
      ">>> step 11500\n",
      "step 11400-11499, precision 0.218526, recall 0.897148, f_score 0.351447\n",
      "=== total 11500 match 4750\n",
      ">>> step 11600\n",
      "step 11500-11599, precision 0.218352, recall 0.897942, f_score 0.351283\n",
      "=== total 11600 match 4795\n",
      ">>> step 11700\n",
      "step 11600-11699, precision 0.218937, recall 0.898981, f_score 0.352120\n",
      "=== total 11700 match 4837\n",
      ">>> step 11800\n",
      "step 11700-11799, precision 0.218603, recall 0.898905, f_score 0.351681\n",
      "=== total 11800 match 4881\n",
      ">>> step 11900\n",
      "step 11800-11899, precision 0.218381, recall 0.899497, f_score 0.351440\n",
      "=== total 11900 match 4918\n",
      ">>> step 12000\n",
      "step 11900-11999, precision 0.218145, recall 0.900166, f_score 0.351185\n",
      "=== total 12000 match 4960\n",
      ">>> step 12100\n",
      "step 12000-12099, precision 0.217869, recall 0.898599, f_score 0.350708\n",
      "=== total 12100 match 5003\n",
      ">>> step 12200\n",
      "step 12100-12199, precision 0.218185, recall 0.897876, f_score 0.351062\n",
      "=== total 12200 match 5037\n",
      ">>> step 12300\n",
      "step 12200-12299, precision 0.218110, recall 0.898621, f_score 0.351022\n",
      "=== total 12300 match 5080\n",
      ">>> step 12400\n",
      "step 12300-12399, precision 0.217603, recall 0.897746, f_score 0.350298\n",
      "=== total 12400 match 5124\n",
      ">>> step 12500\n",
      "step 12400-12499, precision 0.217096, recall 0.897436, f_score 0.349618\n",
      "=== total 12500 match 5159\n",
      ">>> step 12600\n",
      "step 12500-12599, precision 0.216279, recall 0.897764, f_score 0.348581\n",
      "=== total 12600 match 5197\n",
      ">>> step 12700\n",
      "step 12600-12699, precision 0.216454, recall 0.897862, f_score 0.348816\n",
      "=== total 12700 match 5239\n",
      ">>> step 12800\n",
      "step 12700-12799, precision 0.216139, recall 0.897718, f_score 0.348397\n",
      "=== total 12800 match 5279\n",
      ">>> step 12900\n",
      "step 12800-12899, precision 0.216206, recall 0.898438, f_score 0.348538\n",
      "=== total 12900 match 5319\n",
      ">>> step 13000\n",
      "step 12900-12999, precision 0.216246, recall 0.897674, f_score 0.348533\n",
      "=== total 13000 match 5355\n",
      ">>> step 13100\n",
      "step 13000-13099, precision 0.217021, recall 0.897475, f_score 0.349523\n",
      "=== total 13100 match 5405\n",
      ">>> step 13200\n",
      "step 13100-13199, precision 0.217184, recall 0.898254, f_score 0.349793\n",
      "=== total 13200 match 5447\n",
      ">>> step 13300\n",
      "step 13200-13299, precision 0.216512, recall 0.898638, f_score 0.348950\n",
      "=== total 13300 match 5487\n",
      ">>> step 13400\n",
      "step 13300-13399, precision 0.216495, recall 0.898649, f_score 0.348929\n",
      "=== total 13400 match 5529\n",
      ">>> step 13500\n",
      "step 13400-13499, precision 0.217953, recall 0.899259, f_score 0.350867\n",
      "=== total 13500 match 5570\n",
      ">>> step 13600\n",
      "step 13500-13599, precision 0.218973, recall 0.900293, f_score 0.352266\n",
      "=== total 13600 match 5608\n",
      ">>> step 13700\n",
      "step 13600-13699, precision 0.218894, recall 0.900802, f_score 0.352203\n",
      "=== total 13700 match 5642\n",
      ">>> step 13800\n",
      "step 13700-13799, precision 0.218937, recall 0.901449, f_score 0.352308\n",
      "=== total 13800 match 5682\n",
      ">>> step 13900\n",
      "step 13800-13899, precision 0.219235, recall 0.901651, f_score 0.352710\n",
      "=== total 13900 match 5729\n",
      ">>> step 14000\n",
      "step 13900-13999, precision 0.219097, recall 0.902073, f_score 0.352563\n",
      "=== total 14000 match 5760\n",
      ">>> step 14100\n",
      "step 14000-14099, precision 0.218157, recall 0.901569, f_score 0.351306\n",
      "=== total 14100 match 5794\n",
      ">>> step 14200\n",
      "step 14100-14199, precision 0.217608, recall 0.901849, f_score 0.350615\n",
      "=== total 14200 match 5827\n",
      ">>> step 14300\n",
      "step 14200-14299, precision 0.217110, recall 0.902266, f_score 0.350000\n",
      "=== total 14300 match 5868\n",
      ">>> step 14400\n",
      "step 14300-14399, precision 0.217090, recall 0.901616, f_score 0.349925\n",
      "=== total 14400 match 5910\n",
      ">>> step 14500\n",
      "step 14400-14499, precision 0.217033, recall 0.900976, f_score 0.349804\n",
      "=== total 14500 match 5953\n",
      ">>> step 14600\n",
      "step 14500-14599, precision 0.217681, recall 0.900621, f_score 0.350618\n",
      "=== total 14600 match 5995\n",
      ">>> step 14700\n",
      "step 14600-14699, precision 0.218263, recall 0.900205, f_score 0.351341\n",
      "=== total 14700 match 6034\n",
      ">>> step 14800\n",
      "step 14700-14799, precision 0.218122, recall 0.900680, f_score 0.351194\n",
      "=== total 14800 match 6070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 14900\n",
      "step 14800-14899, precision 0.218244, recall 0.900202, f_score 0.351316\n",
      "=== total 14900 match 6117\n",
      ">>> step 15000\n",
      "step 14900-14999, precision 0.218040, recall 0.900804, f_score 0.351097\n",
      "=== total 15000 match 6164\n",
      ">>> step 15100\n",
      "step 15000-15099, precision 0.217503, recall 0.901333, f_score 0.350441\n",
      "=== total 15100 match 6216\n",
      ">>> step 15200\n",
      "step 15100-15199, precision 0.217266, recall 0.901792, f_score 0.350167\n",
      "=== total 15200 match 6255\n",
      ">>> step 15300\n",
      "step 15200-15299, precision 0.217771, recall 0.900723, f_score 0.350742\n",
      "=== total 15300 match 6291\n",
      ">>> step 15400\n",
      "step 15300-15399, precision 0.218015, recall 0.900326, f_score 0.351029\n",
      "=== total 15400 match 6339\n",
      ">>> step 15500\n",
      "step 15400-15499, precision 0.217412, recall 0.900000, f_score 0.350221\n",
      "=== total 15500 match 6375\n",
      ">>> step 15600\n",
      "step 15500-15599, precision 0.216836, recall 0.900324, f_score 0.349497\n",
      "=== total 15600 match 6415\n",
      ">>> step 15700\n",
      "step 15600-15699, precision 0.216819, recall 0.900901, f_score 0.349519\n",
      "=== total 15700 match 6457\n",
      ">>> step 15800\n",
      "step 15700-15799, precision 0.217331, recall 0.899363, f_score 0.350068\n",
      "=== total 15800 match 6497\n",
      ">>> step 15900\n",
      "step 15800-15899, precision 0.216720, recall 0.899176, f_score 0.349261\n",
      "=== total 15900 match 6543\n",
      ">>> step 16000\n",
      "step 15900-15999, precision 0.216717, recall 0.899117, f_score 0.349253\n",
      "=== total 16000 match 6580\n",
      ">>> step 16100\n",
      "step 16000-16099, precision 0.217542, recall 0.898379, f_score 0.350267\n",
      "=== total 16100 match 6624\n",
      ">>> step 16200\n",
      "step 16100-16199, precision 0.217019, recall 0.898695, f_score 0.349613\n",
      "=== total 16200 match 6663\n",
      ">>> step 16300\n",
      "step 16200-16299, precision 0.216781, recall 0.898515, f_score 0.349290\n",
      "=== total 16300 match 6698\n",
      ">>> step 16400\n",
      "step 16300-16399, precision 0.216850, recall 0.898586, f_score 0.349385\n",
      "=== total 16400 match 6742\n",
      ">>> step 16500\n",
      "step 16400-16499, precision 0.217058, recall 0.898595, f_score 0.349655\n",
      "=== total 16500 match 6777\n",
      ">>> step 16600\n",
      "step 16500-16599, precision 0.217009, recall 0.899149, f_score 0.349634\n",
      "=== total 16600 match 6820\n",
      ">>> step 16700\n",
      "step 16600-16699, precision 0.217233, recall 0.898673, f_score 0.349888\n",
      "=== total 16700 match 6859\n",
      ">>> step 16800\n",
      "step 16700-16799, precision 0.217121, recall 0.898681, f_score 0.349743\n",
      "=== total 16800 match 6904\n",
      ">>> step 16900\n",
      "step 16800-16899, precision 0.216496, recall 0.898447, f_score 0.348915\n",
      "=== total 16900 match 6947\n",
      ">>> step 17000\n",
      "step 16900-16999, precision 0.216583, recall 0.899110, f_score 0.349078\n",
      "=== total 17000 match 6995\n",
      ">>> step 17100\n",
      "step 17000-17099, precision 0.216527, recall 0.899175, f_score 0.349010\n",
      "=== total 17100 match 7043\n",
      ">>> step 17200\n",
      "step 17100-17199, precision 0.216514, recall 0.899179, f_score 0.348993\n",
      "=== total 17200 match 7085\n",
      ">>> step 17300\n",
      "step 17200-17299, precision 0.216030, recall 0.899474, f_score 0.348387\n",
      "=== total 17300 match 7124\n",
      ">>> step 17400\n",
      "step 17300-17399, precision 0.215303, recall 0.899125, f_score 0.347415\n",
      "=== total 17400 match 7162\n",
      ">>> step 17500\n",
      "step 17400-17499, precision 0.214663, recall 0.899360, f_score 0.346598\n",
      "=== total 17500 match 7202\n",
      ">>> step 17600\n",
      "step 17500-17599, precision 0.214266, recall 0.899072, f_score 0.346059\n",
      "=== total 17600 match 7234\n",
      ">>> step 17700\n",
      "step 17600-17699, precision 0.213962, recall 0.899480, f_score 0.345693\n",
      "=== total 17700 match 7277\n",
      ">>> step 17800\n",
      "step 17700-17799, precision 0.214071, recall 0.899541, f_score 0.345840\n",
      "=== total 17800 match 7320\n",
      ">>> step 17900\n",
      "step 17800-17899, precision 0.214373, recall 0.899658, f_score 0.346242\n",
      "=== total 17900 match 7361\n",
      ">>> step 18000\n",
      "step 17900-17999, precision 0.214247, recall 0.899036, f_score 0.346032\n",
      "=== total 18000 match 7398\n",
      ">>> step 18100\n",
      "step 18000-18099, precision 0.214401, recall 0.899155, f_score 0.346241\n",
      "=== total 18100 match 7444\n",
      ">>> step 18200\n",
      "step 18100-18199, precision 0.214591, recall 0.899216, f_score 0.346494\n",
      "=== total 18200 match 7484\n",
      ">>> step 18300\n",
      "step 18200-18299, precision 0.215131, recall 0.899889, f_score 0.347248\n",
      "=== total 18300 match 7521\n",
      ">>> step 18400\n",
      "step 18300-18399, precision 0.214579, recall 0.898615, f_score 0.346433\n",
      "=== total 18400 match 7559\n",
      ">>> step 18500\n",
      "step 18400-18499, precision 0.214427, recall 0.898015, f_score 0.346191\n",
      "=== total 18500 match 7597\n",
      ">>> step 18600\n",
      "step 18500-18599, precision 0.214052, recall 0.898407, f_score 0.345731\n",
      "=== total 18600 match 7643\n",
      ">>> step 18700\n",
      "step 18600-18699, precision 0.213998, recall 0.898907, f_score 0.345697\n",
      "=== total 18700 match 7687\n",
      ">>> step 18800\n",
      "step 18700-18799, precision 0.213879, recall 0.898314, f_score 0.345498\n",
      "=== total 18800 match 7724\n",
      ">>> step 18900\n",
      "step 18800-18899, precision 0.213771, recall 0.898323, f_score 0.345358\n",
      "=== total 18900 match 7770\n",
      ">>> step 19000\n",
      "step 18900-18999, precision 0.213390, recall 0.897684, f_score 0.344813\n",
      "=== total 19000 match 7812\n",
      ">>> step 19100\n",
      "step 19000-19099, precision 0.213413, recall 0.897271, f_score 0.344813\n",
      "=== total 19100 match 7858\n",
      ">>> step 19200\n",
      "step 19100-19199, precision 0.213534, recall 0.897709, f_score 0.345004\n",
      "=== total 19200 match 7891\n",
      ">>> step 19300\n",
      "step 19200-19299, precision 0.213718, recall 0.898251, f_score 0.345284\n",
      "=== total 19300 match 7931\n",
      ">>> step 19400\n",
      "step 19300-19399, precision 0.214366, recall 0.899054, f_score 0.346189\n",
      "=== total 19400 match 7977\n",
      ">>> step 19500\n",
      "step 19400-19499, precision 0.213974, recall 0.898376, f_score 0.345627\n",
      "=== total 19500 match 8015\n",
      ">>> step 19600\n",
      "step 19500-19599, precision 0.213833, recall 0.897810, f_score 0.345402\n",
      "=== total 19600 match 8053\n",
      ">>> step 19700\n",
      "step 19600-19699, precision 0.214065, recall 0.897875, f_score 0.345709\n",
      "=== total 19700 match 8091\n",
      ">>> step 19800\n",
      "step 19700-19799, precision 0.214514, recall 0.898043, f_score 0.346307\n",
      "=== total 19800 match 8130\n",
      ">>> step 19900\n",
      "step 19800-19899, precision 0.214627, recall 0.898001, f_score 0.346450\n",
      "=== total 19900 match 8163\n",
      ">>> step 20000\n",
      "step 19900-19999, precision 0.214948, recall 0.898573, f_score 0.346911\n",
      "=== total 20000 match 8202\n",
      ">>> step 20100\n",
      "step 20000-20099, precision 0.214416, recall 0.898322, f_score 0.346199\n",
      "=== total 20100 match 8241\n",
      ">>> step 20200\n",
      "step 20100-20199, precision 0.214346, recall 0.898279, f_score 0.346105\n",
      "=== total 20200 match 8281\n",
      ">>> step 20300\n",
      "step 20200-20299, precision 0.214517, recall 0.898792, f_score 0.346367\n",
      "=== total 20300 match 8321\n",
      ">>> step 20400\n",
      "step 20300-20399, precision 0.214986, recall 0.899050, f_score 0.346996\n",
      "=== total 20400 match 8368\n",
      ">>> step 20500\n",
      "step 20400-20499, precision 0.214770, recall 0.899402, f_score 0.346741\n",
      "=== total 20500 match 8409\n",
      ">>> step 20600\n",
      "step 20500-20599, precision 0.214624, recall 0.899355, f_score 0.346547\n",
      "=== total 20600 match 8452\n",
      ">>> step 20700\n",
      "step 20600-20699, precision 0.214185, recall 0.899110, f_score 0.345956\n",
      "=== total 20700 match 8488\n",
      ">>> step 20800\n",
      "step 20700-20799, precision 0.213968, recall 0.899065, f_score 0.345670\n",
      "=== total 20800 match 8534\n",
      ">>> step 20900\n",
      "step 20800-20899, precision 0.213778, recall 0.899461, f_score 0.345451\n",
      "=== total 20900 match 8579\n",
      ">>> step 21000\n",
      "step 20900-20999, precision 0.213780, recall 0.899463, f_score 0.345455\n",
      "=== total 21000 match 8621\n",
      ">>> step 21100\n",
      "step 21000-21099, precision 0.213626, recall 0.898494, f_score 0.345181\n",
      "=== total 21100 match 8660\n",
      ">>> step 21200\n",
      "step 21100-21199, precision 0.213531, recall 0.898936, f_score 0.345090\n",
      "=== total 21200 match 8706\n",
      ">>> step 21300\n",
      "step 21200-21299, precision 0.213445, recall 0.898028, f_score 0.344910\n",
      "=== total 21300 match 8747\n",
      ">>> step 21400\n",
      "step 21300-21399, precision 0.213270, recall 0.898370, f_score 0.344707\n",
      "=== total 21400 match 8787\n",
      ">>> step 21500\n",
      "step 21400-21499, precision 0.212983, recall 0.898662, f_score 0.344354\n",
      "=== total 21500 match 8827\n",
      ">>> step 21600\n",
      "step 21500-21599, precision 0.212644, recall 0.898571, f_score 0.343904\n",
      "=== total 21600 match 8874\n",
      ">>> step 21700\n",
      "step 21600-21699, precision 0.212635, recall 0.898956, f_score 0.343920\n",
      "=== total 21700 match 8912\n",
      ">>> step 21800\n",
      "step 21700-21799, precision 0.212635, recall 0.898161, f_score 0.343863\n",
      "=== total 21800 match 8959\n",
      ">>> step 21900\n",
      "step 21800-21899, precision 0.212619, recall 0.898592, f_score 0.343874\n",
      "=== total 21900 match 9002\n",
      ">>> step 22000\n",
      "step 21900-21999, precision 0.212430, recall 0.898503, f_score 0.343619\n",
      "=== total 22000 match 9043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 22100\n",
      "step 22000-22099, precision 0.212328, recall 0.898045, f_score 0.343452\n",
      "=== total 22100 match 9085\n",
      ">>> step 22200\n",
      "step 22100-22199, precision 0.212400, recall 0.897685, f_score 0.343520\n",
      "=== total 22200 match 9129\n",
      ">>> step 22300\n",
      "step 22200-22299, precision 0.212082, recall 0.896727, f_score 0.343034\n",
      "=== total 22300 match 9171\n",
      ">>> step 22400\n",
      "step 22300-22399, precision 0.211766, recall 0.896599, f_score 0.342611\n",
      "=== total 22400 match 9213\n",
      ">>> step 22500\n",
      "step 22400-22499, precision 0.212102, recall 0.897166, f_score 0.343092\n",
      "=== total 22500 match 9255\n",
      ">>> step 22600\n",
      "step 22500-22599, precision 0.212405, recall 0.897774, f_score 0.343533\n",
      "=== total 22600 match 9303\n",
      ">>> step 22700\n",
      "step 22600-22699, precision 0.212818, recall 0.897968, f_score 0.344088\n",
      "=== total 22700 match 9346\n",
      ">>> step 22800\n",
      "step 22700-22799, precision 0.212612, recall 0.897886, f_score 0.343812\n",
      "=== total 22800 match 9388\n",
      ">>> step 22900\n",
      "step 22800-22899, precision 0.212356, recall 0.897849, f_score 0.343474\n",
      "=== total 22900 match 9437\n",
      ">>> step 23000\n",
      "step 22900-22999, precision 0.212281, recall 0.897413, f_score 0.343345\n",
      "=== total 23000 match 9478\n",
      ">>> step 23100\n",
      "step 23000-23099, precision 0.212701, recall 0.897516, f_score 0.343901\n",
      "=== total 23100 match 9511\n",
      ">>> step 23200\n",
      "step 23100-23199, precision 0.212813, recall 0.897176, f_score 0.344022\n",
      "=== total 23200 match 9553\n",
      ">>> step 23300\n",
      "step 23200-23299, precision 0.212976, recall 0.897323, f_score 0.344247\n",
      "=== total 23300 match 9602\n",
      ">>> step 23400\n",
      "step 23300-23399, precision 0.212539, recall 0.896416, f_score 0.343609\n",
      "=== total 23400 match 9650\n",
      ">>> step 23500\n",
      "step 23400-23499, precision 0.212687, recall 0.896912, f_score 0.343839\n",
      "=== total 23500 match 9695\n",
      ">>> step 23600\n",
      "step 23500-23599, precision 0.212604, recall 0.896791, f_score 0.343721\n",
      "=== total 23600 match 9727\n",
      ">>> step 23700\n",
      "step 23600-23699, precision 0.212186, recall 0.896582, f_score 0.343160\n",
      "=== total 23700 match 9765\n",
      ">>> step 23800\n",
      "step 23700-23799, precision 0.212146, recall 0.897027, f_score 0.343140\n",
      "=== total 23800 match 9814\n",
      ">>> step 23900\n",
      "step 23800-23899, precision 0.212749, recall 0.897645, f_score 0.343973\n",
      "=== total 23900 match 9852\n",
      ">>> step 24000\n",
      "step 23900-23999, precision 0.212235, recall 0.897392, f_score 0.343282\n",
      "=== total 24000 match 9890\n",
      ">>> step 24100\n",
      "step 24000-24099, precision 0.212414, recall 0.897021, f_score 0.343490\n",
      "=== total 24100 match 9924\n",
      ">>> step 24200\n",
      "step 24100-24199, precision 0.212638, recall 0.896785, f_score 0.343765\n",
      "=== total 24200 match 9970\n",
      ">>> step 24300\n",
      "step 24200-24299, precision 0.212730, recall 0.896799, f_score 0.343886\n",
      "=== total 24300 match 10008\n",
      ">>> step 24400\n",
      "step 24300-24399, precision 0.213092, recall 0.897361, f_score 0.344401\n",
      "=== total 24400 match 10052\n",
      ">>> step 24500\n",
      "step 24400-24499, precision 0.213167, recall 0.896955, f_score 0.344468\n",
      "=== total 24500 match 10086\n",
      ">>> step 24600\n",
      "step 24500-24599, precision 0.213354, recall 0.896638, f_score 0.344690\n",
      "=== total 24600 match 10124\n",
      ">>> step 24700\n",
      "step 24600-24699, precision 0.213147, recall 0.896894, f_score 0.344438\n",
      "=== total 24700 match 10162\n",
      ">>> step 24800\n",
      "step 24700-24799, precision 0.213964, recall 0.896835, f_score 0.345499\n",
      "=== total 24800 match 10198\n",
      ">>> step 24900\n",
      "step 24800-24899, precision 0.213755, recall 0.897089, f_score 0.345247\n",
      "=== total 24900 match 10236\n",
      ">>> step 25000\n",
      "step 24900-24999, precision 0.214029, recall 0.896861, f_score 0.345586\n",
      "=== total 25000 match 10279\n",
      ">>> step 25100\n",
      "step 25000-25099, precision 0.214376, recall 0.897405, f_score 0.346079\n",
      "=== total 25100 match 10323\n",
      ">>> step 25200\n",
      "step 25100-25199, precision 0.214189, recall 0.897654, f_score 0.345854\n",
      "=== total 25200 match 10360\n",
      ">>> step 25300\n",
      "step 25200-25299, precision 0.214244, recall 0.897581, f_score 0.345921\n",
      "=== total 25300 match 10390\n",
      ">>> step 25400\n",
      "step 25300-25399, precision 0.213649, recall 0.896982, f_score 0.345100\n",
      "=== total 25400 match 10433\n",
      ">>> step 25500\n",
      "step 25400-25499, precision 0.213583, recall 0.897271, f_score 0.345035\n",
      "=== total 25500 match 10469\n",
      ">>> step 25600\n",
      "step 25500-25599, precision 0.213653, recall 0.897600, f_score 0.345151\n",
      "=== total 25600 match 10503\n",
      ">>> step 25700\n",
      "step 25600-25699, precision 0.213791, recall 0.897293, f_score 0.345308\n",
      "=== total 25700 match 10543\n",
      ">>> step 25800\n",
      "step 25700-25799, precision 0.213908, recall 0.897344, f_score 0.345464\n",
      "=== total 25800 match 10584\n",
      ">>> step 25900\n",
      "step 25800-25899, precision 0.213701, recall 0.897274, f_score 0.345189\n",
      "=== total 25900 match 10627\n",
      ">>> step 26000\n",
      "step 25900-25999, precision 0.213690, recall 0.896891, f_score 0.345146\n",
      "=== total 26000 match 10665\n",
      ">>> step 26100\n",
      "step 26000-26099, precision 0.213912, recall 0.897024, f_score 0.345446\n",
      "=== total 26100 match 10710\n",
      ">>> step 26200\n",
      "step 26100-26199, precision 0.213894, recall 0.897036, f_score 0.345423\n",
      "=== total 26200 match 10753\n",
      ">>> step 26300\n",
      "step 26200-26299, precision 0.213856, recall 0.896699, f_score 0.345348\n",
      "=== total 26300 match 10797\n",
      ">>> step 26400\n",
      "step 26300-26399, precision 0.213528, recall 0.896899, f_score 0.344936\n",
      "=== total 26400 match 10837\n",
      ">>> step 26500\n",
      "step 26400-26499, precision 0.213339, recall 0.896059, f_score 0.344628\n",
      "=== total 26500 match 10870\n",
      ">>> step 26600\n",
      "step 26500-26599, precision 0.213441, recall 0.895385, f_score 0.344710\n",
      "=== total 26600 match 10907\n",
      ">>> step 26700\n",
      "step 26600-26699, precision 0.213574, recall 0.895442, f_score 0.344889\n",
      "=== total 26700 match 10947\n",
      ">>> step 26800\n",
      "step 26700-26799, precision 0.213603, recall 0.895762, f_score 0.344949\n",
      "=== total 26800 match 10983\n",
      ">>> step 26900\n",
      "step 26800-26899, precision 0.213741, recall 0.895437, f_score 0.345106\n",
      "=== total 26900 match 11018\n",
      ">>> step 27000\n",
      "step 26900-26999, precision 0.213582, recall 0.895375, f_score 0.344893\n",
      "=== total 27000 match 11059\n",
      ">>> step 27100\n",
      "step 27000-27099, precision 0.213655, recall 0.895094, f_score 0.344968\n",
      "=== total 27100 match 11102\n",
      ">>> step 27200\n",
      "step 27100-27199, precision 0.213613, recall 0.894144, f_score 0.344843\n",
      "=== total 27200 match 11151\n",
      ">>> step 27300\n",
      "step 27200-27299, precision 0.213513, recall 0.894422, f_score 0.344733\n",
      "=== total 27300 match 11189\n",
      ">>> step 27400\n",
      "step 27300-27399, precision 0.213369, recall 0.894285, f_score 0.344535\n",
      "=== total 27400 match 11220\n",
      ">>> step 27500\n",
      "step 27400-27499, precision 0.213125, recall 0.894521, f_score 0.344234\n",
      "=== total 27500 match 11261\n",
      ">>> step 27600\n",
      "step 27500-27599, precision 0.213237, recall 0.894581, f_score 0.344384\n",
      "=== total 27600 match 11302\n",
      ">>> step 27700\n",
      "step 27600-27699, precision 0.213429, recall 0.894270, f_score 0.344611\n",
      "=== total 27700 match 11334\n",
      ">>> step 27800\n",
      "step 27700-27799, precision 0.213551, recall 0.894040, f_score 0.344754\n",
      "=== total 27800 match 11379\n",
      ">>> step 27900\n",
      "step 27800-27899, precision 0.213391, recall 0.893906, f_score 0.344535\n",
      "=== total 27900 match 11411\n",
      ">>> step 28000\n",
      "step 27900-27999, precision 0.213563, recall 0.894045, f_score 0.344769\n",
      "=== total 28000 match 11458\n",
      ">>> step 28100\n",
      "step 28000-28099, precision 0.213186, recall 0.894199, f_score 0.344290\n",
      "=== total 28100 match 11497\n",
      ">>> step 28200\n",
      "step 28100-28199, precision 0.213456, recall 0.893972, f_score 0.344625\n",
      "=== total 28200 match 11534\n",
      ">>> step 28300\n",
      "step 28200-28299, precision 0.213453, recall 0.894356, f_score 0.344650\n",
      "=== total 28300 match 11581\n",
      ">>> step 28400\n",
      "step 28300-28399, precision 0.213395, recall 0.894414, f_score 0.344579\n",
      "=== total 28400 match 11631\n",
      ">>> step 28500\n",
      "step 28400-28499, precision 0.213625, recall 0.893869, f_score 0.344837\n",
      "=== total 28500 match 11670\n",
      ">>> step 28600\n",
      "step 28500-28599, precision 0.213791, recall 0.893891, f_score 0.345056\n",
      "=== total 28600 match 11703\n",
      ">>> step 28700\n",
      "step 28600-28699, precision 0.213702, recall 0.894231, f_score 0.344965\n",
      "=== total 28700 match 11750\n",
      ">>> step 28800\n",
      "step 28700-28799, precision 0.213940, recall 0.894047, f_score 0.345262\n",
      "=== total 28800 match 11793\n",
      ">>> step 28900\n",
      "step 28800-28899, precision 0.213899, recall 0.893993, f_score 0.345204\n",
      "=== total 28900 match 11828\n",
      ">>> step 29000\n",
      "step 28900-28999, precision 0.213744, recall 0.893347, f_score 0.344954\n",
      "=== total 29000 match 11874\n",
      ">>> step 29100\n",
      "step 29000-29099, precision 0.213794, recall 0.893296, f_score 0.345015\n",
      "=== total 29100 match 11904\n",
      ">>> step 29200\n",
      "step 29100-29199, precision 0.213879, recall 0.893357, f_score 0.345130\n",
      "=== total 29200 match 11946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 29300\n",
      "step 29200-29299, precision 0.213982, recall 0.893106, f_score 0.345245\n",
      "=== total 29300 match 11987\n",
      ">>> step 29400\n",
      "step 29300-29399, precision 0.213763, recall 0.893056, f_score 0.344957\n",
      "=== total 29400 match 12032\n",
      ">>> step 29500\n",
      "step 29400-29499, precision 0.213771, recall 0.893352, f_score 0.344989\n",
      "=== total 29500 match 12069\n",
      ">>> step 29600\n",
      "step 29500-29599, precision 0.213879, recall 0.893485, f_score 0.345140\n",
      "=== total 29600 match 12119\n",
      ">>> step 29700\n",
      "step 29600-29699, precision 0.213780, recall 0.893054, f_score 0.344979\n",
      "=== total 29700 match 12148\n",
      ">>> step 29800\n",
      "step 29700-29799, precision 0.214168, recall 0.893187, f_score 0.345494\n",
      "=== total 29800 match 12182\n",
      ">>> step 29900\n",
      "step 29800-29899, precision 0.214449, recall 0.893015, f_score 0.345847\n",
      "=== total 29900 match 12222\n",
      ">>> step 30000\n",
      "step 29900-29999, precision 0.214315, recall 0.893306, f_score 0.345694\n",
      "=== total 30000 match 12267\n",
      ">>> step 30100\n",
      "step 30000-30099, precision 0.214030, recall 0.893451, f_score 0.345334\n",
      "=== total 30100 match 12302\n",
      ">>> step 30200\n",
      "step 30100-30199, precision 0.213927, recall 0.893775, f_score 0.345224\n",
      "=== total 30200 match 12350\n",
      ">>> step 30300\n",
      "step 30200-30299, precision 0.214072, recall 0.893868, f_score 0.345420\n",
      "=== total 30300 match 12393\n",
      ">>> step 30400\n",
      "step 30300-30399, precision 0.213912, recall 0.893817, f_score 0.345208\n",
      "=== total 30400 match 12435\n",
      ">>> step 30500\n",
      "step 30400-30499, precision 0.213679, recall 0.893995, f_score 0.344917\n",
      "=== total 30500 match 12472\n",
      ">>> step 30600\n",
      "step 30500-30599, precision 0.213474, recall 0.893503, f_score 0.344614\n",
      "=== total 30600 match 12498\n",
      ">>> step 30700\n",
      "step 30600-30699, precision 0.213511, recall 0.893525, f_score 0.344663\n",
      "=== total 30700 match 12538\n",
      ">>> step 30800\n",
      "step 30700-30799, precision 0.213167, recall 0.893071, f_score 0.344181\n",
      "=== total 30800 match 12577\n",
      ">>> step 30900\n",
      "step 30800-30899, precision 0.213176, recall 0.892762, f_score 0.344170\n",
      "=== total 30900 match 12614\n",
      ">>> step 31000\n",
      "step 30900-30999, precision 0.213101, recall 0.892455, f_score 0.344049\n",
      "=== total 31000 match 12656\n",
      ">>> step 31100\n",
      "step 31000-31099, precision 0.212868, recall 0.892079, f_score 0.343718\n",
      "=== total 31100 match 12698\n",
      ">>> step 31200\n",
      "step 31100-31199, precision 0.212906, recall 0.892105, f_score 0.343770\n",
      "=== total 31200 match 12738\n",
      ">>> step 31300\n",
      "step 31200-31299, precision 0.213224, recall 0.892272, f_score 0.344196\n",
      "=== total 31300 match 12780\n",
      ">>> step 31400\n",
      "step 31300-31399, precision 0.213634, recall 0.892764, f_score 0.344767\n",
      "=== total 31400 match 12821\n",
      ">>> step 31500\n",
      "step 31400-31499, precision 0.213791, recall 0.892278, f_score 0.344936\n",
      "=== total 31500 match 12863\n",
      ">>> step 31600\n",
      "step 31500-31599, precision 0.213549, recall 0.892452, f_score 0.344633\n",
      "=== total 31600 match 12901\n",
      ">>> step 31700\n",
      "step 31600-31699, precision 0.213800, recall 0.892869, f_score 0.344991\n",
      "=== total 31700 match 12942\n",
      ">>> step 31800\n",
      "step 31700-31799, precision 0.214203, recall 0.892777, f_score 0.345509\n",
      "=== total 31800 match 12983\n",
      ">>> step 31900\n",
      "step 31800-31899, precision 0.214236, recall 0.893086, f_score 0.345575\n",
      "=== total 31900 match 13023\n",
      ">>> step 32000\n",
      "step 31900-31999, precision 0.214166, recall 0.893142, f_score 0.345487\n",
      "=== total 32000 match 13074\n",
      ">>> step 32100\n",
      "step 32000-32099, precision 0.213932, recall 0.893096, f_score 0.345180\n",
      "=== total 32100 match 13121\n",
      ">>> step 32200\n",
      "step 32100-32199, precision 0.214134, recall 0.893185, f_score 0.345449\n",
      "=== total 32200 match 13160\n",
      ">>> step 32300\n",
      "step 32200-32299, precision 0.214275, recall 0.892959, f_score 0.345616\n",
      "=== total 32300 match 13198\n",
      ">>> step 32400\n",
      "step 32300-32399, precision 0.214113, recall 0.892880, f_score 0.345399\n",
      "=== total 32400 match 13236\n",
      ">>> step 32500\n",
      "step 32400-32499, precision 0.213947, recall 0.893115, f_score 0.345200\n",
      "=== total 32500 match 13279\n",
      ">>> step 32600\n",
      "step 32500-32599, precision 0.214002, recall 0.893484, f_score 0.345299\n",
      "=== total 32600 match 13327\n",
      ">>> step 32700\n",
      "step 32600-32699, precision 0.213874, recall 0.893684, f_score 0.345148\n",
      "=== total 32700 match 13363\n",
      ">>> step 32800\n",
      "step 32700-32799, precision 0.214041, recall 0.893769, f_score 0.345371\n",
      "=== total 32800 match 13404\n",
      ">>> step 32900\n",
      "step 32800-32899, precision 0.213839, recall 0.893379, f_score 0.345080\n",
      "=== total 32900 match 13440\n",
      ">>> step 33000\n",
      "step 32900-32999, precision 0.213576, recall 0.892990, f_score 0.344708\n",
      "=== total 33000 match 13480\n",
      ">>> step 33100\n",
      "step 33000-33099, precision 0.213451, recall 0.892912, f_score 0.344539\n",
      "=== total 33100 match 13516\n",
      ">>> step 33200\n",
      "step 33100-33199, precision 0.213701, recall 0.893342, f_score 0.344897\n",
      "=== total 33200 match 13561\n",
      ">>> step 33300\n",
      "step 33200-33299, precision 0.213761, recall 0.893669, f_score 0.344999\n",
      "=== total 33300 match 13604\n",
      ">>> step 33400\n",
      "step 33300-33399, precision 0.213605, recall 0.893865, f_score 0.344811\n",
      "=== total 33400 match 13642\n",
      ">>> step 33500\n",
      "step 33400-33499, precision 0.213497, recall 0.893786, f_score 0.344665\n",
      "=== total 33500 match 13677\n",
      ">>> step 33600\n",
      "step 33500-33599, precision 0.213500, recall 0.893260, f_score 0.344629\n",
      "=== total 33600 match 13719\n",
      ">>> step 33700\n",
      "step 33600-33699, precision 0.213528, recall 0.893585, f_score 0.344690\n",
      "=== total 33700 match 13764\n",
      ">>> step 33800\n",
      "step 33700-33799, precision 0.213577, recall 0.893604, f_score 0.344755\n",
      "=== total 33800 match 13803\n",
      ">>> step 33900\n",
      "step 33800-33899, precision 0.213486, recall 0.893894, f_score 0.344659\n",
      "=== total 33900 match 13851\n",
      ">>> step 34000\n",
      "step 33900-33999, precision 0.213612, recall 0.894277, f_score 0.344852\n",
      "=== total 34000 match 13899\n",
      ">>> step 34100\n",
      "step 34000-34099, precision 0.213399, recall 0.894199, f_score 0.344568\n",
      "=== total 34100 match 13941\n",
      ">>> step 34200\n",
      "step 34100-34199, precision 0.213407, recall 0.893917, f_score 0.344557\n",
      "=== total 34200 match 13978\n",
      ">>> step 34300\n",
      "step 34200-34299, precision 0.213414, recall 0.893903, f_score 0.344565\n",
      "=== total 34300 match 14015\n",
      ">>> step 34400\n",
      "step 34300-34399, precision 0.213402, recall 0.893389, f_score 0.344511\n",
      "=== total 34400 match 14058\n",
      ">>> step 34500\n",
      "step 34400-34499, precision 0.213100, recall 0.893218, f_score 0.344105\n",
      "=== total 34500 match 14092\n",
      ">>> step 34600\n",
      "step 34500-34599, precision 0.213047, recall 0.893207, f_score 0.344036\n",
      "=== total 34600 match 14133\n",
      ">>> step 34700\n",
      "step 34600-34699, precision 0.212936, recall 0.892931, f_score 0.343869\n",
      "=== total 34700 match 14178\n",
      ">>> step 34800\n",
      "step 34700-34799, precision 0.213191, recall 0.893341, f_score 0.344233\n",
      "=== total 34800 match 14222\n",
      ">>> step 34900\n",
      "step 34800-34899, precision 0.212948, recall 0.893204, f_score 0.343906\n",
      "=== total 34900 match 14257\n",
      ">>> step 35000\n",
      "step 34900-34999, precision 0.212997, recall 0.893486, f_score 0.343990\n",
      "=== total 35000 match 14296\n",
      ">>> step 35100\n",
      "step 35000-35099, precision 0.213091, recall 0.893337, f_score 0.344102\n",
      "=== total 35100 match 14346\n",
      ">>> step 35200\n",
      "step 35100-35199, precision 0.213025, recall 0.892805, f_score 0.343976\n",
      "=== total 35200 match 14388\n",
      ">>> step 35300\n",
      "step 35200-35299, precision 0.213087, recall 0.892826, f_score 0.344060\n",
      "=== total 35300 match 14426\n",
      ">>> step 35400\n",
      "step 35300-35399, precision 0.212800, recall 0.892981, f_score 0.343696\n",
      "=== total 35400 match 14469\n",
      ">>> step 35500\n",
      "step 35400-35499, precision 0.212823, recall 0.893229, f_score 0.343745\n",
      "=== total 35500 match 14505\n",
      ">>> step 35600\n",
      "step 35500-35599, precision 0.212881, recall 0.893537, f_score 0.343844\n",
      "=== total 35600 match 14548\n",
      ">>> step 35700\n",
      "step 35600-35699, precision 0.212851, recall 0.893752, f_score 0.343819\n",
      "=== total 35700 match 14583\n",
      ">>> step 35800\n",
      "step 35700-35799, precision 0.212888, recall 0.893483, f_score 0.343848\n",
      "=== total 35800 match 14618\n",
      ">>> step 35900\n",
      "step 35800-35899, precision 0.212868, recall 0.893215, f_score 0.343802\n",
      "=== total 35900 match 14657\n",
      ">>> step 36000\n",
      "step 35900-35999, precision 0.212833, recall 0.892949, f_score 0.343736\n",
      "=== total 36000 match 14697\n",
      ">>> step 36100\n",
      "step 36000-36099, precision 0.212866, recall 0.893223, f_score 0.343800\n",
      "=== total 36100 match 14737\n",
      ">>> step 36200\n",
      "step 36100-36199, precision 0.212706, recall 0.893436, f_score 0.343607\n",
      "=== total 36200 match 14781\n",
      ">>> step 36300\n",
      "step 36200-36299, precision 0.212575, recall 0.893394, f_score 0.343433\n",
      "=== total 36300 match 14823\n",
      ">>> step 36400\n",
      "step 36300-36399, precision 0.212676, recall 0.893190, f_score 0.343550\n",
      "=== total 36400 match 14863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 36500\n",
      "step 36400-36499, precision 0.212733, recall 0.893239, f_score 0.343628\n",
      "=== total 36500 match 14906\n",
      ">>> step 36600\n",
      "step 36500-36599, precision 0.212599, recall 0.893228, f_score 0.343453\n",
      "=== total 36600 match 14953\n",
      ">>> step 36700\n",
      "step 36600-36699, precision 0.212823, recall 0.893557, f_score 0.343769\n",
      "=== total 36700 match 14989\n",
      ">>> step 36800\n",
      "step 36700-36799, precision 0.212631, recall 0.893706, f_score 0.343530\n",
      "=== total 36800 match 15026\n",
      ">>> step 36900\n",
      "step 36800-36899, precision 0.212598, recall 0.893694, f_score 0.343485\n",
      "=== total 36900 match 15066\n",
      ">>> step 37000\n",
      "step 36900-36999, precision 0.212593, recall 0.893682, f_score 0.343478\n",
      "=== total 37000 match 15104\n",
      ">>> step 37100\n",
      "step 37000-37099, precision 0.212838, recall 0.894036, f_score 0.343823\n",
      "=== total 37100 match 15143\n",
      ">>> step 37200\n",
      "step 37100-37199, precision 0.212766, recall 0.894241, f_score 0.343745\n",
      "=== total 37200 match 15181\n",
      ">>> step 37300\n",
      "step 37200-37299, precision 0.212812, recall 0.894010, f_score 0.343788\n",
      "=== total 37300 match 15220\n",
      ">>> step 37400\n",
      "step 37300-37399, precision 0.212685, recall 0.893969, f_score 0.343619\n",
      "=== total 37400 match 15262\n",
      ">>> step 37500\n",
      "step 37400-37499, precision 0.212717, recall 0.893985, f_score 0.343663\n",
      "=== total 37500 match 15302\n",
      ">>> step 37600\n",
      "step 37500-37599, precision 0.212884, recall 0.894031, f_score 0.343883\n",
      "=== total 37600 match 15337\n",
      ">>> step 37700\n",
      "step 37600-37699, precision 0.212771, recall 0.893745, f_score 0.343716\n",
      "=== total 37700 match 15378\n",
      ">>> step 37800\n",
      "step 37700-37799, precision 0.212553, recall 0.893919, f_score 0.343444\n",
      "=== total 37800 match 15422\n"
     ]
    }
   ],
   "source": [
    "# RE-TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "train_imgs, train_lbls = \\\n",
    "    load_train_from_disk(ROOT_FOLDER + \"train_concats3/\")\n",
    "tf.reset_default_graph()\n",
    "# model_tf(250)    \n",
    "model_tf_deep(250, 2)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45972"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8539"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[1] == 1 for x in train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.78\n",
    "step 99, training accuracy 0.84\n",
    "step 149, training accuracy 0.9\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.84\n",
    "step 349, training accuracy 0.92\n",
    "step 399, training accuracy 0.86\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.86\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.84\n",
    "step 99, training accuracy 0.94\n",
    "step 149, training accuracy 0.86\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.96\n",
    "step 399, training accuracy 0.96\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.96\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "WARNING:tensorflow:From /home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
    "Instructions for updating:\n",
    "Use `tf.global_variables_initializer` instead.\n",
    "step 49, training accuracy 0.82\n",
    "step 99, training accuracy 0.92\n",
    "step 149, training accuracy 0.72\n",
    "step 199, training accuracy 0.8\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.88\n",
    "step 349, training accuracy 0.94\n",
    "step 399, training accuracy 0.84\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.84\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.76\n",
    "step 99, training accuracy 0.82\n",
    "step 149, training accuracy 0.96\n",
    "step 199, training accuracy 0.86\n",
    "step 249, training accuracy 0.76\n",
    "step 299, training accuracy 0.82\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.88\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.88\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.88\n",
    "step 149, training accuracy 0.88\n",
    "step 199, training accuracy 0.88\n",
    "step 249, training accuracy 0.92\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.94\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.94\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.96\n",
    "step 149, training accuracy 0.84\n",
    "step 199, training accuracy 0.84\n",
    "step 249, training accuracy 0.84\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.98\n",
    "step 399, training accuracy 0.9\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.9\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    ">>> step 100\n",
    "step 0-99, precision 0.695652, recall 0.640000, f_score 0.666667\n",
    ">>> step 200\n",
    "step 100-199, precision 0.512500, recall 0.732143, f_score 0.602941\n",
    ">>> step 300\n",
    "step 200-299, precision 0.350427, recall 0.732143, f_score 0.473988\n",
    ">>> step 400\n",
    "step 300-399, precision 0.408805, recall 0.677083, f_score 0.509804\n",
    ">>> step 500\n",
    "step 400-499, precision 0.366834, recall 0.651786, f_score 0.469453\n",
    ">>> step 600\n",
    "step 500-599, precision 0.349282, recall 0.651786, f_score 0.454829\n",
    ">>> step 700\n",
    "step 600-699, precision 0.376471, recall 0.662069, f_score 0.480000\n",
    ">>> step 800\n",
    "step 700-799, precision 0.383142, recall 0.595238, f_score 0.466200\n",
    ">>> step 900\n",
    "step 800-899, precision 0.394649, recall 0.617801, f_score 0.481633\n",
    ">>> step 1000\n",
    "step 900-999, precision 0.407821, recall 0.651786, f_score 0.501718\n",
    ">>> step 1100\n",
    "step 1000-1099, precision 0.404432, recall 0.651786, f_score 0.499145\n",
    ">>> step 1200\n",
    "step 1100-1199, precision 0.423559, recall 0.657588, f_score 0.515244\n",
    ">>> step 1300\n",
    "step 1200-1299, precision 0.408983, recall 0.662835, f_score 0.505848\n",
    ">>> step 1400\n",
    "step 1300-1399, precision 0.405640, recall 0.667857, f_score 0.504723\n",
    ">>> step 1500\n",
    "step 1400-1499, precision 0.415020, recall 0.670927, f_score 0.512821\n",
    ">>> step 1600\n",
    "step 1500-1599, precision 0.402852, recall 0.672619, f_score 0.503902\n",
    ">>> step 1700\n",
    "step 1600-1699, precision 0.401681, recall 0.647696, f_score 0.495851\n",
    ">>> step 1800\n",
    "step 1700-1799, precision 0.387987, recall 0.647696, f_score 0.485279\n",
    ">>> step 1900\n",
    "step 1800-1899, precision 0.385484, recall 0.647696, f_score 0.483316\n",
    ">>> step 2000\n",
    "step 1900-1999, precision 0.388802, recall 0.647668, f_score 0.485909\n",
    ">>> step 2100\n",
    "step 2000-2099, precision 0.379161, recall 0.650124, f_score 0.478976\n",
    ">>> step 2200\n",
    "step 2100-2199, precision 0.374286, recall 0.650124, f_score 0.475068\n",
    ">>> step 2300\n",
    "step 2200-2299, precision 0.381868, recall 0.651054, f_score 0.481385\n",
    ">>> step 2400\n",
    "step 2300-2399, precision 0.392622, recall 0.659292, f_score 0.492155\n",
    ">>> step 2500\n",
    "step 2400-2499, precision 0.393604, recall 0.665281, f_score 0.494590\n",
    ">>> step 2600\n",
    "step 2500-2599, precision 0.392601, recall 0.664646, f_score 0.493623\n",
    ">>> step 2700\n",
    "step 2600-2699, precision 0.389810, recall 0.664646, f_score 0.491412\n",
    ">>> step 2800\n",
    "step 2700-2799, precision 0.384884, recall 0.655446, f_score 0.484982\n",
    ">>> step 2900\n",
    "step 2800-2899, precision 0.390572, recall 0.664122, f_score 0.491873\n",
    ">>> step 3000\n",
    "step 2900-2999, precision 0.401064, recall 0.679279, f_score 0.504348\n",
    ">>> step 3100\n",
    "step 3000-3099, precision 0.418534, recall 0.695431, f_score 0.522568\n",
    ">>> step 3200\n",
    "step 3100-3199, precision 0.430129, recall 0.704545, f_score 0.534154\n",
    ">>> step 3300\n",
    "step 3200-3299, precision 0.446036, recall 0.716258, f_score 0.549735\n",
    ">>> step 3400\n",
    "step 3300-3399, precision 0.448563, recall 0.722388, f_score 0.553459\n",
    ">>> step 3500\n",
    "step 3400-3499, precision 0.455204, recall 0.719599, f_score 0.557650\n",
    ">>> step 3600\n",
    "step 3500-3599, precision 0.456560, recall 0.718271, f_score 0.558266\n",
    ">>> step 3700\n",
    "step 3600-3699, precision 0.457841, recall 0.724000, f_score 0.560950\n",
    ">>> step 3800\n",
    "step 3700-3799, precision 0.455285, recall 0.720721, f_score 0.558047\n",
    ">>> step 3900\n",
    "step 3800-3899, precision 0.459969, recall 0.724351, f_score 0.562650\n",
    ">>> step 4000\n",
    "step 3900-3999, precision 0.445289, recall 0.724351, f_score 0.551529\n",
    ">>> step 4100\n",
    "step 4000-4099, precision 0.442598, recall 0.724351, f_score 0.549461\n",
    ">>> step 4200\n",
    "step 4100-4199, precision 0.445596, recall 0.725301, f_score 0.552040\n",
    ">>> step 4300\n",
    "step 4200-4299, precision 0.429793, recall 0.724760, f_score 0.539597\n",
    ">>> step 4400\n",
    "step 4300-4399, precision 0.426450, recall 0.724760, f_score 0.536955\n",
    ">>> step 4500\n",
    "step 4400-4499, precision 0.427491, recall 0.727485, f_score 0.538528\n",
    ">>> step 4600\n",
    "step 4500-4599, precision 0.427310, recall 0.723820, f_score 0.537377\n",
    ">>> step 4700\n",
    "step 4600-4699, precision 0.422999, recall 0.723820, f_score 0.533956\n",
    ">>> step 4800\n",
    "step 4700-4799, precision 0.419893, recall 0.723820, f_score 0.531474\n",
    ">>> step 4900\n",
    "step 4800-4899, precision 0.423740, recall 0.723669, f_score 0.534504\n",
    ">>> step 5000\n",
    "step 4900-4999, precision 0.428387, recall 0.725683, f_score 0.538742\n",
    ">>> step 5100\n",
    "step 5000-5099, precision 0.417348, recall 0.725683, f_score 0.529928\n",
    ">>> step 5200\n",
    "step 5100-5199, precision 0.414482, recall 0.725683, f_score 0.527612\n",
    ">>> step 5300\n",
    "step 5200-5299, precision 0.412935, recall 0.725683, f_score 0.526358\n",
    ">>> step 5400\n",
    "step 5300-5399, precision 0.418093, recall 0.720759, f_score 0.529207\n",
    ">>> step 5500\n",
    "step 5400-5499, precision 0.409934, recall 0.720294, f_score 0.522502\n",
    ">>> step 5600\n",
    "step 5500-5599, precision 0.409280, recall 0.719665, f_score 0.521805\n",
    ">>> step 5700\n",
    "step 5600-5699, precision 0.411176, recall 0.716189, f_score 0.522422\n",
    ">>> step 5800\n",
    "step 5700-5799, precision 0.413056, recall 0.707921, f_score 0.521707\n",
    ">>> step 5900\n",
    "step 5800-5899, precision 0.408427, recall 0.701061, f_score 0.516152\n",
    ">>> step 6000\n",
    "step 5900-5999, precision 0.404113, recall 0.701061, f_score 0.512694\n",
    ">>> step 6100\n",
    "step 6000-6099, precision 0.399565, recall 0.700382, f_score 0.508839\n",
    ">>> step 6200\n",
    "step 6100-6199, precision 0.398913, recall 0.700382, f_score 0.508310\n",
    ">>> step 6300\n",
    "step 6200-6299, precision 0.402681, recall 0.704503, f_score 0.512453\n",
    ">>> step 6400\n",
    "step 6300-6399, precision 0.404178, recall 0.709441, f_score 0.514970\n",
    ">>> step 6500\n",
    "step 6400-6499, precision 0.402955, recall 0.713255, f_score 0.514974\n",
    ">>> step 6600\n",
    "step 6500-6599, precision 0.402532, recall 0.713645, f_score 0.514730\n",
    ">>> step 6700\n",
    "step 6600-6699, precision 0.413011, recall 0.712585, f_score 0.522933\n",
    ">>> step 6800\n",
    "step 6700-6799, precision 0.416506, recall 0.713813, f_score 0.526059\n",
    ">>> step 6900\n",
    "step 6800-6899, precision 0.421002, recall 0.712439, f_score 0.529253\n",
    ">>> step 7000\n",
    "step 6900-6999, precision 0.428773, recall 0.709176, f_score 0.534427\n",
    ">>> step 7100\n",
    "step 7000-7099, precision 0.430497, recall 0.704718, f_score 0.534488\n",
    ">>> step 7200\n",
    "step 7100-7199, precision 0.437929, recall 0.695273, f_score 0.537381\n",
    ">>> step 7300\n",
    "step 7200-7299, precision 0.440254, recall 0.693133, f_score 0.538483\n",
    ">>> step 7400\n",
    "step 7300-7399, precision 0.441348, recall 0.690577, f_score 0.538525\n",
    ">>> step 7500\n",
    "step 7400-7499, precision 0.439821, recall 0.688375, f_score 0.536719\n",
    ">>> step 7600\n",
    "step 7500-7599, precision 0.440693, recall 0.683196, f_score 0.535782\n",
    ">>> step 7700\n",
    "step 7600-7699, precision 0.440618, recall 0.676152, f_score 0.533547\n",
    ">>> step 7800\n",
    "step 7700-7799, precision 0.435618, recall 0.676152, f_score 0.529865\n",
    ">>> step 7900\n",
    "step 7800-7899, precision 0.431701, recall 0.674044, f_score 0.526316\n",
    ">>> step 8000\n",
    "step 7900-7999, precision 0.429482, recall 0.668651, f_score 0.523021\n",
    ">>> step 8100\n",
    "step 8000-8099, precision 0.428027, recall 0.667768, f_score 0.521672\n",
    ">>> step 8200\n",
    "step 8100-8199, precision 0.429717, recall 0.664057, f_score 0.521784\n",
    ">>> step 8300\n",
    "step 8200-8299, precision 0.428451, recall 0.664057, f_score 0.520849\n",
    ">>> step 8400\n",
    "step 8300-8399, precision 0.429589, recall 0.660438, f_score 0.520569\n",
    ">>> step 8500\n",
    "step 8400-8499, precision 0.428870, recall 0.660438, f_score 0.520041\n",
    ">>> step 8600\n",
    "step 8500-8599, precision 0.427618, recall 0.660438, f_score 0.519119\n",
    ">>> step 8700\n",
    "step 8600-8699, precision 0.424528, recall 0.655478, f_score 0.515310\n",
    ">>> step 8800\n",
    "step 8700-8799, precision 0.421074, recall 0.655478, f_score 0.512757\n",
    ">>> step 8900\n",
    "step 8800-8899, precision 0.420032, recall 0.650814, f_score 0.510555\n",
    ">>> step 9000\n",
    "step 8900-8999, precision 0.415667, recall 0.650814, f_score 0.507317\n",
    ">>> step 9100\n",
    "step 9000-9099, precision 0.414201, recall 0.646154, f_score 0.504808\n",
    ">>> step 9200\n",
    "step 9100-9199, precision 0.414510, recall 0.642944, f_score 0.504053\n",
    ">>> step 9300\n",
    "step 9200-9299, precision 0.413793, recall 0.639138, f_score 0.502352\n",
    ">>> step 9400\n",
    "step 9300-9399, precision 0.408101, recall 0.639138, f_score 0.498134\n",
    ">>> step 9500\n",
    "step 9400-9499, precision 0.407789, recall 0.639138, f_score 0.497902\n",
    ">>> step 9600\n",
    "step 9500-9599, precision 0.408248, recall 0.639218, f_score 0.498268\n",
    ">>> step 9700\n",
    "step 9600-9699, precision 0.406097, recall 0.639218, f_score 0.496663\n",
    ">>> step 9800\n",
    "step 9700-9799, precision 0.408482, recall 0.639487, f_score 0.498524\n",
    ">>> step 9900\n",
    "step 9800-9899, precision 0.407298, recall 0.639098, f_score 0.497524\n",
    ">>> step 10000\n",
    "step 9900-9999, precision 0.406250, recall 0.639098, f_score 0.496741\n",
    ">>> step 10100\n",
    "step 10000-10099, precision 0.407407, recall 0.636312, f_score 0.496758\n",
    ">>> step 10200\n",
    "step 10100-10199, precision 0.408892, recall 0.636415, f_score 0.497892\n",
    ">>> step 10300\n",
    "step 10200-10299, precision 0.407852, recall 0.636415, f_score 0.497120\n",
    ">>> step 10400\n",
    "step 10300-10399, precision 0.408796, recall 0.637079, f_score 0.498024\n",
    ">>> step 10500\n",
    "step 10400-10499, precision 0.406452, recall 0.637079, f_score 0.496280\n",
    ">>> step 10600\n",
    "step 10500-10599, precision 0.405540, recall 0.637989, f_score 0.495875\n",
    ">>> step 10700\n",
    "step 10600-10699, precision 0.403583, recall 0.636918, f_score 0.494087\n",
    ">>> step 10800\n",
    "step 10700-10799, precision 0.401748, recall 0.636918, f_score 0.492710\n",
    ">>> step 10900\n",
    "step 10800-10899, precision 0.402490, recall 0.636066, f_score 0.493011\n",
    ">>> step 11000\n",
    "step 10900-10999, precision 0.400619, recall 0.633496, f_score 0.490836\n",
    ">>> step 11100\n",
    "step 11000-11099, precision 0.396798, recall 0.633496, f_score 0.487958\n",
    ">>> step 11200\n",
    "step 11100-11199, precision 0.396944, recall 0.629510, f_score 0.486880\n",
    ">>> step 11300\n",
    "step 11200-11299, precision 0.397928, recall 0.628496, f_score 0.487316\n",
    ">>> step 11400\n",
    "step 11300-11399, precision 0.398474, recall 0.625521, f_score 0.486826\n",
    ">>> step 11500\n",
    "step 11400-11499, precision 0.397233, recall 0.620690, f_score 0.484435\n",
    ">>> step 11600\n",
    "step 11500-11599, precision 0.396189, recall 0.620690, f_score 0.483658\n",
    ">>> step 11700\n",
    "step 11600-11699, precision 0.397906, recall 0.620725, f_score 0.484945\n",
    ">>> step 11800\n",
    "step 11700-11799, precision 0.397856, recall 0.618999, f_score 0.484381\n",
    ">>> step 11900\n",
    "step 11800-11899, precision 0.396184, recall 0.618999, f_score 0.483139\n",
    ">>> step 12000\n",
    "step 11900-11999, precision 0.394779, recall 0.618999, f_score 0.482094\n",
    ">>> step 12100\n",
    "step 12000-12099, precision 0.397894, recall 0.619473, f_score 0.484554\n",
    ">>> step 12200\n",
    "step 12100-12199, precision 0.402704, recall 0.621242, f_score 0.488652\n",
    ">>> step 12300\n",
    "step 12200-12299, precision 0.408271, recall 0.618464, f_score 0.491852\n",
    ">>> step 12400\n",
    "step 12300-12399, precision 0.411149, recall 0.615207, f_score 0.492893\n",
    ">>> step 12500\n",
    "step 12400-12499, precision 0.414962, recall 0.612991, f_score 0.494902\n",
    ">>> step 12600\n",
    "step 12500-12599, precision 0.416262, recall 0.606275, f_score 0.493614\n",
    ">>> step 12700\n",
    "step 12600-12699, precision 0.419520, recall 0.603456, f_score 0.494951\n",
    ">>> step 12800\n",
    "step 12700-12799, precision 0.418013, recall 0.603456, f_score 0.493901\n",
    ">>> step 12900\n",
    "step 12800-12899, precision 0.417014, recall 0.600943, f_score 0.492362\n",
    ">>> step 13000\n",
    "step 12900-12999, precision 0.412596, recall 0.600943, f_score 0.489269\n",
    ">>> step 13100\n",
    "step 13000-13099, precision 0.410480, recall 0.599745, f_score 0.487383\n",
    ">>> step 13200\n",
    "step 13100-13199, precision 0.408577, recall 0.599745, f_score 0.486039\n",
    ">>> step 13300\n",
    "step 13200-13299, precision 0.407397, recall 0.599745, f_score 0.485203\n",
    ">>> step 13400\n",
    "step 13300-13399, precision 0.406178, recall 0.599409, f_score 0.484228\n",
    ">>> step 13500\n",
    "step 13400-13499, precision 0.401925, recall 0.599409, f_score 0.481193\n",
    ">>> step 13600\n",
    "step 13500-13599, precision 0.401584, recall 0.599409, f_score 0.480948\n",
    ">>> step 13700\n",
    "step 13600-13699, precision 0.400391, recall 0.600335, f_score 0.480389\n",
    ">>> step 13800\n",
    "step 13700-13799, precision 0.399224, recall 0.598753, f_score 0.479042\n",
    ">>> step 13900\n",
    "step 13800-13899, precision 0.395553, recall 0.598919, f_score 0.476442\n",
    ">>> step 14000\n",
    "step 13900-13999, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14100\n",
    "step 14000-14099, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14200\n",
    "step 14100-14199, precision 0.392741, recall 0.598432, f_score 0.474244\n",
    ">>> step 14300\n",
    "step 14200-14299, precision 0.393562, recall 0.597536, f_score 0.474560\n",
    ">>> step 14400\n",
    "step 14300-14399, precision 0.390675, recall 0.594374, f_score 0.471463\n",
    ">>> step 14500\n",
    "step 14400-14499, precision 0.389944, recall 0.592924, f_score 0.470474\n",
    ">>> step 14600\n",
    "step 14500-14599, precision 0.386943, recall 0.592924, f_score 0.468283\n",
    ">>> step 14700\n",
    "step 14600-14699, precision 0.386532, recall 0.592924, f_score 0.467983\n",
    ">>> step 14800\n",
    "step 14700-14799, precision 0.387326, recall 0.595152, f_score 0.469258\n",
    ">>> step 14900\n",
    "step 14800-14899, precision 0.386903, recall 0.595343, f_score 0.469007\n",
    ">>> step 15000\n",
    "step 14900-14999, precision 0.388658, recall 0.595694, f_score 0.470403\n",
    ">>> step 15100\n",
    "step 15000-15099, precision 0.387264, recall 0.595304, f_score 0.469260\n",
    ">>> step 15200\n",
    "step 15100-15199, precision 0.386923, recall 0.596679, f_score 0.469435\n",
    ">>> step 15300\n",
    "step 15200-15299, precision 0.387583, recall 0.597950, f_score 0.470315\n",
    ">>> step 15400\n",
    "step 15300-15399, precision 0.384228, recall 0.599214, f_score 0.468222\n",
    ">>> step 15500\n",
    "step 15400-15499, precision 0.383070, recall 0.599214, f_score 0.467361\n",
    ">>> step 15600\n",
    "step 15500-15599, precision 0.384500, recall 0.599143, f_score 0.468403\n",
    ">>> step 15700\n",
    "step 15600-15699, precision 0.386607, recall 0.601239, f_score 0.470606\n",
    ">>> step 15800\n",
    "step 15700-15799, precision 0.382607, recall 0.601239, f_score 0.467630\n",
    ">>> step 15900\n",
    "step 15800-15899, precision 0.382626, recall 0.603309, f_score 0.468269\n",
    ">>> step 16000\n",
    "step 15900-15999, precision 0.383248, recall 0.603296, f_score 0.468731\n",
    ">>> step 16100\n",
    "step 16000-16099, precision 0.382317, recall 0.603296, f_score 0.468034\n",
    ">>> step 16200\n",
    "step 16100-16199, precision 0.381946, recall 0.603296, f_score 0.467756\n",
    ">>> step 16300\n",
    "step 16200-16299, precision 0.379460, recall 0.603296, f_score 0.465887\n",
    ">>> step 16400\n",
    "step 16300-16399, precision 0.382472, recall 0.604669, f_score 0.468563\n",
    ">>> step 16500\n",
    "step 16400-16499, precision 0.385124, recall 0.605644, f_score 0.470843\n",
    ">>> step 16600\n",
    "step 16500-16599, precision 0.386226, recall 0.609174, f_score 0.472732\n",
    ">>> step 16700\n",
    "step 16600-16699, precision 0.385509, recall 0.609174, f_score 0.472195\n",
    ">>> step 16800\n",
    "step 16700-16799, precision 0.387558, recall 0.609641, f_score 0.473870\n",
    ">>> step 16900\n",
    "step 16800-16899, precision 0.389587, recall 0.611689, f_score 0.476004\n",
    ">>> step 17000\n",
    "step 16900-16999, precision 0.392534, recall 0.612858, f_score 0.478555\n",
    ">>> step 17100\n",
    "step 17000-17099, precision 0.393763, recall 0.611711, f_score 0.479115\n",
    ">>> step 17200\n",
    "step 17100-17199, precision 0.392442, recall 0.611711, f_score 0.478136\n",
    ">>> step 17300\n",
    "step 17200-17299, precision 0.394895, recall 0.614296, f_score 0.480746\n",
    ">>> step 17400\n",
    "step 17300-17399, precision 0.397101, recall 0.616644, f_score 0.483100\n",
    ">>> step 17500\n",
    "step 17400-17499, precision 0.401132, recall 0.617085, f_score 0.486208\n",
    ">>> step 17600\n",
    "step 17500-17599, precision 0.399220, recall 0.617085, f_score 0.484801\n",
    ">>> step 17700\n",
    "step 17600-17699, precision 0.400259, recall 0.617539, f_score 0.485707\n",
    ">>> step 17800\n",
    "step 17700-17799, precision 0.402402, recall 0.618734, f_score 0.487653\n",
    ">>> step 17900\n",
    "step 17800-17899, precision 0.403036, recall 0.617830, f_score 0.487836\n",
    ">>> step 18000\n",
    "step 17900-17999, precision 0.404838, recall 0.618076, f_score 0.489231\n",
    ">>> step 18100\n",
    "step 18000-18099, precision 0.407634, recall 0.618758, f_score 0.491482\n",
    ">>> step 18200\n",
    "step 18100-18199, precision 0.406473, recall 0.618682, f_score 0.490614\n",
    ">>> step 18300\n",
    "step 18200-18299, precision 0.407004, recall 0.619139, f_score 0.491144\n",
    ">>> step 18400\n",
    "step 18300-18399, precision 0.406982, recall 0.619472, f_score 0.491232\n",
    ">>> step 18500\n",
    "step 18400-18499, precision 0.405737, recall 0.619093, f_score 0.490206\n",
    ">>> step 18600\n",
    "step 18500-18599, precision 0.404076, recall 0.620025, f_score 0.489282\n",
    ">>> step 18700\n",
    "step 18600-18699, precision 0.401760, recall 0.620025, f_score 0.487581\n",
    ">>> step 18800\n",
    "step 18700-18799, precision 0.399308, recall 0.620025, f_score 0.485771\n",
    ">>> step 18900\n",
    "step 18800-18899, precision 0.398062, recall 0.620321, f_score 0.484938\n",
    ">>> step 19000\n",
    "step 18900-18999, precision 0.396462, recall 0.620321, f_score 0.483748\n",
    ">>> step 19100\n",
    "step 19000-19099, precision 0.396994, recall 0.620614, f_score 0.484234\n",
    ">>> step 19200\n",
    "step 19100-19199, precision 0.395224, recall 0.620431, f_score 0.482859\n",
    ">>> step 19300\n",
    "step 19200-19299, precision 0.393585, recall 0.620281, f_score 0.481589\n",
    ">>> step 19400\n",
    "step 19300-19399, precision 0.393508, recall 0.620281, f_score 0.481531\n",
    ">>> step 19500\n",
    "step 19400-19499, precision 0.393243, recall 0.621160, f_score 0.481597\n",
    ">>> step 19600\n",
    "step 19500-19599, precision 0.392549, recall 0.621160, f_score 0.481077\n",
    ">>> step 19700\n",
    "step 19600-19699, precision 0.392088, recall 0.621160, f_score 0.480730\n",
    ">>> step 19800\n",
    "step 19700-19799, precision 0.391704, recall 0.621160, f_score 0.480442\n",
    ">>> step 19900\n",
    "step 19800-19899, precision 0.391016, recall 0.621160, f_score 0.479923\n",
    ">>> step 20000\n",
    "step 19900-19999, precision 0.390358, recall 0.620519, f_score 0.479236\n",
    ">>> step 20100\n",
    "step 20000-20099, precision 0.390504, recall 0.620764, f_score 0.479419\n",
    ">>> step 20200\n",
    "step 20100-20199, precision 0.386887, recall 0.621114, f_score 0.476787\n",
    ">>> step 20300\n",
    "step 20200-20299, precision 0.384308, recall 0.621114, f_score 0.474824\n",
    ">>> step 20400\n",
    "step 20300-20399, precision 0.384161, recall 0.621114, f_score 0.474712\n",
    ">>> step 20500\n",
    "step 20400-20499, precision 0.383577, recall 0.621114, f_score 0.474266\n",
    ">>> step 20600\n",
    "step 20500-20599, precision 0.383660, recall 0.622861, f_score 0.474837\n",
    ">>> step 20700\n",
    "step 20600-20699, precision 0.383569, recall 0.624886, f_score 0.475355\n",
    ">>> step 20800\n",
    "step 20700-20799, precision 0.382785, recall 0.624886, f_score 0.474752\n",
    ">>> step 20900\n",
    "step 20800-20899, precision 0.384644, recall 0.627704, f_score 0.476995\n",
    ">>> step 21000\n",
    "step 20900-20999, precision 0.383092, recall 0.627704, f_score 0.475800\n",
    ">>> step 21100\n",
    "step 21000-21099, precision 0.383464, recall 0.626977, f_score 0.475878\n",
    ">>> step 21200\n",
    "step 21100-21199, precision 0.383045, recall 0.626977, f_score 0.475555\n",
    ">>> step 21300\n",
    "step 21200-21299, precision 0.383430, recall 0.626852, f_score 0.475816\n",
    ">>> step 21400\n",
    "step 21300-21399, precision 0.384685, recall 0.628119, f_score 0.477146\n",
    ">>> step 21500\n",
    "step 21400-21499, precision 0.383581, recall 0.628119, f_score 0.476296\n",
    ">>> step 21600\n",
    "step 21500-21599, precision 0.384506, recall 0.629446, f_score 0.477391\n",
    ">>> step 21700\n",
    "step 21600-21699, precision 0.382462, recall 0.629446, f_score 0.475813\n",
    ">>> step 21800\n",
    "step 21700-21799, precision 0.382716, recall 0.628439, f_score 0.475721\n",
    ">>> step 21900\n",
    "step 21800-21899, precision 0.380835, recall 0.628439, f_score 0.474265\n",
    ">>> step 22000\n",
    "step 21900-21999, precision 0.380235, recall 0.628439, f_score 0.473799\n",
    ">>> step 22100\n",
    "step 22000-22099, precision 0.380420, recall 0.629084, f_score 0.474126\n",
    ">>> step 22200\n",
    "step 22100-22199, precision 0.378838, recall 0.629758, f_score 0.473086\n",
    ">>> step 22300\n",
    "step 22200-22299, precision 0.378196, recall 0.628481, f_score 0.472225\n",
    ">>> step 22400\n",
    "step 22300-22399, precision 0.377388, recall 0.627647, f_score 0.471359\n",
    ">>> step 22500\n",
    "step 22400-22499, precision 0.377378, recall 0.626815, f_score 0.471117\n",
    ">>> step 22600\n",
    "step 22500-22599, precision 0.377937, recall 0.628183, f_score 0.471939\n",
    ">>> step 22700\n",
    "step 22600-22699, precision 0.376335, recall 0.628183, f_score 0.470688\n",
    ">>> step 22800\n",
    "step 22700-22799, precision 0.376016, recall 0.628183, f_score 0.470439\n",
    ">>> step 22900\n",
    "step 22800-22899, precision 0.375968, recall 0.629473, f_score 0.470762\n",
    ">>> step 23000\n",
    "step 22900-22999, precision 0.375210, recall 0.629473, f_score 0.470167\n",
    ">>> step 23100\n",
    "step 23000-23099, precision 0.374708, recall 0.630191, f_score 0.469973\n",
    ">>> step 23200\n",
    "step 23100-23199, precision 0.376144, recall 0.631564, f_score 0.471484\n",
    ">>> step 23300\n",
    "step 23200-23299, precision 0.376630, recall 0.632834, f_score 0.472219\n",
    ">>> step 23400\n",
    "step 23300-23399, precision 0.375021, recall 0.632834, f_score 0.470952\n",
    ">>> step 23500\n",
    "step 23400-23499, precision 0.374286, recall 0.633527, f_score 0.470564\n",
    ">>> step 23600\n",
    "step 23500-23599, precision 0.374068, recall 0.634763, f_score 0.470732\n",
    ">>> step 23700\n",
    "step 23600-23699, precision 0.373814, recall 0.635494, f_score 0.470731\n",
    ">>> step 23800\n",
    "step 23700-23799, precision 0.372496, recall 0.635494, f_score 0.469685\n",
    ">>> step 23900\n",
    "step 23800-23899, precision 0.372906, recall 0.635918, f_score 0.470127\n",
    ">>> step 24000\n",
    "step 23900-23999, precision 0.372976, recall 0.634865, f_score 0.469894\n",
    ">>> step 24100\n",
    "step 24000-24099, precision 0.373713, recall 0.634750, f_score 0.470448\n",
    ">>> step 24200\n",
    "step 24100-24199, precision 0.373123, recall 0.634750, f_score 0.469979\n",
    ">>> step 24300\n",
    "step 24200-24299, precision 0.374528, recall 0.635369, f_score 0.471263\n",
    ">>> step 24400\n",
    "step 24300-24399, precision 0.374765, recall 0.635397, f_score 0.471458"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
