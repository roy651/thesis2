{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.491919Z",
     "start_time": "2019-10-18T12:23:04.153952Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Main import block and TODO list\n",
    "\n",
    "# TODO: see how uri calculated the ridges\n",
    "\n",
    "# TODO: Perform Histogram equalization - start with it\n",
    "# TODO: \n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph \n",
    "# This will be the peakness measure for the PSD ==> The desired ridge index\n",
    "# TODO:\n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph - it's the peakness measure for the PSD\n",
    "# must select a peak above a min threshold in order to ignore noisy frequency\n",
    "# must ignore peaks above a certain threshold in order to detect meaningful frequency\n",
    "# run the PSD in moving windows every 200 px (deduced from the below PSD pointing to a freq of 1/0.02=50-> times 4= 200px)\n",
    "# and medianf the result of the windows\n",
    "# TODO:\n",
    "# Another alternative: (with Yariv)\n",
    "# Run PSD column by column - get the phase, freq, peakness and reconstruct an artificial ridge slice\n",
    "# from this - reconstruct a \"clean\" artificial ridge image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "#import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import mahotas as mh\n",
    "from mahotas import polygon\n",
    "# import pymorph as pm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "import skimage.transform as transform\n",
    "import skimage.morphology as mp\n",
    "import skimage.io as sio\n",
    "import scipy.misc as sm\n",
    "from skimage.filters import threshold_otsu, threshold_adaptive\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "from skimage import exposure\n",
    "from skimage import data, img_as_float\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from bisect import bisect_left\n",
    "import math\n",
    "import warnings\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.645963Z",
     "start_time": "2019-10-18T12:23:06.494255Z"
    },
    "code_folding": [
     0,
     28,
     31,
     49,
     57,
     71,
     77,
     83,
     98,
     107,
     122,
     140,
     157,
     175,
     276,
     286,
     296,
     306,
     318,
     323,
     328,
     335,
     357,
     364,
     373,
     410
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Utility functions\n",
    "\n",
    "# One time init\n",
    "# with open('results.csv', 'w') as csvfile:\n",
    "#     csvout = csv.writer(csvfile)\n",
    "#     csvout.writerow([\"File\", \"Model\", \"Gap\", \"Slice_size\", \"Count\", \"Precision\", \"Recall\", \"F-score\", \"True Count\", \"Error Rate\"])\n",
    "\n",
    "#BASIC CROP FRAME\n",
    "X_START = 1000\n",
    "X_END = 6000\n",
    "Y_START = 800\n",
    "Y_END = 4300\n",
    "BG_2_OBJ_RATIO = 0.91\n",
    "CUBE_SIZE = 250\n",
    "EDGE_GAP = 50\n",
    "ROOT_FOLDER = \"/media/1KGB_ILAN/papyrus/\"\n",
    "fragments_folder = \"files/fragments_train_front/\"\n",
    "# ROOT_FOLDER = \"/Users/il239838/Downloads/private/Thesis/Papyrus/PX303/files/\"\n",
    "LEARNING_RATE = 0.001\n",
    "BATCHES = 2000\n",
    "BATCH_SIZE = 60\n",
    "BREAK_VAL = 1000\n",
    "\n",
    "# Simple crop by x/y ranges\n",
    "def crop(image, ymin, ymax, xmin, xmax):\n",
    "    return image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "# returns a logical matrix of values beyond a threshld\n",
    "def thresholded(image, val): \n",
    "    return np.logical_and(*[image[...] > val  for t in enumerate([0, 0])])\n",
    "\n",
    "def find_min_max_without_orphand_pixels(nonzero_dimension, crop_filter=20):\n",
    "    sorted = np.sort(nonzero_dimension)\n",
    "    prev=-1\n",
    "    min_val = sorted[0]\n",
    "    for i, x in enumerate(sorted[:100]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            min_val = x\n",
    "        prev = x\n",
    "    prev=-1\n",
    "    max_val = sorted[-1]\n",
    "    for i, x in enumerate(sorted[-100:]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            max_val = prev\n",
    "            break\n",
    "        prev = x\n",
    "    \n",
    "    return min_val, max_val\n",
    "\n",
    "def calc_min_max_coordinates(image, crop_val=50):\n",
    "    temp = thresholded(image, crop_val)\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "def calc_min_max_coordinates_dynamic(image, cutoff=1):\n",
    "    temp = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    flat = np.sort(np.matrix.getA1(temp))\n",
    "    sum_all = np.sum(flat)\n",
    "    index = np.argmin(flat.cumsum() < (sum_all * cutoff))\n",
    "\n",
    "    temp = thresholded(temp, flat[index])\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "# initial static crop and a seondary dynamic crop based on signal2noise ratio\n",
    "def crop_full_scan(image, x_start, x_end, y_start, y_end):\n",
    "    temp = crop(image, y_start, y_end, x_start, x_end)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(temp, cutoff=BG_2_OBJ_RATIO)\n",
    "    temp = crop(image, y_start+ymin, y_start+ymax, x_start+xmin, x_start+xmax)\n",
    "    return temp\n",
    "\n",
    "def crop_thresholded(image):\n",
    "    temp = crop(image, 0, image.shape[0]-1, 0, image.shape[1]-1)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates(temp)\n",
    "    temp = crop(image, ymin, ymax, xmin, xmax)\n",
    "    return temp\n",
    "\n",
    "def read_and_crop(image_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END):\n",
    "    #if \"il239838\" in os.getcwd():\n",
    "    #    image = img.imread(ROOT_FOLDER + image_name)\n",
    "    #else:\n",
    "    #    f = urllib.request.urlopen(\"https://dl.dropboxusercontent.com/s/31b96942qdcn73k/\" + image_name)\n",
    "    #    image = img.imread(f, format='jpeg')\n",
    "    image = img.imread(ROOT_FOLDER + image_name)\n",
    "    \n",
    "    # Smart-crop the image to get rid of all the noise and redundant area\n",
    "    # return crop_full_scan(image)\n",
    "    cropped = crop_full_scan(image, x_start, x_end, y_start, y_end)\n",
    "    return exposure.equalize_adapthist(cropped, clip_limit=0.03)\n",
    "\n",
    "\n",
    "# TODO: fix performance!!! http://scikit-image.org/docs/dev/user_guide/tutorial_parallelization.html\n",
    "def combine_3_images_to_RGB(red, green, blue):\n",
    "    new_image = np.empty((blue.shape[0],blue.shape[1],3))\n",
    "    for x in range(0, blue.shape[0]):\n",
    "        for y in range(0, blue.shape[1]):\n",
    "            new_image[x,y,0] = red[x,y]\n",
    "            new_image[x,y,1] = green[x,y]\n",
    "            new_image[x,y,2] = blue[x,y]\n",
    "    return new_image\n",
    "\n",
    "def slice_image_left_edge(original, width=200, rotate=0):\n",
    "    rot = ndimage.rotate(original, rotate)\n",
    "    # Slice the left slice of the so-called \"blue\" image\n",
    "    left_edge_orig = crop(rot, 1, 1400, 1, width)\n",
    "    left_edge_orig = crop_thresholded(left_edge_orig)\n",
    "\n",
    "    # Copy to a new array so we don't thrash the origin\n",
    "    left_edge = np.empty_like (left_edge_orig)\n",
    "    np.copyto(left_edge, left_edge_orig)\n",
    "\n",
    "    # Zero down low level \"noise\" values\n",
    "    low_values_indices = left_edge < 30  # Where values are low\n",
    "    left_edge[low_values_indices] = 0  # All low values set to 0\n",
    "    return left_edge\n",
    "\n",
    "def get_best_angle_rotation(original, crop=True, width=200):\n",
    "    min_var = 99999999999\n",
    "    best_angle = -10\n",
    "    for x in range(-5,5):\n",
    "        if crop:            \n",
    "            rot_edge = slice_image_left_edge(original, width, x)\n",
    "        else:\n",
    "            rot_edge = ndimage.rotate(original, x)\n",
    "        left_var = np.var(rot_edge, axis=1)\n",
    "        # left_var = np.apply_along_axis(lambda v: np.var(v[np.nonzero(v)]), 1, rot_edge)\n",
    "        var_sum = np.sum(left_var)\n",
    "        if (var_sum < min_var):\n",
    "            min_var = var_sum\n",
    "            best_angle = x\n",
    "    print (\"best_angle=\"+str(best_angle))\n",
    "    return best_angle\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "def calc_neighbors(slice_map, col, row):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if ((col-1, row) in slice_map and slice_map[(col-1, row)] != None):\n",
    "        slice_map[(col, row)][\"left\"] = slice_map[(col-1, row)]\n",
    "        slice_map[(col-1, row)][\"right\"] = slice_map[(col, row)]\n",
    "    if ((col+1, row) in slice_map and slice_map[(col+1, row)] != None):\n",
    "        slice_map[(col, row)][\"right\"] = slice_map[(col+1, row)]\n",
    "        slice_map[(col+1, row)][\"left\"] = slice_map[(col, row)]\n",
    "    if ((col, row-1) in slice_map and slice_map[(col, row-1)] != None):\n",
    "        slice_map[(col, row)][\"top\"] = slice_map[(col, row-1)]\n",
    "        slice_map[(col, row-1)][\"bottom\"] = slice_map[(col, row)]\n",
    "    if ((col, row+1) in slice_map and slice_map[(col, row+1)] != None):\n",
    "        slice_map[(col, row)][\"bottom\"] = slice_map[(col, row+1)]\n",
    "        slice_map[(col, row+1)][\"top\"] = slice_map[(col, row)]\n",
    "    \n",
    "\n",
    "\n",
    "def VAL_create_cube(name, raw, x, y):\n",
    "    cube = {}\n",
    "    cube[\"cube\"] = raw\n",
    "    cube[\"file\"] = name\n",
    "    #     if name.find('P') == 0:\n",
    "    #         cube[\"index\"] = int(name[name.find('P')+1:name.find('P')+4]) * 1000 + int(name[name.find('Fg')+2:name.find('Fg')+5])\n",
    "    #     else:\n",
    "    # print(\"Found a ZERO index cube with the name:\"+name)\n",
    "    cube[\"index\"] = 0\n",
    "    cube[\"top_row\"] = x\n",
    "    cube[\"left_col\"] = y\n",
    "    cube[\"right_col\"] = y + CUBE_SIZE\n",
    "    return cube\n",
    "    \n",
    "\n",
    "ZERO_CUBE = VAL_create_cube(\"ZERO\", np.zeros((CUBE_SIZE, CUBE_SIZE), dtype=np.int), -1, -2)\n",
    "\n",
    "# slice an image to cubes with 250X250 pixel size\n",
    "def VAL_slice_TEAR_to_static_slices(name, cropped_original):\n",
    "    structure = {}\n",
    "    # cropped_original = cropped_original / 256 # divide by 256 to \"normalize\" between 0 and 1\n",
    "\n",
    "    x, y = cropped_original[\"cut\"].shape\n",
    "    # print (x,y)\n",
    "    n = 0\n",
    "    # see n offset to see offset in pixels on the x axis == rows. every n equals CUBE_SIZE\n",
    "    while ((n + 1) * CUBE_SIZE < x):\n",
    "        # mark the piece as narrow so the first would be counted as lastt too\n",
    "        narrow = True if ((CUBE_SIZE + (4 * EDGE_GAP)) > y) else False\n",
    "        # cut a cube of 250X250 at the FIRST column\n",
    "        start_row_px = int(np.round(n * CUBE_SIZE, -1))\n",
    "        end_row_px = int(np.round((n + 1) * CUBE_SIZE, -1))\n",
    "\n",
    "        # crop a cube on the starting edge of the current row. \n",
    "        # start with EDGE_GAP and move inwards untill we have value\n",
    "        fg_value = 0\n",
    "        iteration = 1\n",
    "        while fg_value <= 0.2 and iteration <= 5:\n",
    "            cube = (crop(cropped_original[\"cut\"], start_row_px, end_row_px, \\\n",
    "                         EDGE_GAP * iteration, CUBE_SIZE + (EDGE_GAP * iteration)))\n",
    "            iteration += 1\n",
    "            fg_value = np.median(cube)\n",
    "            \n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, start_row_px, EDGE_GAP)\n",
    "            new_cube[\"col\"] = 0 # marks that the cube is on the first col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = narrow # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + CUBE_SIZE + EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + start_row_px\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + end_row_px\n",
    "            structure[(0, n)] = new_cube\n",
    "\n",
    "        # crop a cube on the starting edge of the current row. \n",
    "        # start with EDGE_GAP and move inwards untill we have value\n",
    "        fg_value = 0\n",
    "        iteration = 1\n",
    "        while fg_value <= 0.2 and iteration <= 5:\n",
    "            cube = (crop(cropped_original[\"cut\"], start_row_px, end_row_px, \\\n",
    "                         y - CUBE_SIZE - (EDGE_GAP * iteration), y - (EDGE_GAP * iteration)))\n",
    "            iteration += 1\n",
    "            fg_value = np.median(cube)\n",
    "            \n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        # aligned with the normalization 0.2 correlates to 50\n",
    "        if np.median(cube) > 0.2:\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, start_row_px, y - CUBE_SIZE - EDGE_GAP)\n",
    "            new_cube[\"col\"] = 1 # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = not narrow # like col - marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + y - CUBE_SIZE - EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + y - EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + start_row_px\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + end_row_px\n",
    "            structure[(1, n)] = new_cube\n",
    "\n",
    "    #         m = 0\n",
    "    #         # every 250 pixels on the y axis == cols\n",
    "    #         while ((m + 1) * CUBE_SIZE < y):            \n",
    "    #             if ((m == 0) or ((m + 2) * CUBE_SIZE >= y)): # Only keep the left and right edges of the piece for matching!!\n",
    "    #                 # cut a cube of 250X250\n",
    "    #                 cube = crop(cropped_original[\"cut\"], n * CUBE_SIZE, (n + 1) * CUBE_SIZE, m * CUBE_SIZE, (m + 1) * CUBE_SIZE)\n",
    "    #                 # keep only cubes for which half of the pixels have some \"color\"\n",
    "    #                 # print(np.median(cube))\n",
    "    #                 if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "    #                     # keep the cube\n",
    "    #                     new_cube = VAL_create_cube(name, cube, n * CUBE_SIZE, m * CUBE_SIZE)\n",
    "    #                     new_cube[\"col\"] = m\n",
    "    #                     new_cube[\"row\"] = n\n",
    "    #                     new_cube[\"orig\"] = cropped_original\n",
    "    #                     new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + m * CUBE_SIZE\n",
    "    #                     new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + (m + 1) * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + n * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + (n + 1) * CUBE_SIZE\n",
    "    #                     if ((m + 2) * CUBE_SIZE >= y):\n",
    "    #                         new_cube[\"last\"] = True\n",
    "    #                     else:\n",
    "    #                         new_cube[\"last\"] = False\n",
    "    #                     structure[(m, n)] = new_cube\n",
    "    #             m += 1\n",
    "        n += 0.2 # currently set to jump in 50 px offset\n",
    "        n = np.round(n, 1)\n",
    "        \n",
    "    # this loop has to be performed only after we've established all the None cubes\n",
    "    # import pdb; pdb.set_trace()\n",
    "    for cube in structure.values():\n",
    "        # set the reference to neighbor cubes\n",
    "        if cube != None:\n",
    "            calc_neighbors(structure, cube[\"col\"], cube[\"row\"])\n",
    "            \n",
    "    # return the data structure with all the cubes and the counters of the rows and columns\n",
    "    return structure.values()\n",
    "\n",
    "def pad_above(original, above, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount), above[\"cube\"][-amount:], axis=0)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE,CUBE_SIZE+amount), axis=0)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"] - amount, original[\"left_col\"])\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"]\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"]\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"] - amount\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"] - amount\n",
    "    return cube\n",
    "\n",
    "def pad_below(original, below, amount):\n",
    "    res = np.insert(original[\"cube\"], np.full(amount, CUBE_SIZE), below[\"cube\"][:amount], axis=0)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=0)\n",
    "    cube =  VAL_create_cube(original[\"file\"], res, original[\"top_row\"] + amount, original[\"left_col\"])\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"]\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"]\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"] + amount\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"] + amount\n",
    "    return cube\n",
    "    \n",
    "def pad_left(original, left, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount, dtype=int), left[\"cube\"][:,-amount:], axis=1)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE, CUBE_SIZE+amount), axis=1)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] - amount)\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"] - amount\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"] - amount\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"]\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"]\n",
    "    return cube\n",
    "\n",
    "def pad_right(original, right, amount):\n",
    "    res = np.insert(original[\"cube\"], [CUBE_SIZE], right[\"cube\"][:,:amount], axis=1)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=1)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] + amount)\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"] + amount\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"] + amount\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"]\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"]\n",
    "    return cube\n",
    "    \n",
    "\n",
    "# \"Shave\" the right edge of the cube with <gap> pixels and pad with zeros on the left\n",
    "def shave_right(original, amount):\n",
    "    return pad_left(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# \"Shave\" the left edge of the cube with <gap> pixels and pad with zeros on the right    \n",
    "def shave_left(original, amount):\n",
    "    return pad_right(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# concatenate cubes \n",
    "def concatenate_cubes(left, right, slice_size):\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right[\"cube\"][:,:slice_size]), axis=1)\n",
    "    x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "    return con, x_delta, y_delta\n",
    "\n",
    "# concatenate cubes \n",
    "def VAL_concatenate_cubes(left, right, slice_size):\n",
    "    right_img = right[\"cube\"]\n",
    "    # next block is not relevant for training ...\n",
    "    #     # if the left cube is matched to another left cube (or right cube to another right cube) then rotate the right\n",
    "    #     # cube by 180 so we try to match it upside down, covering the option that the cube was pictured rotated\n",
    "    #     if ((left[\"col\"] == 0 and right[\"col\"] == 0) or (left[\"col\"] != 0 and right[\"col\"] != 0)):\n",
    "    #         right_img = np.rot90(right[\"cube\"], 2);\n",
    "\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right_img[:,:slice_size]), axis=1)\n",
    "\n",
    "    # next block calculates distance based on the distance between left's right-top corner and right's left-top corner    \n",
    "    #     x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    #     y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "\n",
    "    # next block calculates the distance between the centers of cubes, accounting for test set's possibility of reverse slices (left instead of right and vice versa)\n",
    "    x_delta = right[\"row_px_top\"] - left[\"row_px_top\"] # equivalent to distance between vertical centers\n",
    "    y_delta = (right[\"col_px_left\"] + (slice_size / 2)) - (left[\"col_px_right\"] - (slice_size / 2)) # measuring the distance between horizontal centers of the slices\n",
    "\n",
    "    return con, x_delta, y_delta, left[\"file\"], right[\"file\"]\n",
    "    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cube with zeros\n",
    "def concatenate_cubes_zero_pad_gaps(left_orig, right_orig, gap):\n",
    "    left = left_orig if gap == 0 else shave_right(left_orig, gap)\n",
    "    right = right_orig if gap == 0 else shave_left(right_orig, gap)\n",
    "    return concatenate_cubes(left, right)    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cobe with the nearby\n",
    "# continuation of the cubes\n",
    "def concatenate_cubes_with_gap(left_orig, right_orig, gap, left_pad, right_pad, slice_size):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    left = left_orig if gap == 0 else pad_left(left_orig, left_pad, gap)\n",
    "    right = right_orig if gap == 0 else pad_right(right_orig, right_pad, gap)\n",
    "    return concatenate_cubes(left, right, slice_size)        \n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def VAL_build_train_set_for_euclidean_distance(cubes, slice_size, folder):\n",
    "    # clean folder before starting\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    train_left_obj = []\n",
    "    train_right_obj = []\n",
    "    # iterate over all cubes   \n",
    "    for curr in cubes:\n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in cubes:\n",
    "            if (adj[\"file\"] != curr[\"file\"]): # no need to test against self CURRENTLY checking from directions!!!\n",
    "                #import pdb; pdb.set_trace()\n",
    "                # append the adjacent image to the current image\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "                output = folder+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                np.save(output, conc)\n",
    "                train_imgs.append(output)\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                train_left_obj.append(curr)\n",
    "                train_right_obj.append(adj)\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_x_delta, train_y_delta, train_left_obj, train_right_obj\n",
    "\n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def ORIG_build_train_set(cubes, gap):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    # iterate over the rows and cols, essentially going over the grid of sliced cubes\n",
    "    for row in range(0, rows):\n",
    "        for col in range(0, cols):\n",
    "            # if this cube exists (could have been removed previously due to lack of data)\n",
    "            if (cubes[(col, row)] != None):\n",
    "                # for each \"current\" image in the iteration\n",
    "                curr = cubes[(col, row)]\n",
    "                # iterate over all the cubes to find all the \"other\" (adjacent) cubes\n",
    "                for adj_row in range(0, rows):\n",
    "                    for adj_col in range(0, cols):\n",
    "                        if (adj_row != row or adj_col != col):\n",
    "                            if (cubes[(adj_col, adj_row)] != None):\n",
    "                                adj = cubes[(adj_col, adj_row)]\n",
    "                                # append the adjacent image to the current image\n",
    "                                # pass the filling cubes on the right and left to pad against the gap\n",
    "                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                    if (gap == 0):\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes(curr, adj, slice_size)\n",
    "                                    else:\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr, adj, gap, curr[\"left\"], adj[\"right\"], slice_size)\n",
    "                                    train_imgs.append(conc)\n",
    "                                    train_x_delta.append(x_delta)\n",
    "                                    train_y_delta.append(y_delta)\n",
    "                                    # if the adj image is on the same row and on the right of the curr image - it will be marked as match    \n",
    "                                    if (adj_row == row and adj_col == (col + 1)):\n",
    "                                        # mark the image as matched\n",
    "                                        train_lbls.append([0,1])\n",
    "                                        # need to enrich the set with a few more tru positive samples - so we offset \n",
    "                                        # the matched images up ad down a few times and create more matches\n",
    "                                        if (\"top\" in curr.keys() and \"top\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                                                adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"top\" in curr[\"left\"].keys() and \"top\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_above(curr[\"left\"], curr[\"left\"][\"top\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_above(adj[\"right\"], curr[\"right\"][\"top\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                                                adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"bottom\" in curr[\"left\"].keys() and \"bottom\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_below(curr[\"left\"], curr[\"left\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_below(adj[\"right\"], curr[\"right\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"left\" in curr.keys()): # enough to check only the curr as the left of the adj is the curr\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_left(curr, curr[\"left\"],i)\n",
    "                                                adj1 = pad_left(adj, adj[\"left\"],i) # essentially the curr\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_left(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_left(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"right\" in adj.keys()): # enough to check only the adj as the right of the curr is the adj\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_right(curr, curr[\"right\"],i) # essentially the adj\n",
    "                                                adj1 = pad_right(adj, adj[\"right\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_right(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_right(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                    else:\n",
    "                                        # mark the image as not matched\n",
    "                                        train_lbls.append([1,0])\n",
    "                                \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.693365Z",
     "start_time": "2019-10-18T12:23:06.647598Z"
    },
    "code_folding": [
     0,
     2,
     7,
     86,
     248,
     253,
     278,
     297
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN Utility functions 2\n",
    "SAVE_PNG=False\n",
    "def save_img(path, img):\n",
    "    np.save(path, img)  \n",
    "    if SAVE_PNG:\n",
    "        plt.imsave(path+\".png\", img, cmap=plt.cm.gray)\n",
    "                    \n",
    "def VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                              train_y_delta, is_enriched, curr, adj, tolerance_factor=0):\n",
    "    # need to enhance the set with a few more true positive samples\n",
    "    # allowing some up and down tolerance\n",
    "    if (\"top\" in curr.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "            adj1 = adj\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "            is_enriched.append(True)\n",
    "    if (\"top\" in adj.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = curr\n",
    "            adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "            is_enriched.append(True)\n",
    "    if (\"bottom\" in curr.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "            adj1 = adj\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "            is_enriched.append(True)\n",
    "    if (\"bottom\"in adj.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = curr\n",
    "            adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "            is_enriched.append(True)\n",
    "\n",
    "                            \n",
    "# IMPORTANT: enrich_factor determines how many \"duplications\" of TRUE values will we have in the train set\n",
    "# This allows for a more balanced train set however, it reduces the strictness of the matches \n",
    "# i.e. (not sure why) when we have multiple nearby \"duplicates\" matches we get much more matches in the validation\n",
    "# PARAMS: enrich_factor=1 means no enrich/duplicate, 20 means duplicate by 20, every 10 pixels\n",
    "# PARAMS: tolerance_factor=0 means only match against exact horizon, each notch equals additional 10 pixels tolerance\n",
    "def NEW_build_train_set_for_binary_labeling(cubes, adjs, slice_size, folder, enrich_factor=1, tolerance_factor=0): \n",
    "    # enrich_factor is split by 2 because it is dual-sided and 1 means actually no enrichment - i.e. 0.5\n",
    "    enrich_factor = enrich_factor / 2 \n",
    "    # clean folder before starting\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    is_enriched = []\n",
    "    discard_c = 0\n",
    "    skipped_c = 0\n",
    "    curr_idx = 0\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        curr_idx = curr_idx + 1\n",
    "        if curr_idx % 100 == 0:\n",
    "            print(\"*** CUBES=\"+str(curr_idx))\n",
    "            print(\" MATCHED=\"+str(sum(x[1] == 1 for x in train_lbls)))\n",
    "            print(\" NOT MATCHED=\"+str(sum(x[0] == 1 for x in train_lbls)))\n",
    "            print(\" DISCARDED=\"+str(discard_c))\n",
    "            print(\" SKIPPED=\"+str(skipped_c))\n",
    "            \n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in adjs:\n",
    "            # Initial filter: what CAN be matched against what?\n",
    "            # 1 - not of the same fragment (file==fragment)\n",
    "            # 2 - they ARE of the same tear - don't want to confuse the learning with false data coming from different tears\n",
    "            # 3 - no need to test against self and avoid checking from both directions\n",
    "            if  adj[\"file\"] != curr[\"file\"] and \\\n",
    "                adj[\"tear\"] == curr[\"tear\"] and \\\n",
    "                curr[\"piece_col\"] < adj[\"piece_col\"]: \n",
    "                # last condition above - actually ignores pieces of the same col but different rows\n",
    "                # the assumption is that they are either \"not-match\" and then will tilt the balance further to not-match\n",
    "                # or they are \"somewhat-matching\" but in a way that might confuse the algorithm\n",
    "                # import pdb; pdb.set_trace()\n",
    "                \n",
    "                # print(\">>> >>>\"+str(curr[\"cube\"].shape)+\" <<< <<<\"+str(adj[\"cube\"].shape))\n",
    "                # append the adjacent image to the current image\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                                \n",
    "                # Condition for marking as match:\n",
    "                # 1 - the adj piece is on the same row as the curr\n",
    "                # 2 - the adj piece is just to the right of the curr\n",
    "                # 3 - the curr cube is on the right edge of the piece\n",
    "                # 4 - the adj cube is on the left edge of the piece\n",
    "                # 5 - the cubes are in the same horizon\n",
    "                if  curr[\"piece_row\"] == adj[\"piece_row\"] and \\\n",
    "                    curr[\"piece_col\"] + 1 == adj[\"piece_col\"] and \\\n",
    "                    (curr[\"col\"] != 0 or curr[\"last\"]) and \\\n",
    "                    (adj[\"col\"] == 0 or not adj[\"last\"]) and \\\n",
    "                    np.abs(x_delta) < 50: \n",
    "\n",
    "                    #print(\"TTT:\", curr[\"col\"], curr[\"last\"], adj[\"col\"], adj[\"last\"], x_delta)\n",
    "                    #print(\"TTT:\", curr[\"left_col\"], curr[\"top_row\"], adj[\"left_col\"], adj[\"top_row\"], x_delta)\n",
    "                    \n",
    "                    # tag the image as matched (2nd digit) and not-enriched (1st digit)\n",
    "                    output = folder+\"0_1=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\">>> MATCH >>>\"+output)\n",
    "                    save_img(output, conc)\n",
    "                    # print(\">>> >>> >>> SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([0,1])\n",
    "                    is_enriched.append(False)\n",
    "\n",
    "                    # TOLERANCE\n",
    "                    VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                              train_y_delta, is_enriched, curr, adj, tolerance_factor)\n",
    "                    \n",
    "                    # ENRICH/DUPLICATE\n",
    "                    # need to enrich the set with a few more true positive samples - so we offset \n",
    "                    # the matched images up and down a few times and create more matches\n",
    "                    if (\"top\" in curr.keys() and \"top\" in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                            adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            # tag the image as matched (2nd digit) and enriched (1st digit)\n",
    "                            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            save_img(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    "                            is_enriched.append(True)\n",
    "\n",
    "                            # TOLERANCE\n",
    "                            VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                                      train_y_delta, is_enriched, curr1, adj1, tolerance_factor)\n",
    "                            \n",
    "                    if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                            adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            # tag the image as matched (2nd digit) and enriched (1st digit)\n",
    "                            output = folder+\"1_1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            save_img(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    "                            is_enriched.append(True)\n",
    "\n",
    "                            # TOLERANCE\n",
    "                            VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                                      train_y_delta, is_enriched, curr1, adj1, tolerance_factor)\n",
    " \n",
    "                # adding a condition for marking as not-matched - we mark only the \"key\" cubes which are every 250px\n",
    "                # and not overlap - hence we reduce the ratio in favour of not matched which is enormous\n",
    "                \n",
    "                # altering between the next 2 lines allows to control the number/ratio of non-match:match\n",
    "                # elif int(curr[\"row\"]) == curr[\"row\"] and int(adj[\"row\"]) == adj[\"row\"]: # this condition will match curr key cubes with adj key cubes only\n",
    "                elif int(adj[\"row\"]) == adj[\"row\"]: # this condition will allow curr cubes which are not just key\n",
    "                    #print(\"NNN:\", curr[\"col\"], curr[\"last\"], adj[\"col\"], adj[\"last\"], x_delta)\n",
    "                    #print(\"NNN:\", curr[\"left_col\"], curr[\"top_row\"], adj[\"left_col\"], adj[\"top_row\"], x_delta)\n",
    "                    # tag the image as not-matched (2nd digit) and not-enriched (1st digit)\n",
    "                    output = folder+\"0_0=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\"<<< nonmatch <<<\"+output)\n",
    "                    save_img(output, conc)\n",
    "                    # print(\"<<< <<< <<< SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([1,0]) # not matched\n",
    "                    is_enriched.append(False)\n",
    "                \n",
    "                # discard not matched which are not \"key\" cubes (every 250px)\n",
    "                else:\n",
    "                    #if x_delta == 0:\n",
    "                    #    print(\"DDD:\", curr[\"col\"], curr[\"last\"], adj[\"col\"], adj[\"last\"], x_delta)\n",
    "                    #    print(\"DDD:\", curr[\"left_col\"], curr[\"top_row\"], adj[\"left_col\"], adj[\"top_row\"], x_delta)\n",
    "                    discard_c += 1\n",
    "            else:\n",
    "                skipped_c += 1\n",
    "    print(\"*** MATCHED=\"+str(sum(x[1] == 1 for x in train_lbls)))\n",
    "    print(\"*** NOT MATCHED=\"+str(sum(x[0] == 1 for x in train_lbls)))\n",
    "    print(\"*** DISCARDED=\"+str(discard_c))\n",
    "    print(\"*** SKIPPED=\"+str(skipped_c))\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched\n",
    "\n",
    "\n",
    "def PARALLEL_build_train_set_for_binary_labeling(args): \n",
    "    print('args:', args)\n",
    "    return NEW_build_train_set_for_binary_labeling(args[0], args[1], args[2], args[3], args[4], args[5])\n",
    "\n",
    "\n",
    "def frame_to_n_by_m(orig, start_vector, end_vector, is_col):\n",
    "    max_val = np.amax(end_vector)\n",
    "    min_val = np.amin(start_vector)\n",
    "    width = max_val - min_val\n",
    "    if width < CUBE_SIZE:\n",
    "        width = CUBE_SIZE\n",
    "    if (is_col):\n",
    "        result = np.zeros((start_vector.size, width))\n",
    "    else:\n",
    "        result = np.zeros((width, start_vector.size))\n",
    "    \n",
    "    for i in range(0, start_vector.size):\n",
    "        if (is_col):\n",
    "            row_vec = orig[i, start_vector[i]:end_vector[i]]\n",
    "        else:\n",
    "            row_vec = orig[start_vector[i]:end_vector[i],i]\n",
    "        temp = np.lib.pad(row_vec, (start_vector[i]-min_val, max_val-end_vector[i]), 'constant', constant_values=(0.09, 0.09))\n",
    "        if (is_col):\n",
    "            if (result[i].size != width):\n",
    "                import pdb; pdb.set_trace()\n",
    "            result[i] = temp[0:width]\n",
    "        else:\n",
    "            result[:,i] = temp[0:width]\n",
    "    return min_val, result\n",
    "\n",
    "def rough_tear_line(orig, start_vector, cut_mean, is_col, chew_factor):\n",
    "    end_vector = np.empty(start_vector.size).astype(int)\n",
    "    if (is_col and np.absolute(cut_mean-orig.shape[1]) < 10):\n",
    "        end_vector.fill(orig.shape[1])\n",
    "    elif (not is_col and np.absolute(cut_mean-orig.shape[0]) < 10):\n",
    "        end_vector.fill(orig.shape[0])\n",
    "    else:\n",
    "        deviation_vector = np.random.normal(0, chew_factor, start_vector.size).astype(int)\n",
    "        end_vector[0] = cut_mean + deviation_vector[0]\n",
    "        for i in range(1, end_vector.size):\n",
    "            end_vector[i] = end_vector[i - 1] + deviation_vector[i]\n",
    "\n",
    "        cut_max = start_vector + (CUBE_SIZE + EDGE_GAP)\n",
    "        max_and_end = np.append([end_vector], [cut_max], axis=0)\n",
    "        end_vector = np.amax(max_and_end, axis=0)\n",
    "            \n",
    "    start_px, cut_piece = frame_to_n_by_m(orig, start_vector, end_vector, is_col)    \n",
    "    return start_px, cut_piece, end_vector\n",
    "\n",
    "def rough_tear_image(image, cols, rows):\n",
    "    pieces = []\n",
    "    col_width = int(image.shape[1] / cols)\n",
    "    row_height = int(image.shape[0] / rows)\n",
    "    # print(col_width, row_height)\n",
    "    next_col_start_vec = np.zeros((image.shape[0],), dtype=int)\n",
    "    for col_idx in range(0, cols):\n",
    "    #         import pdb; pdb.set_trace()\n",
    "        start_col_px, cut_column, next_col_start_vec =  rough_tear_line(image, next_col_start_vec, col_width * (col_idx + 1), True, 3)\n",
    "        next_row_start_vec = np.zeros((cut_column.shape[1],), dtype=int)\n",
    "        for row_idx in range(0, rows):\n",
    "            start_row_px, cut_piece, next_row_start_vec = rough_tear_line(cut_column, next_row_start_vec, row_height * (row_idx + 1), False, 1)\n",
    "\n",
    "            ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(cut_piece, cutoff=BG_2_OBJ_RATIO)\n",
    "            temp = crop(cut_piece, ymin, ymax, xmin, xmax)\n",
    "            \n",
    "            #import pdb; pdb.set_trace()\n",
    "            piece = {}\n",
    "            piece[\"orig\"] = cut_piece\n",
    "            piece[\"cut\"] = temp\n",
    "            piece[\"col\"] = col_idx\n",
    "            piece[\"row\"] = row_idx\n",
    "            piece[\"col_px\"] = start_col_px + xmin\n",
    "            piece[\"row_px\"] = start_row_px + ymin\n",
    "            pieces.append(piece)\n",
    "            \n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.704621Z",
     "start_time": "2019-10-18T12:23:06.695832Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Define model util functions\n",
    "\n",
    "# initialize a shaped matrix of weights with random values\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# initialize a shaped matrix of bias with random values\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 2, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 1, 1],\n",
    "                        strides=[1, 2, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x5(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 5, 1],\n",
    "                        strides=[1, 5, 5, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 2, 1],\n",
    "                        strides=[1, 5, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.730800Z",
     "start_time": "2019-10-18T12:23:06.707205Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Image utility functions (external source)\n",
    "def branchedPoints(skel):\n",
    "    branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])\n",
    "    branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])\n",
    "    branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])\n",
    "    branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])\n",
    "    branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])\n",
    "    branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])\n",
    "    branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])\n",
    "    br1=mh.morph.hitmiss(skel,branch1)\n",
    "    br2=mh.morph.hitmiss(skel,branch2)\n",
    "    br3=mh.morph.hitmiss(skel,branch3)\n",
    "    br4=mh.morph.hitmiss(skel,branch4)\n",
    "    br5=mh.morph.hitmiss(skel,branch5)\n",
    "    br6=mh.morph.hitmiss(skel,branch6)\n",
    "    br7=mh.morph.hitmiss(skel,branch7)\n",
    "    br8=mh.morph.hitmiss(skel,branch8)\n",
    "    br9=mh.morph.hitmiss(skel,branch9)\n",
    "    return br1+br2+br3+br4+br5+br6+br7+br8+br9\n",
    "\n",
    "def endPoints(skel):\n",
    "    endpoint1=np.array([[0, 0, 0],\n",
    "                        [0, 1, 0],\n",
    "                        [2, 1, 2]])\n",
    "    \n",
    "    endpoint2=np.array([[0, 0, 0],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 2, 1]])\n",
    "    \n",
    "    endpoint3=np.array([[0, 0, 2],\n",
    "                        [0, 1, 1],\n",
    "                        [0, 0, 2]])\n",
    "    \n",
    "    endpoint4=np.array([[0, 2, 1],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint5=np.array([[2, 1, 2],\n",
    "                        [0, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint6=np.array([[1, 2, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint7=np.array([[2, 0, 0],\n",
    "                        [1, 1, 0],\n",
    "                        [2, 0, 0]])\n",
    "    \n",
    "    endpoint8=np.array([[0, 0, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [1, 2, 0]])\n",
    "    \n",
    "    ep1=mh.morph.hitmiss(skel,endpoint1)\n",
    "    ep2=mh.morph.hitmiss(skel,endpoint2)\n",
    "    ep3=mh.morph.hitmiss(skel,endpoint3)\n",
    "    ep4=mh.morph.hitmiss(skel,endpoint4)\n",
    "    ep5=mh.morph.hitmiss(skel,endpoint5)\n",
    "    ep6=mh.morph.hitmiss(skel,endpoint6)\n",
    "    ep7=mh.morph.hitmiss(skel,endpoint7)\n",
    "    ep8=mh.morph.hitmiss(skel,endpoint8)\n",
    "    ep = ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8\n",
    "    return ep\n",
    "\n",
    "def pruning(skeleton, size):\n",
    "    '''remove iteratively end points \"size\" \n",
    "       times from the skeleton\n",
    "    '''\n",
    "    for i in range(0, size):\n",
    "        endpoints = endPoints(skeleton)\n",
    "        endpoints = np.logical_not(endpoints)\n",
    "        skeleton = np.logical_and(skeleton,endpoints)\n",
    "    return skeleton\n",
    "\n",
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_adjustable('box-forced')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.757595Z",
     "start_time": "2019-10-18T12:23:06.734539Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_deep - Define the model - 250, 125, 62, 25\n",
    "def model_tf_deep(input_width, forced_bias=0): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob, probability, probabilities #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv0 = bias_variable([32])\n",
    "    h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "    h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "#     W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "#     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    c_fc2 = tf.constant([0, forced_bias], dtype=tf.float32)\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 + c_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "      # tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
    "\n",
    "    probability = tf.nn.softmax(y_conv,1)\n",
    "    \n",
    "    probabilities=tf.reduce_sum(probability,1)\n",
    "    \n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.769927Z",
     "start_time": "2019-10-18T12:23:06.758876Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_orig - Define the model - 250, 125, 62, 25\n",
    "def model_tf_orig(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "#     b_conv0 = bias_variable([32])\n",
    "#     h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "#     h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "#     h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.782742Z",
     "start_time": "2019-10-18T12:23:06.771378Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_wide - Define the model - 250, 125, 62, 25\n",
    "def model_tf_wide(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 1, 64])\n",
    "    b_conv1 = bias_variable([64])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*64]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*64, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([2048, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.791538Z",
     "start_time": "2019-10-18T12:23:06.784170Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN train\n",
    "def train(train_imgs, train_lbls, output_model, input_model=\"\"):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING:\")\n",
    "    print(\"MODEL:\"+output_model)\n",
    "    print(\"#####################################################################\")\n",
    "\n",
    "    from random import randrange\n",
    "    \n",
    "    # TRAIN Prepare the session\n",
    "\n",
    "    # create a saver object\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "    if input_model != \"\":\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, input_model)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # TRAIN Train the model\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # run the train batches\n",
    "    for i in range(BATCHES):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(BATCH_SIZE):\n",
    "            random_index = randrange(0,len(train_imgs))\n",
    "            image = np.load(train_imgs[random_index]+\".npy\")\n",
    "            # print(train_imgs[random_index])\n",
    "            x_batch.append(image)\n",
    "            y_batch.append(train_lbls[random_index])\n",
    "\n",
    "        # train\n",
    "        # print(\"step %d\"%(i))\n",
    "        train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "        # print the accuracy thus far\n",
    "        if (i+1)%50 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %.2f\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, output_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    # Close the Session when we're done. If un-commented - need to run next bock of restore...\n",
    "    sess.close()   \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.796804Z",
     "start_time": "2019-10-18T12:23:06.792877Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process - OLD?\n",
    "def pre_process(folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+folder)\n",
    "    print(\"#####################################################################\")\n",
    "    result = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            # Read the image\n",
    "            # image = img.imread(os.path.join(root, file_))\n",
    "            image = np.load(os.path.join(root, file_))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            cubes = VAL_slice_to_static_slices(file_, image)\n",
    "            print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "            result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.808438Z",
     "start_time": "2019-10-18T12:23:06.798094Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process_training - crop image, then tear it randomly to various tears, then per tear create cubes out of the edges, return cube set\n",
    "    # cols and rows are transposed from FRONT\n",
    "def pre_process_training(img_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END, max_cols=4, max_rows=8):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+img_name)\n",
    "    print(\"#####################################################################\")\n",
    "    short_name = img_name[:img_name.rfind('-D')]\n",
    "    image = read_and_crop(img_name, x_start, x_end, y_start, y_end)\n",
    "    \n",
    "    # need to rotate for FRONT\n",
    "    image = np.rot90(image)\n",
    "    result = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(ROOT_FOLDER+fragments_folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    # cols and rows are transposed from FRONT\n",
    "    for col_cut in range(2, max_cols): # 9 3...10\n",
    "        for row_cut in range(3, max_rows): # 6 2...5\n",
    "            print(\"PRE_PROCESS:::\"+\"TEAR_\"+str(col_cut)+\"X\"+str(row_cut))\n",
    "            pieces = rough_tear_image(image, col_cut, row_cut)\n",
    "            \n",
    "            for piece in pieces:\n",
    "                # print(\"PRE_PROCESS:::\"+\"PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"]))\n",
    "                fragment_name = short_name + \"_TEAR_\"+str(col_cut)+\"X\"+str(row_cut)+\"_PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "                fragment_file_name = short_name + \"_\"+str(col_cut)+\"X\"+str(row_cut)+\"_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "                # import pdb; pdb.set_trace()\n",
    "                plt.imsave(os.path.join(ROOT_FOLDER+fragments_folder,fragment_file_name+\".jpg\"), piece[\"cut\"], cmap=plt.cm.gray)\n",
    "                cubes = VAL_slice_TEAR_to_static_slices(fragment_name, piece)\n",
    "                for cube in cubes:\n",
    "                    cube[\"tear\"] = str(col_cut)+\"X\"+str(row_cut)\n",
    "                    cube[\"piece_col\"] = piece[\"col\"]\n",
    "                    cube[\"piece_row\"] = piece[\"row\"]\n",
    "                # print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "                result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.815500Z",
     "start_time": "2019-10-18T12:23:06.810980Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate1(cubes, model, slice_size, folder, curr_cube):    \n",
    "    # VALIDATE prepare the data sets\n",
    "    test_imgs, test_x_delta, test_y_delta, test_x_file, test_y_file = VAL_build_train_set(cubes, slice_size, folder, curr_cube)\n",
    "    print(\"loaded %d images\"%(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.828737Z",
     "start_time": "2019-10-18T12:23:06.817268Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate2(folder, model, slice_size):\n",
    "    test_imgs = []\n",
    "    test_x_file = []\n",
    "    test_y_file = []\n",
    "    the_root = \"\"\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        the_root = root\n",
    "        for file_ in files:\n",
    "            test_imgs.append( os.path.join(root, file_) )\n",
    "            test_x_file.append(file_[:file_.rfind('---P')])\n",
    "            test_y_file.append(file_[file_.rfind('---P')+3:])\n",
    "            \n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    v1t = []\n",
    "    count = 0\n",
    "    length = len(test_imgs)\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i])\n",
    "                x_batch.append(image)\n",
    "                count += 1\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        x_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "\n",
    "\n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    A = np.array(test_x_file)\n",
    "    B = np.array(test_y_file)\n",
    "    C = np.array(test_imgs)\n",
    "    match_x_files = A[match_indexes]\n",
    "    match_y_files = B[match_indexes]\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    for matched_img in match_images:\n",
    "        load_img = np.load(matched_img)\n",
    "        plt.imsave(os.path.join(\"/Volumes/250GB/matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            os.remove( os.path.join(root, file_) ) # delete it from the FS\n",
    "                \n",
    "    with open('matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            print(\"MATCH %s === %s\"%(test_x_file[match_index], test_y_file[match_index]))\n",
    "            # print(\"MATCH %s === %s\"%(A[match_index], B[match_index]))\n",
    "            # csvout.writerow([A[match_index], B[match_index]])\n",
    "            csvout.writerow([test_x_file[match_index], test_y_file[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.842667Z",
     "start_time": "2019-10-18T12:23:06.830035Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def validate2_for_cross_validation(test_imgs, test_lbls, is_enriched, model, max_samples=0):\n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    count = 0\n",
    "    se = 0\n",
    "    st = 0\n",
    "    v1t = []\n",
    "    v2t = []\n",
    "    v1tt = []\n",
    "    v2tt = []\n",
    "    length = len(test_imgs)\n",
    "    if max_samples != 0:\n",
    "        length = max_samples\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i]+\".npy\")\n",
    "                x_batch.append(image)\n",
    "                y_batch.append(train_lbls[i])                \n",
    "                \n",
    "        # print the accuracy thus far\n",
    "    #         train_accuracy = accuracy.eval(feed_dict={\n",
    "    #             x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    #         print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "    #         my_prediction=tf.argmax(y_conv,1)\n",
    "    #         v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "    #         v1t = np.concatenate((v1t, v1), axis=0)\n",
    "\n",
    "        ######## printing the predictions and their normalized values\n",
    "        # print(\"y_conv=\"+str(y_conv.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probability=\"+str(probability.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probabilities=\"+str(probabilities.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        \n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        my_target=tf.argmax(y_,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v2 = my_target.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        v2t = np.concatenate((v2t, v2), axis=0)\n",
    "\n",
    "        c1 = np.sum(np.absolute(np.subtract(v2, v1)))\n",
    "        c2 = np.sum(np.absolute(v2))\n",
    "        se += c1\n",
    "        st += c2\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "        \n",
    "        count += ((i+1) - start)\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "        print(\"step %d-%d, precision %f, recall %f, f_score %f\"%(start, i, precision, recall, f_score))\n",
    "        # print(\"Accumulated total true = %d\"%(st));\n",
    "        # print(\"Accumulated total error rate = %f\"%(se/count));\n",
    "        # v1tt = np.concatenate((v1tt, v1t), axis=0)\n",
    "        # v2tt = np.concatenate((v2tt, v2t), axis=0)\n",
    "        print(\"=== total %d match %d\"%(count, len(np.nonzero(v1t)[0])))\n",
    "\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "    print(\"TOTAL %d, precision %f, recall %f, f_score %f\"%(count, precision, recall, f_score))\n",
    "    print(\"TOTAL true = %d\"%(st));\n",
    "    print(\"TOTAL error rate = %f\"%(se/count));\n",
    "    \n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    C = np.array(test_imgs)\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    # for matched_img in match_images:\n",
    "    #    load_img = np.load(matched_img+\".npy\")\n",
    "    #    plt.imsave(os.path.join(ROOT_FOLDER+\"synt_matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "\n",
    "    with open(strftime(\"%Y%m%d_%H%M%S\", gmtime())+'_synt_all.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for idx, test_img in enumerate(test_imgs):\n",
    "            # print(\"MATCH %s === %s\"%(test_imgs[match_index], train_lbls[match_index]))\n",
    "            match_class = 0\n",
    "            if idx in match_indexes:\n",
    "                match_class = 1\n",
    "            csvout.writerow([test_img, train_lbls[idx], match_class, is_enriched[idx]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    with open(strftime(\"%Y%m%d_%H%M%S\", gmtime())+'_synt_matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            # print(\"MATCH %s === %s\"%(test_imgs[match_index], train_lbls[match_index]))\n",
    "            csvout.writerow([test_imgs[match_index], train_lbls[match_index], is_enriched[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.848017Z",
     "start_time": "2019-10-18T12:23:06.843926Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def iter_validate(cubes, model, slice_size, folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATING\")\n",
    "    print(\"#####################################################################\")\n",
    "    cubes_len = len(cubes)\n",
    "    batch_size = 100\n",
    "    count = 0\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        count += 1\n",
    "        if count < batch_size: ### TEMP LIMITATION\n",
    "            print(\"CUBE:%s\"%(curr[\"file\"]+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])))\n",
    "            validate1(cubes, model, slice_size, folder, curr)\n",
    "            validate2(folder, model, slice_size)\n",
    "        \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATION ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.851659Z",
     "start_time": "2019-10-18T12:23:06.849230Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_all(folder, model, slice_size):\n",
    "    model_tf(slice_size)\n",
    "    cubes_set = pre_process(folder)\n",
    "    validate(cubes_set, model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.854931Z",
     "start_time": "2019-10-18T12:23:06.853052Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# HELPER block\n",
    "# image = read_and_crop(\"PX303/FG001/PX303-Fg001-V-C01-R01-D05032015-T112602-ML924__012.jpg\")\n",
    "## image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R01-D08032015-T110900-ML924__012.jpg\", 100, -1, 400, -1)\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R02-D08032015-T105147-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R01-D08032015-T110025-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R02-D08032015-T105553-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R01-D08032015-T120605-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R02-D08032015-T115230-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R01-D08032015-T120158-ML924__012.jpg\")\n",
    "##image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R02-D08032015-T115704-ML924__012.jpg\", 0, 6200, 0, 4400)\n",
    "##plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:06.862033Z",
     "start_time": "2019-10-18T12:23:06.857660Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_train_from_disk(path):\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    is_enriched = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_ in files:\n",
    "            file_name = os.path.join(root, file_)\n",
    "            file_name = file_name[:file_name.rfind(\".\")]\n",
    "            train_imgs.append(file_name)\n",
    "    #        train_lbls.append([1,0] if file_.startswith(\"0=\") else [0,1])\n",
    "    # return train_imgs, train_lbls\n",
    "            enriched = file_[0] == '1'\n",
    "            is_enriched.append(enriched)\n",
    "            label = [0,1] if file_[2] == '1' else [1,0] \n",
    "            train_lbls.append(label)\n",
    "    return train_imgs, train_lbls, is_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:23:16.497307Z",
     "start_time": "2019-10-18T12:23:16.493573Z"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "########################################## RUN ALL ABOVE !!! Then select the relevant code below\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T04:59:30.940130Z",
     "start_time": "2019-09-30T04:33:29.673860Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg001-R-C01-R01-D05032015-T112011-ML638__006.jpg\n",
      "#####################################################################\n",
      "PRE_PROCESS:::TEAR_2X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilan/venv35/lib/python3.5/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_2X4\n",
      "PRE_PROCESS:::TEAR_2X5\n",
      "PRE_PROCESS:::TEAR_2X6\n",
      "PRE_PROCESS:::TEAR_2X7\n",
      "PRE_PROCESS:::TEAR_3X3\n",
      "PRE_PROCESS:::TEAR_3X4\n",
      "PRE_PROCESS:::TEAR_3X5\n",
      "PRE_PROCESS:::TEAR_3X6\n",
      "PRE_PROCESS:::TEAR_3X7\n",
      "*** CUBES=100\n",
      " MATCHED=3189\n",
      " NOT MATCHED=3345\n",
      " DISCARDED=12390\n",
      " SKIPPED=334917\n",
      "*** CUBES=200\n",
      " MATCHED=5172\n",
      " NOT MATCHED=5507\n",
      " DISCARDED=20405\n",
      " SKIPPED=678977\n",
      "*** CUBES=300\n",
      " MATCHED=5172\n",
      " NOT MATCHED=5507\n",
      " DISCARDED=20405\n",
      " SKIPPED=1033277\n",
      "*** CUBES=400\n",
      " MATCHED=6973\n",
      " NOT MATCHED=7699\n",
      " DISCARDED=28938\n",
      " SKIPPED=1376785\n",
      "*** CUBES=500\n",
      " MATCHED=9162\n",
      " NOT MATCHED=9916\n",
      " DISCARDED=37578\n",
      " SKIPPED=1720151\n",
      "*** CUBES=600\n",
      " MATCHED=9162\n",
      " NOT MATCHED=9916\n",
      " DISCARDED=37578\n",
      " SKIPPED=2074451\n",
      "*** CUBES=700\n",
      " MATCHED=11374\n",
      " NOT MATCHED=12410\n",
      " DISCARDED=46417\n",
      " SKIPPED=2417330\n",
      "*** CUBES=800\n",
      " MATCHED=13046\n",
      " NOT MATCHED=14565\n",
      " DISCARDED=54068\n",
      " SKIPPED=2761760\n",
      "*** CUBES=900\n",
      " MATCHED=13046\n",
      " NOT MATCHED=14565\n",
      " DISCARDED=54068\n",
      " SKIPPED=3116060\n",
      "*** CUBES=1000\n",
      " MATCHED=15033\n",
      " NOT MATCHED=17482\n",
      " DISCARDED=63164\n",
      " SKIPPED=3458256\n",
      "*** CUBES=1100\n",
      " MATCHED=16076\n",
      " NOT MATCHED=19055\n",
      " DISCARDED=68072\n",
      " SKIPPED=3806028\n",
      "*** CUBES=1200\n",
      " MATCHED=16121\n",
      " NOT MATCHED=19471\n",
      " DISCARDED=69573\n",
      " SKIPPED=4158408\n",
      "*** CUBES=1300\n",
      " MATCHED=17730\n",
      " NOT MATCHED=22054\n",
      " DISCARDED=78905\n",
      " SKIPPED=4500708\n",
      "*** CUBES=1400\n",
      " MATCHED=18066\n",
      " NOT MATCHED=22441\n",
      " DISCARDED=80300\n",
      " SKIPPED=4853208\n",
      "*** CUBES=1500\n",
      " MATCHED=20054\n",
      " NOT MATCHED=26785\n",
      " DISCARDED=96363\n",
      " SKIPPED=5187033\n",
      "*** CUBES=1600\n",
      " MATCHED=22898\n",
      " NOT MATCHED=33044\n",
      " DISCARDED=119486\n",
      " SKIPPED=5511861\n",
      "*** CUBES=1700\n",
      " MATCHED=25301\n",
      " NOT MATCHED=36226\n",
      " DISCARDED=130823\n",
      " SKIPPED=5851561\n",
      "*** CUBES=1800\n",
      " MATCHED=26951\n",
      " NOT MATCHED=38038\n",
      " DISCARDED=137279\n",
      " SKIPPED=6197539\n",
      "*** CUBES=1900\n",
      " MATCHED=26951\n",
      " NOT MATCHED=38038\n",
      " DISCARDED=137279\n",
      " SKIPPED=6551839\n",
      "*** CUBES=2000\n",
      " MATCHED=29168\n",
      " NOT MATCHED=44327\n",
      " DISCARDED=159815\n",
      " SKIPPED=6877233\n",
      "*** CUBES=2100\n",
      " MATCHED=31172\n",
      " NOT MATCHED=48975\n",
      " DISCARDED=176576\n",
      " SKIPPED=7210046\n",
      "*** CUBES=2200\n",
      " MATCHED=33834\n",
      " NOT MATCHED=51855\n",
      " DISCARDED=187102\n",
      " SKIPPED=7550846\n",
      "*** CUBES=2300\n",
      " MATCHED=34335\n",
      " NOT MATCHED=52200\n",
      " DISCARDED=188362\n",
      " SKIPPED=7903526\n",
      "*** CUBES=2400\n",
      " MATCHED=35362\n",
      " NOT MATCHED=55477\n",
      " DISCARDED=199829\n",
      " SKIPPED=8243039\n",
      "*** CUBES=2500\n",
      " MATCHED=37470\n",
      " NOT MATCHED=61133\n",
      " DISCARDED=219539\n",
      " SKIPPED=8571887\n",
      "*** CUBES=2600\n",
      " MATCHED=39633\n",
      " NOT MATCHED=64015\n",
      " DISCARDED=229170\n",
      " SKIPPED=8913587\n",
      "*** CUBES=2700\n",
      " MATCHED=40507\n",
      " NOT MATCHED=65081\n",
      " DISCARDED=232732\n",
      " SKIPPED=9263225\n",
      "*** CUBES=2800\n",
      " MATCHED=41274\n",
      " NOT MATCHED=67515\n",
      " DISCARDED=240179\n",
      " SKIPPED=9607609\n",
      "*** CUBES=2900\n",
      " MATCHED=43789\n",
      " NOT MATCHED=74015\n",
      " DISCARDED=260072\n",
      " SKIPPED=9935401\n",
      "*** CUBES=3000\n",
      " MATCHED=45379\n",
      " NOT MATCHED=76997\n",
      " DISCARDED=269218\n",
      " SKIPPED=10277501\n",
      "*** CUBES=3100\n",
      " MATCHED=46844\n",
      " NOT MATCHED=78303\n",
      " DISCARDED=273213\n",
      " SKIPPED=10626433\n",
      "*** CUBES=3200\n",
      " MATCHED=47358\n",
      " NOT MATCHED=80066\n",
      " DISCARDED=279820\n",
      " SKIPPED=10972335\n",
      "*** CUBES=3300\n",
      " MATCHED=49034\n",
      " NOT MATCHED=85165\n",
      " DISCARDED=298927\n",
      " SKIPPED=11302337\n",
      "*** CUBES=3400\n",
      " MATCHED=50362\n",
      " NOT MATCHED=87549\n",
      " DISCARDED=307769\n",
      " SKIPPED=11645337\n",
      "*** CUBES=3500\n",
      " MATCHED=51016\n",
      " NOT MATCHED=88287\n",
      " DISCARDED=310498\n",
      " SKIPPED=11996134\n",
      "*** MATCHED=51016\n",
      "*** NOT MATCHED=88287\n",
      "*** DISCARDED=310498\n",
      "*** SKIPPED=12152026\n",
      "WARNING:tensorflow:From <ipython-input-6-6d6c00010ca0>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "#####################################################################\n",
      "TRAINING:\n",
      "MODEL:/media/1KGB_ILAN/papyrus/models/front_tear_model1.ckpt\n",
      "#####################################################################\n",
      "WARNING:tensorflow:From /home/ilan/venv35/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 49, training accuracy 0.65\n",
      "step 99, training accuracy 0.68\n",
      "step 149, training accuracy 0.73\n",
      "step 199, training accuracy 0.68\n",
      "step 249, training accuracy 0.67\n",
      "step 299, training accuracy 0.80\n",
      "step 349, training accuracy 0.78\n",
      "step 399, training accuracy 0.82\n",
      "step 449, training accuracy 0.78\n",
      "step 499, training accuracy 0.77\n",
      "step 549, training accuracy 0.78\n",
      "step 599, training accuracy 0.77\n",
      "step 649, training accuracy 0.78\n",
      "step 699, training accuracy 0.80\n",
      "step 749, training accuracy 0.78\n",
      "step 799, training accuracy 0.80\n",
      "step 849, training accuracy 0.85\n",
      "step 899, training accuracy 0.85\n",
      "step 949, training accuracy 0.82\n",
      "step 999, training accuracy 0.80\n",
      "step 1049, training accuracy 0.82\n",
      "step 1099, training accuracy 0.80\n",
      "step 1149, training accuracy 0.90\n",
      "step 1199, training accuracy 0.87\n",
      "step 1249, training accuracy 0.87\n",
      "step 1299, training accuracy 0.92\n",
      "step 1349, training accuracy 0.90\n",
      "step 1399, training accuracy 0.85\n",
      "step 1449, training accuracy 0.85\n",
      "step 1499, training accuracy 0.97\n",
      "step 1549, training accuracy 0.95\n",
      "step 1599, training accuracy 0.92\n",
      "step 1649, training accuracy 0.97\n",
      "step 1699, training accuracy 0.97\n",
      "step 1749, training accuracy 0.90\n",
      "step 1799, training accuracy 0.90\n",
      "step 1849, training accuracy 0.95\n",
      "step 1899, training accuracy 0.92\n",
      "step 1949, training accuracy 0.93\n",
      "step 1999, training accuracy 0.90\n",
      "Optimization Finished!\n",
      "step 1999, training accuracy 0.9\n",
      "Model saved in file: /media/1KGB_ILAN/papyrus/models/front_tear_model1.ckpt\n",
      "#####################################################################\n",
      "TRAINING ENDED\n",
      "#####################################################################\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# RUN1 - take 1st large pieces and train on it\n",
    "cubes_set = pre_process_training(\"PX303-Fg001-R-C01-R01-D05032015-T112011-ML638__006.jpg\")\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 12, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)\n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/front_tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T14:21:01.997116Z",
     "start_time": "2019-04-03T14:16:57.383608Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RE-RUN1 - take 1st large pieces and train on it\n",
    "train_imgs, train_lbls, is_enriched = \\\n",
    "    load_train_from_disk(ROOT_FOLDER + \"train_concats3/\")\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)\n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T06:55:34.462694Z",
     "start_time": "2019-04-01T06:55:34.270110Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TEMP DEBUG\n",
    "len_cubes = len(cubes_set)\n",
    "print(len_cubes)\n",
    "# bat_cubes = 1000\n",
    "# BATCHES = 500 # smaller train batches\n",
    "# for start in range(0, len_cubes, bat_cubes):\n",
    "#     end = ((start+bat_cubes) if start+bat_cubes < len_cubes else (len_cubes - 1))\n",
    "#     print(\"start:%s end:%s\", start, end)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set[1:2], cubes_set, CUBE_SIZE, \n",
    "                                            ROOT_FOLDER + \"train_concats2/\", 1, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T05:35:05.952850Z",
     "start_time": "2019-09-30T04:59:30.942012Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg004-R-C01-R01-D05032015-T115135-ML638__006.jpg\n",
      "#####################################################################\n",
      "PRE_PROCESS:::TEAR_2X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilan/venv35/lib/python3.5/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_2X4\n",
      "PRE_PROCESS:::TEAR_2X5\n",
      "PRE_PROCESS:::TEAR_2X6\n",
      "PRE_PROCESS:::TEAR_2X7\n",
      "PRE_PROCESS:::TEAR_3X3\n",
      "PRE_PROCESS:::TEAR_3X4\n",
      "PRE_PROCESS:::TEAR_3X5\n",
      "PRE_PROCESS:::TEAR_3X6\n",
      "PRE_PROCESS:::TEAR_3X7\n",
      "*** CUBES=100\n",
      " MATCHED=2093\n",
      " NOT MATCHED=4737\n",
      " DISCARDED=17962\n",
      " SKIPPED=492921\n",
      "*** CUBES=200\n",
      " MATCHED=5180\n",
      " NOT MATCHED=9517\n",
      " DISCARDED=36083\n",
      " SKIPPED=990821\n",
      "*** CUBES=300\n",
      " MATCHED=5446\n",
      " NOT MATCHED=9996\n",
      " DISCARDED=37896\n",
      " SKIPPED=1509421\n",
      "*** CUBES=400\n",
      " MATCHED=5446\n",
      " NOT MATCHED=9996\n",
      " DISCARDED=37896\n",
      " SKIPPED=2030321\n",
      "*** CUBES=500\n",
      " MATCHED=6397\n",
      " NOT MATCHED=12751\n",
      " DISCARDED=47948\n",
      " SKIPPED=2538381\n",
      "*** CUBES=600\n",
      " MATCHED=9472\n",
      " NOT MATCHED=17329\n",
      " DISCARDED=64671\n",
      " SKIPPED=3037881\n",
      "*** CUBES=700\n",
      " MATCHED=10875\n",
      " NOT MATCHED=19481\n",
      " DISCARDED=72530\n",
      " SKIPPED=3548723\n",
      "*** CUBES=800\n",
      " MATCHED=10875\n",
      " NOT MATCHED=19481\n",
      " DISCARDED=72530\n",
      " SKIPPED=4069623\n",
      "*** CUBES=900\n",
      " MATCHED=11597\n",
      " NOT MATCHED=21348\n",
      " DISCARDED=78944\n",
      " SKIPPED=4582216\n",
      "*** CUBES=1000\n",
      " MATCHED=13923\n",
      " NOT MATCHED=26130\n",
      " DISCARDED=95374\n",
      " SKIPPED=5081816\n",
      "*** CUBES=1100\n",
      " MATCHED=15620\n",
      " NOT MATCHED=29044\n",
      " DISCARDED=105394\n",
      " SKIPPED=5589723\n",
      "*** CUBES=1200\n",
      " MATCHED=15620\n",
      " NOT MATCHED=29044\n",
      " DISCARDED=105394\n",
      " SKIPPED=6110623\n",
      "*** CUBES=1300\n",
      " MATCHED=16019\n",
      " NOT MATCHED=30263\n",
      " DISCARDED=109412\n",
      " SKIPPED=6626271\n",
      "*** CUBES=1400\n",
      " MATCHED=18077\n",
      " NOT MATCHED=34942\n",
      " DISCARDED=124849\n",
      " SKIPPED=7126971\n",
      "*** CUBES=1500\n",
      " MATCHED=19823\n",
      " NOT MATCHED=38123\n",
      " DISCARDED=135338\n",
      " SKIPPED=7634135\n",
      "*** CUBES=1600\n",
      " MATCHED=19823\n",
      " NOT MATCHED=38123\n",
      " DISCARDED=135338\n",
      " SKIPPED=8155035\n",
      "*** CUBES=1700\n",
      " MATCHED=20130\n",
      " NOT MATCHED=39440\n",
      " DISCARDED=139948\n",
      " SKIPPED=8669995\n",
      "*** CUBES=1800\n",
      " MATCHED=22282\n",
      " NOT MATCHED=43821\n",
      " DISCARDED=155273\n",
      " SKIPPED=9171095\n",
      "*** CUBES=1900\n",
      " MATCHED=23596\n",
      " NOT MATCHED=46449\n",
      " DISCARDED=164471\n",
      " SKIPPED=9680115\n",
      "*** CUBES=2000\n",
      " MATCHED=23596\n",
      " NOT MATCHED=46449\n",
      " DISCARDED=164471\n",
      " SKIPPED=10201015\n",
      "*** CUBES=2100\n",
      " MATCHED=24487\n",
      " NOT MATCHED=50476\n",
      " DISCARDED=179611\n",
      " SKIPPED=10702721\n",
      "*** CUBES=2200\n",
      " MATCHED=27366\n",
      " NOT MATCHED=60057\n",
      " DISCARDED=215641\n",
      " SKIPPED=11177921\n",
      "*** CUBES=2300\n",
      " MATCHED=30023\n",
      " NOT MATCHED=68488\n",
      " DISCARDED=247307\n",
      " SKIPPED=11658641\n",
      "*** CUBES=2400\n",
      " MATCHED=32351\n",
      " NOT MATCHED=73273\n",
      " DISCARDED=265150\n",
      " SKIPPED=12156841\n",
      "*** CUBES=2500\n",
      " MATCHED=35602\n",
      " NOT MATCHED=78051\n",
      " DISCARDED=282971\n",
      " SKIPPED=12655041\n",
      "*** CUBES=2600\n",
      " MATCHED=35720\n",
      " NOT MATCHED=78338\n",
      " DISCARDED=284042\n",
      " SKIPPED=13174579\n",
      "*** CUBES=2700\n",
      " MATCHED=35720\n",
      " NOT MATCHED=78338\n",
      " DISCARDED=284042\n",
      " SKIPPED=13695479\n",
      "*** CUBES=2800\n",
      " MATCHED=37012\n",
      " NOT MATCHED=84424\n",
      " DISCARDED=306990\n",
      " SKIPPED=14187301\n",
      "*** CUBES=2900\n",
      " MATCHED=40030\n",
      " NOT MATCHED=93506\n",
      " DISCARDED=341212\n",
      " SKIPPED=14664801\n",
      "*** CUBES=3000\n",
      " MATCHED=42136\n",
      " NOT MATCHED=100107\n",
      " DISCARDED=366275\n",
      " SKIPPED=15153965\n",
      "*** CUBES=3100\n",
      " MATCHED=44823\n",
      " NOT MATCHED=104591\n",
      " DISCARDED=383502\n",
      " SKIPPED=15653065\n",
      "*** CUBES=3200\n",
      " MATCHED=46658\n",
      " NOT MATCHED=107368\n",
      " DISCARDED=394182\n",
      " SKIPPED=16160449\n",
      "*** CUBES=3300\n",
      " MATCHED=46658\n",
      " NOT MATCHED=107368\n",
      " DISCARDED=394182\n",
      " SKIPPED=16681349\n",
      "*** CUBES=3400\n",
      " MATCHED=46958\n",
      " NOT MATCHED=109345\n",
      " DISCARDED=400693\n",
      " SKIPPED=17193749\n",
      "*** CUBES=3500\n",
      " MATCHED=49779\n",
      " NOT MATCHED=119225\n",
      " DISCARDED=433216\n",
      " SKIPPED=17672149\n",
      "*** CUBES=3600\n",
      " MATCHED=52304\n",
      " NOT MATCHED=128204\n",
      " DISCARDED=462796\n",
      " SKIPPED=18154401\n",
      "*** CUBES=3700\n",
      " MATCHED=54417\n",
      " NOT MATCHED=133087\n",
      " DISCARDED=478934\n",
      " SKIPPED=18654201\n",
      "*** CUBES=3800\n",
      " MATCHED=57114\n",
      " NOT MATCHED=137771\n",
      " DISCARDED=494413\n",
      " SKIPPED=19154845\n",
      "*** CUBES=3900\n",
      " MATCHED=57114\n",
      " NOT MATCHED=137771\n",
      " DISCARDED=494413\n",
      " SKIPPED=19675745\n",
      "*** CUBES=4000\n",
      " MATCHED=57114\n",
      " NOT MATCHED=137771\n",
      " DISCARDED=494413\n",
      " SKIPPED=20196645\n",
      "*** CUBES=4100\n",
      " MATCHED=59315\n",
      " NOT MATCHED=146402\n",
      " DISCARDED=523550\n",
      " SKIPPED=20679694\n",
      "*** CUBES=4200\n",
      " MATCHED=61809\n",
      " NOT MATCHED=155680\n",
      " DISCARDED=554878\n",
      " SKIPPED=21159894\n",
      "*** CUBES=4300\n",
      " MATCHED=63269\n",
      " NOT MATCHED=160454\n",
      " DISCARDED=571378\n",
      " SKIPPED=21659458\n",
      "*** CUBES=4400\n",
      " MATCHED=65899\n",
      " NOT MATCHED=164934\n",
      " DISCARDED=586900\n",
      " SKIPPED=22160258\n",
      "*** CUBES=4500\n",
      " MATCHED=66345\n",
      " NOT MATCHED=165471\n",
      " DISCARDED=588761\n",
      " SKIPPED=22678746\n",
      "*** CUBES=4600\n",
      " MATCHED=66345\n",
      " NOT MATCHED=165471\n",
      " DISCARDED=588761\n",
      " SKIPPED=23199646\n",
      "*** CUBES=4700\n",
      " MATCHED=68066\n",
      " NOT MATCHED=172764\n",
      " DISCARDED=616197\n",
      " SKIPPED=23685746\n",
      "*** CUBES=4800\n",
      " MATCHED=70395\n",
      " NOT MATCHED=181146\n",
      " DISCARDED=647718\n",
      " SKIPPED=24166646\n",
      "*** CUBES=4900\n",
      " MATCHED=72495\n",
      " NOT MATCHED=185747\n",
      " DISCARDED=665033\n",
      " SKIPPED=24665546\n",
      "*** CUBES=5000\n",
      " MATCHED=74425\n",
      " NOT MATCHED=189932\n",
      " DISCARDED=680766\n",
      " SKIPPED=25166446\n",
      "*** CUBES=5100\n",
      " MATCHED=74785\n",
      " NOT MATCHED=190350\n",
      " DISCARDED=682336\n",
      " SKIPPED=25685346\n",
      "*** CUBES=5200\n",
      " MATCHED=74785\n",
      " NOT MATCHED=190350\n",
      " DISCARDED=682336\n",
      " SKIPPED=26206246\n",
      "*** MATCHED=74785\n",
      "*** NOT MATCHED=190350\n",
      "*** DISCARDED=682336\n",
      "*** SKIPPED=26258336\n",
      "#####################################################################\n",
      "TRAINING:\n",
      "MODEL:/media/1KGB_ILAN/papyrus/models/front_tear_model2.ckpt\n",
      "#####################################################################\n",
      "INFO:tensorflow:Restoring parameters from /media/1KGB_ILAN/papyrus/models/front_tear_model1.ckpt\n",
      "Model restored.\n",
      "step 49, training accuracy 0.78\n",
      "step 99, training accuracy 0.75\n",
      "step 149, training accuracy 0.70\n",
      "step 199, training accuracy 0.85\n",
      "step 249, training accuracy 0.77\n",
      "step 299, training accuracy 0.80\n",
      "step 349, training accuracy 0.80\n",
      "step 399, training accuracy 0.88\n",
      "step 449, training accuracy 0.80\n",
      "step 499, training accuracy 0.80\n",
      "step 549, training accuracy 0.82\n",
      "step 599, training accuracy 0.90\n",
      "step 649, training accuracy 0.85\n",
      "step 699, training accuracy 0.85\n",
      "step 749, training accuracy 0.88\n",
      "step 799, training accuracy 0.87\n",
      "step 849, training accuracy 0.82\n",
      "step 899, training accuracy 0.73\n",
      "step 949, training accuracy 0.82\n",
      "step 999, training accuracy 0.92\n",
      "step 1049, training accuracy 0.83\n",
      "step 1099, training accuracy 0.83\n",
      "step 1149, training accuracy 0.78\n",
      "step 1199, training accuracy 0.88\n",
      "step 1249, training accuracy 0.95\n",
      "step 1299, training accuracy 0.88\n",
      "step 1349, training accuracy 0.88\n",
      "step 1399, training accuracy 0.85\n",
      "step 1449, training accuracy 0.92\n",
      "step 1499, training accuracy 0.88\n",
      "step 1549, training accuracy 0.87\n",
      "step 1599, training accuracy 0.88\n",
      "step 1649, training accuracy 0.83\n",
      "step 1699, training accuracy 0.80\n",
      "step 1749, training accuracy 0.90\n",
      "step 1799, training accuracy 0.95\n",
      "step 1849, training accuracy 0.93\n",
      "step 1899, training accuracy 0.90\n",
      "step 1949, training accuracy 0.93\n",
      "step 1999, training accuracy 0.90\n",
      "Optimization Finished!\n",
      "step 1999, training accuracy 0.9\n",
      "Model saved in file: /media/1KGB_ILAN/papyrus/models/front_tear_model2.ckpt\n",
      "#####################################################################\n",
      "TRAINING ENDED\n",
      "#####################################################################\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# RUN2 - take 2nd large pieces and train on it\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "#    parallelize_build_train_set(cubes_set, ROOT_FOLDER + \"train_concats2/\", 1, 7, PARALLEL_build_train_set_for_binary_labeling)\n",
    "cubes_set = pre_process_training(\"PX303-Fg004-R-C01-R01-D05032015-T115135-ML638__006.jpg\", \\\n",
    "                                 100, -1, 400, -1)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 12, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)\n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/front_tear_model2.ckpt\", ROOT_FOLDER + \"models/front_tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T23:05:02.443698Z",
     "start_time": "2019-06-01T22:09:12.687507Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", \\\n",
    "#                                  100, -1, 400, -1, max_cols=8, max_rows=5)\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# len_cubes = len(cubes_set)\n",
    "# bat_cubes = 1000\n",
    "# # COPY MANUALLY!!!\n",
    "# #shutil.copy(ROOT_FOLDER + \"models/tear_model1.ckpt\", ROOT_FOLDER + \"models/tear_model2.ckpt\")\n",
    "# for start_cubes in range(0, len_cubes, bat_cubes):\n",
    "#     end_cubes = ((start+bat_cubes) if start+bat_cubes < len_cubes else (len_cubes - 1))\n",
    "#     BATCHES = int((end_cubes - start_cubes) / 10) # smaller train batches\n",
    "#     train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "#         NEW_build_train_set_for_binary_labeling(cubes_set[start_cubes:end_cubes], cubes_set, CUBE_SIZE, \n",
    "#                                                 ROOT_FOLDER + \"train_concats2/\", 1, 5)\n",
    "#     train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model2.ckpt\", ROOT_FOLDER + \"models/tear_model2.ckpt\")  \n",
    "# BATCHES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MULTI PROCESS\n",
    "import multiprocessing\n",
    "from itertools import product, repeat\n",
    "\n",
    "num_cores = 4# multiprocessing.cpu_count()\n",
    "num_partitions = num_cores * 4\n",
    "\n",
    "def chunkify(lst,n):\n",
    "    return [lst[i::n] for i in range(n)]\n",
    "\n",
    "def parallelize_build_train_set(cubes_set, folder, enrich_factor, tolerance_factor, func):\n",
    "    adjs_list = chunkify(cubes_set, num_partitions)\n",
    "    print('list:', adjs_list)\n",
    "    # test block for running serialy    \n",
    "    # results = []\n",
    "    # for item in zip(np.arange(num_partitions), df_split):\n",
    "    #    results.append(func(item))\n",
    "    # df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # parallel block\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    args = zip(repeat(cubes_set), adjs_list, repeat(CUBE_SIZE), \\\n",
    "               repeat(ROOT_FOLDER + folder), repeat(enrich_factor), repeat(tolerance_factor))\n",
    "\n",
    "    results = pool.map(func, args)\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    is_enriched = []\n",
    "    \n",
    "    for result in results:\n",
    "        train_imgs.append(result[0])\n",
    "        train_lbls.append(result[1])\n",
    "        train_x_delta.append(result[2])\n",
    "        train_y_delta.append(result[3])\n",
    "        is_enriched.append(result[4])\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Parallel RUN2 - take 2nd large pieces and train on it\n",
    "cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", 100, -1, 400, -1, max_cols=9, max_rows=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RE-RUN2 - take 2nd large pieces and train on it\n",
    "train_imgs, train_lbls, is_enriched = \\\n",
    "    load_train_from_disk(ROOT_FOLDER + \"train_concats2/\")\n",
    "print(len(train_imgs))\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)\n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\", ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T12:31:53.223189Z",
     "start_time": "2019-10-18T12:23:38.196169Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg006-R-C02-R02-D08032015-T114223-ML638__006.jpg\n",
      "#####################################################################\n",
      "PRE_PROCESS:::TEAR_2X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilan/venv35/lib/python3.5/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_2X4\n",
      "PRE_PROCESS:::TEAR_2X5\n",
      "PRE_PROCESS:::TEAR_2X6\n",
      "PRE_PROCESS:::TEAR_2X7\n",
      "PRE_PROCESS:::TEAR_3X3\n",
      "PRE_PROCESS:::TEAR_3X4\n",
      "PRE_PROCESS:::TEAR_3X5\n",
      "PRE_PROCESS:::TEAR_3X6\n",
      "PRE_PROCESS:::TEAR_3X7\n",
      "*** CUBES=100\n",
      " MATCHED=16\n",
      " NOT MATCHED=2768\n",
      " DISCARDED=10383\n",
      " SKIPPED=415503\n",
      "*** CUBES=200\n",
      " MATCHED=57\n",
      " NOT MATCHED=5419\n",
      " DISCARDED=20326\n",
      " SKIPPED=835868\n",
      "*** CUBES=300\n",
      " MATCHED=57\n",
      " NOT MATCHED=5419\n",
      " DISCARDED=20326\n",
      " SKIPPED=1268868\n",
      "*** CUBES=400\n",
      " MATCHED=86\n",
      " NOT MATCHED=8006\n",
      " DISCARDED=29806\n",
      " SKIPPED=1689772\n",
      "*** CUBES=500\n",
      " MATCHED=180\n",
      " NOT MATCHED=11587\n",
      " DISCARDED=42931\n",
      " SKIPPED=2105972\n",
      "*** CUBES=600\n",
      " MATCHED=192\n",
      " NOT MATCHED=12088\n",
      " DISCARDED=44770\n",
      " SKIPPED=2536620\n",
      "*** CUBES=700\n",
      " MATCHED=196\n",
      " NOT MATCHED=12699\n",
      " DISCARDED=46945\n",
      " SKIPPED=2966830\n",
      "*** CUBES=800\n",
      " MATCHED=239\n",
      " NOT MATCHED=16090\n",
      " DISCARDED=59011\n",
      " SKIPPED=3384330\n",
      "*** CUBES=900\n",
      " MATCHED=302\n",
      " NOT MATCHED=18355\n",
      " DISCARDED=67068\n",
      " SKIPPED=3806945\n",
      "*** CUBES=1000\n",
      " MATCHED=302\n",
      " NOT MATCHED=18355\n",
      " DISCARDED=67068\n",
      " SKIPPED=4239945\n",
      "*** CUBES=1100\n",
      " MATCHED=348\n",
      " NOT MATCHED=20921\n",
      " DISCARDED=77404\n",
      " SKIPPED=4659997\n",
      "*** CUBES=1200\n",
      " MATCHED=433\n",
      " NOT MATCHED=23975\n",
      " DISCARDED=89703\n",
      " SKIPPED=5077559\n",
      "*** CUBES=1300\n",
      " MATCHED=433\n",
      " NOT MATCHED=23975\n",
      " DISCARDED=89703\n",
      " SKIPPED=5510559\n",
      "*** CUBES=1400\n",
      " MATCHED=450\n",
      " NOT MATCHED=25365\n",
      " DISCARDED=94077\n",
      " SKIPPED=5937778\n",
      "*** CUBES=1500\n",
      " MATCHED=505\n",
      " NOT MATCHED=28750\n",
      " DISCARDED=104737\n",
      " SKIPPED=6356678\n",
      "*** CUBES=1600\n",
      " MATCHED=530\n",
      " NOT MATCHED=29698\n",
      " DISCARDED=107712\n",
      " SKIPPED=6785730\n",
      "*** CUBES=1700\n",
      " MATCHED=547\n",
      " NOT MATCHED=32237\n",
      " DISCARDED=117246\n",
      " SKIPPED=7206640\n",
      "*** CUBES=1800\n",
      " MATCHED=627\n",
      " NOT MATCHED=40421\n",
      " DISCARDED=147982\n",
      " SKIPPED=7600640\n",
      "*** CUBES=1900\n",
      " MATCHED=711\n",
      " NOT MATCHED=47203\n",
      " DISCARDED=173466\n",
      " SKIPPED=8001290\n",
      "*** CUBES=2000\n",
      " MATCHED=784\n",
      " NOT MATCHED=51387\n",
      " DISCARDED=189209\n",
      " SKIPPED=8414290\n",
      "*** CUBES=2100\n",
      " MATCHED=837\n",
      " NOT MATCHED=53685\n",
      " DISCARDED=197858\n",
      " SKIPPED=8836290\n",
      "*** CUBES=2200\n",
      " MATCHED=837\n",
      " NOT MATCHED=53685\n",
      " DISCARDED=197858\n",
      " SKIPPED=9269290\n",
      "*** CUBES=2300\n",
      " MATCHED=859\n",
      " NOT MATCHED=56921\n",
      " DISCARDED=210440\n",
      " SKIPPED=9686450\n",
      "*** CUBES=2400\n",
      " MATCHED=952\n",
      " NOT MATCHED=64103\n",
      " DISCARDED=238365\n",
      " SKIPPED=10084250\n",
      "*** CUBES=2500\n",
      " MATCHED=1013\n",
      " NOT MATCHED=69371\n",
      " DISCARDED=258636\n",
      " SKIPPED=10491650\n",
      "*** CUBES=2600\n",
      " MATCHED=1101\n",
      " NOT MATCHED=73355\n",
      " DISCARDED=273764\n",
      " SKIPPED=10905450\n",
      "*** CUBES=2700\n",
      " MATCHED=1101\n",
      " NOT MATCHED=73355\n",
      " DISCARDED=273764\n",
      " SKIPPED=11338450\n",
      "*** CUBES=2800\n",
      " MATCHED=1104\n",
      " NOT MATCHED=73994\n",
      " DISCARDED=276082\n",
      " SKIPPED=11768490\n",
      "*** CUBES=2900\n",
      " MATCHED=1192\n",
      " NOT MATCHED=81977\n",
      " DISCARDED=305011\n",
      " SKIPPED=12164490\n",
      "*** CUBES=3000\n",
      " MATCHED=1275\n",
      " NOT MATCHED=88920\n",
      " DISCARDED=330149\n",
      " SKIPPED=12565326\n",
      "*** CUBES=3100\n",
      " MATCHED=1366\n",
      " NOT MATCHED=92902\n",
      " DISCARDED=344476\n",
      " SKIPPED=12979926\n",
      "*** CUBES=3200\n",
      " MATCHED=1425\n",
      " NOT MATCHED=95288\n",
      " DISCARDED=353071\n",
      " SKIPPED=13401886\n",
      "*** CUBES=3300\n",
      " MATCHED=1425\n",
      " NOT MATCHED=95288\n",
      " DISCARDED=353071\n",
      " SKIPPED=13834886\n",
      "*** CUBES=3400\n",
      " MATCHED=1455\n",
      " NOT MATCHED=99369\n",
      " DISCARDED=368448\n",
      " SKIPPED=14248398\n",
      "*** CUBES=3500\n",
      " MATCHED=1547\n",
      " NOT MATCHED=106652\n",
      " DISCARDED=395873\n",
      " SKIPPED=14646598\n",
      "*** CUBES=3600\n",
      " MATCHED=1638\n",
      " NOT MATCHED=110938\n",
      " DISCARDED=412364\n",
      " SKIPPED=15058730\n",
      "*** CUBES=3700\n",
      " MATCHED=1724\n",
      " NOT MATCHED=114197\n",
      " DISCARDED=425035\n",
      " SKIPPED=15475714\n",
      "*** CUBES=3800\n",
      " MATCHED=1724\n",
      " NOT MATCHED=114197\n",
      " DISCARDED=425035\n",
      " SKIPPED=15908714\n",
      "*** CUBES=3900\n",
      " MATCHED=1740\n",
      " NOT MATCHED=116669\n",
      " DISCARDED=432513\n",
      " SKIPPED=16331748\n",
      "*** CUBES=4000\n",
      " MATCHED=1815\n",
      " NOT MATCHED=124148\n",
      " DISCARDED=455159\n",
      " SKIPPED=16734548\n",
      "*** CUBES=4100\n",
      " MATCHED=1894\n",
      " NOT MATCHED=129288\n",
      " DISCARDED=470626\n",
      " SKIPPED=17146862\n",
      "*** CUBES=4200\n",
      " MATCHED=1936\n",
      " NOT MATCHED=131924\n",
      " DISCARDED=478532\n",
      " SKIPPED=17569278\n",
      "*** CUBES=4300\n",
      " MATCHED=1936\n",
      " NOT MATCHED=131924\n",
      " DISCARDED=478532\n",
      " SKIPPED=18002278\n",
      "*** MATCHED=1936\n",
      "*** NOT MATCHED=131924\n",
      "*** DISCARDED=478532\n",
      "*** SKIPPED=18136508\n",
      "WARNING:tensorflow:From <ipython-input-6-6d6c00010ca0>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "#####################################################################\n",
      "TRAINING:\n",
      "MODEL:/media/1KGB_ILAN/papyrus/model_binary/front_tear_model3.ckpt\n",
      "#####################################################################\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/media/1KGB_ILAN/papyrus/model_binary; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-71897954ab8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_tf_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lbls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROOT_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model_binary/front_tear_model3.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROOT_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model_binary/front_tear_model2.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model1.ckpt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d3b9bcb3e790>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_imgs, train_lbls, output_model, input_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_model\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Restore variables from disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1534\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[1;32m   1538\u001b[0m                        + compat.as_text(save_path))\n",
      "\u001b[0;32m~/venv35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    362\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    363\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 364\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv35/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    344\u001b[0m               compat.as_bytes(single_filename), status)\n",
      "\u001b[0;32m~/venv35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /media/1KGB_ILAN/papyrus/model_binary; No such file or directory"
     ]
    }
   ],
   "source": [
    "# OPTIONAL RUN3 - take 3rd large pieces and train on it OR TEST in next block\n",
    "\n",
    "cubes_set = pre_process_training(\"PX303-Fg006-R-C02-R02-D08032015-T114223-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\")\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)    \n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/front_tear_model3.ckpt\", ROOT_FOLDER + \"models/front_tear_model2.ckpt\")\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T06:03:08.614256Z",
     "start_time": "2019-09-30T05:35:05.957541Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg006-R-C02-R02-D08032015-T114223-ML638__006.jpg\n",
      "#####################################################################\n",
      "PRE_PROCESS:::TEAR_2X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilan/venv35/lib/python3.5/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_2X4\n",
      "PRE_PROCESS:::TEAR_2X5\n",
      "PRE_PROCESS:::TEAR_2X6\n",
      "PRE_PROCESS:::TEAR_2X7\n",
      "PRE_PROCESS:::TEAR_3X3\n",
      "PRE_PROCESS:::TEAR_3X4\n",
      "PRE_PROCESS:::TEAR_3X5\n",
      "PRE_PROCESS:::TEAR_3X6\n",
      "PRE_PROCESS:::TEAR_3X7\n",
      "*** CUBES=100\n",
      " MATCHED=1208\n",
      " NOT MATCHED=3849\n",
      " DISCARDED=14800\n",
      " SKIPPED=409860\n",
      "*** CUBES=200\n",
      " MATCHED=3127\n",
      " NOT MATCHED=7419\n",
      " DISCARDED=28529\n",
      " SKIPPED=825372\n",
      "*** CUBES=300\n",
      " MATCHED=3127\n",
      " NOT MATCHED=7419\n",
      " DISCARDED=28529\n",
      " SKIPPED=1258272\n",
      "*** CUBES=400\n",
      " MATCHED=3155\n",
      " NOT MATCHED=7931\n",
      " DISCARDED=30578\n",
      " SKIPPED=1688607\n",
      "*** CUBES=500\n",
      " MATCHED=3337\n",
      " NOT MATCHED=10631\n",
      " DISCARDED=41364\n",
      " SKIPPED=2108007\n",
      "*** CUBES=600\n",
      " MATCHED=4338\n",
      " NOT MATCHED=12457\n",
      " DISCARDED=48665\n",
      " SKIPPED=2531727\n",
      "*** CUBES=700\n",
      " MATCHED=4338\n",
      " NOT MATCHED=12457\n",
      " DISCARDED=48665\n",
      " SKIPPED=2964627\n",
      "*** CUBES=800\n",
      " MATCHED=5176\n",
      " NOT MATCHED=15938\n",
      " DISCARDED=61525\n",
      " SKIPPED=3381134\n",
      "*** CUBES=900\n",
      " MATCHED=6846\n",
      " NOT MATCHED=19016\n",
      " DISCARDED=72895\n",
      " SKIPPED=3799500\n",
      "*** CUBES=1000\n",
      " MATCHED=6846\n",
      " NOT MATCHED=19016\n",
      " DISCARDED=72895\n",
      " SKIPPED=4232400\n",
      "*** CUBES=1100\n",
      " MATCHED=6915\n",
      " NOT MATCHED=20185\n",
      " DISCARDED=77837\n",
      " SKIPPED=4659180\n",
      "*** CUBES=1200\n",
      " MATCHED=7502\n",
      " NOT MATCHED=22779\n",
      " DISCARDED=88796\n",
      " SKIPPED=5078480\n",
      "*** CUBES=1300\n",
      " MATCHED=7999\n",
      " NOT MATCHED=23502\n",
      " DISCARDED=91852\n",
      " SKIPPED=5507572\n",
      "*** CUBES=1400\n",
      " MATCHED=8145\n",
      " NOT MATCHED=24686\n",
      " DISCARDED=95334\n",
      " SKIPPED=5935792\n",
      "*** CUBES=1500\n",
      " MATCHED=8504\n",
      " NOT MATCHED=27975\n",
      " DISCARDED=105010\n",
      " SKIPPED=6355692\n",
      "*** CUBES=1600\n",
      " MATCHED=8840\n",
      " NOT MATCHED=28992\n",
      " DISCARDED=107999\n",
      " SKIPPED=6784562\n",
      "*** CUBES=1700\n",
      " MATCHED=9280\n",
      " NOT MATCHED=31913\n",
      " DISCARDED=118981\n",
      " SKIPPED=7203539\n",
      "*** CUBES=1800\n",
      " MATCHED=10908\n",
      " NOT MATCHED=39397\n",
      " DISCARDED=147117\n",
      " SKIPPED=7600739\n",
      "*** CUBES=1900\n",
      " MATCHED=12569\n",
      " NOT MATCHED=45463\n",
      " DISCARDED=169923\n",
      " SKIPPED=8004690\n",
      "*** CUBES=2000\n",
      " MATCHED=13273\n",
      " NOT MATCHED=49652\n",
      " DISCARDED=185684\n",
      " SKIPPED=8417590\n",
      "*** CUBES=2100\n",
      " MATCHED=13408\n",
      " NOT MATCHED=50238\n",
      " DISCARDED=187889\n",
      " SKIPPED=8847690\n",
      "*** CUBES=2200\n",
      " MATCHED=13408\n",
      " NOT MATCHED=50238\n",
      " DISCARDED=187889\n",
      " SKIPPED=9280590\n",
      "*** CUBES=2300\n",
      " MATCHED=14614\n",
      " NOT MATCHED=57108\n",
      " DISCARDED=213983\n",
      " SKIPPED=9680466\n",
      "*** CUBES=2400\n",
      " MATCHED=16411\n",
      " NOT MATCHED=65090\n",
      " DISCARDED=244308\n",
      " SKIPPED=10074966\n",
      "*** CUBES=2500\n",
      " MATCHED=17970\n",
      " NOT MATCHED=69194\n",
      " DISCARDED=259903\n",
      " SKIPPED=10488090\n",
      "*** CUBES=2600\n",
      " MATCHED=19814\n",
      " NOT MATCHED=72974\n",
      " DISCARDED=274271\n",
      " SKIPPED=10902750\n",
      "*** CUBES=2700\n",
      " MATCHED=19814\n",
      " NOT MATCHED=72974\n",
      " DISCARDED=274271\n",
      " SKIPPED=11335650\n",
      "*** CUBES=2800\n",
      " MATCHED=19916\n",
      " NOT MATCHED=74013\n",
      " DISCARDED=277958\n",
      " SKIPPED=11763818\n",
      "*** CUBES=2900\n",
      " MATCHED=21361\n",
      " NOT MATCHED=81995\n",
      " DISCARDED=306293\n",
      " SKIPPED=12160318\n",
      "*** CUBES=3000\n",
      " MATCHED=22920\n",
      " NOT MATCHED=88695\n",
      " DISCARDED=330150\n",
      " SKIPPED=12562578\n",
      "*** CUBES=3100\n",
      " MATCHED=24572\n",
      " NOT MATCHED=92677\n",
      " DISCARDED=344476\n",
      " SKIPPED=12977078\n",
      "*** CUBES=3200\n",
      " MATCHED=25545\n",
      " NOT MATCHED=94586\n",
      " DISCARDED=351350\n",
      " SKIPPED=13401146\n",
      "*** CUBES=3300\n",
      " MATCHED=25545\n",
      " NOT MATCHED=94586\n",
      " DISCARDED=351350\n",
      " SKIPPED=13834046\n",
      "*** CUBES=3400\n",
      " MATCHED=26078\n",
      " NOT MATCHED=99270\n",
      " DISCARDED=369337\n",
      " SKIPPED=14244234\n",
      "*** CUBES=3500\n",
      " MATCHED=27830\n",
      " NOT MATCHED=106151\n",
      " DISCARDED=395754\n",
      " SKIPPED=14643734\n",
      "*** CUBES=3600\n",
      " MATCHED=29268\n",
      " NOT MATCHED=109832\n",
      " DISCARDED=410059\n",
      " SKIPPED=15058560\n",
      "*** CUBES=3700\n",
      " MATCHED=30048\n",
      " NOT MATCHED=112019\n",
      " DISCARDED=418560\n",
      " SKIPPED=15480724\n",
      "*** CUBES=3800\n",
      " MATCHED=30048\n",
      " NOT MATCHED=112019\n",
      " DISCARDED=418560\n",
      " SKIPPED=15913624\n",
      "*** CUBES=3900\n",
      " MATCHED=30604\n",
      " NOT MATCHED=117237\n",
      " DISCARDED=433966\n",
      " SKIPPED=16325860\n",
      "*** CUBES=4000\n",
      " MATCHED=32038\n",
      " NOT MATCHED=125517\n",
      " DISCARDED=458390\n",
      " SKIPPED=16725960\n",
      "*** CUBES=4100\n",
      " MATCHED=33311\n",
      " NOT MATCHED=129779\n",
      " DISCARDED=470967\n",
      " SKIPPED=17141936\n",
      "*** CUBES=4200\n",
      " MATCHED=34256\n",
      " NOT MATCHED=132450\n",
      " DISCARDED=478857\n",
      " SKIPPED=17564212\n",
      "*** CUBES=4300\n",
      " MATCHED=34256\n",
      " NOT MATCHED=132450\n",
      " DISCARDED=478857\n",
      " SKIPPED=17997112\n",
      "*** MATCHED=34256\n",
      "*** NOT MATCHED=132450\n",
      "*** DISCARDED=478857\n",
      "*** SKIPPED=18126982\n",
      "166706\n",
      "INFO:tensorflow:Restoring parameters from /media/1KGB_ILAN/papyrus/models/front_tear_model2.ckpt\n",
      "Model restored.\n",
      ">>> step 100\n",
      "step 0-99, precision 0.515152, recall 1.000000, f_score 0.680000\n",
      "=== total 100 match 33\n",
      ">>> step 200\n",
      "step 100-199, precision 0.246753, recall 1.000000, f_score 0.395833\n",
      "=== total 200 match 77\n",
      ">>> step 300\n",
      "step 200-299, precision 0.158333, recall 1.000000, f_score 0.273381\n",
      "=== total 300 match 120\n",
      ">>> step 400\n",
      "step 300-399, precision 0.164557, recall 1.000000, f_score 0.282609\n",
      "=== total 400 match 158\n",
      ">>> step 500\n",
      "step 400-499, precision 0.162304, recall 1.000000, f_score 0.279279\n",
      "=== total 500 match 191\n",
      ">>> step 600\n",
      "step 500-599, precision 0.165957, recall 1.000000, f_score 0.284672\n",
      "=== total 600 match 235\n",
      ">>> step 700\n",
      "step 600-699, precision 0.142336, recall 1.000000, f_score 0.249201\n",
      "=== total 700 match 274\n",
      ">>> step 800\n",
      "step 700-799, precision 0.183544, recall 1.000000, f_score 0.310160\n",
      "=== total 800 match 316\n",
      ">>> step 900\n",
      "step 800-899, precision 0.213514, recall 0.987500, f_score 0.351111\n",
      "=== total 900 match 370\n",
      ">>> step 1000\n",
      "step 900-999, precision 0.241135, recall 0.990291, f_score 0.387833\n",
      "=== total 1000 match 423\n",
      ">>> step 1100\n",
      "step 1000-1099, precision 0.259179, recall 0.967742, f_score 0.408859\n",
      "=== total 1100 match 463\n",
      ">>> step 1200\n",
      "step 1100-1199, precision 0.255061, recall 0.851351, f_score 0.392523\n",
      "=== total 1200 match 494\n",
      ">>> step 1300\n",
      "step 1200-1299, precision 0.274047, recall 0.872832, f_score 0.417127\n",
      "=== total 1300 match 551\n",
      ">>> step 1400\n",
      "step 1300-1399, precision 0.259005, recall 0.872832, f_score 0.399471\n",
      "=== total 1400 match 583\n",
      ">>> step 1500\n",
      "step 1400-1499, precision 0.269841, recall 0.885417, f_score 0.413625\n",
      "=== total 1500 match 630\n",
      ">>> step 1600\n",
      "step 1500-1599, precision 0.263636, recall 0.887755, f_score 0.406542\n",
      "=== total 1600 match 660\n",
      ">>> step 1700\n",
      "step 1600-1699, precision 0.253219, recall 0.889447, f_score 0.394209\n",
      "=== total 1700 match 699\n",
      ">>> step 1800\n",
      "step 1700-1799, precision 0.240216, recall 0.890000, f_score 0.378321\n",
      "=== total 1800 match 741\n",
      ">>> step 1900\n",
      "step 1800-1899, precision 0.240409, recall 0.858447, f_score 0.375624\n",
      "=== total 1900 match 782\n",
      ">>> step 2000\n",
      "step 1900-1999, precision 0.245421, recall 0.866379, f_score 0.382493\n",
      "=== total 2000 match 819\n",
      ">>> step 2100\n",
      "step 2000-2099, precision 0.252033, recall 0.864542, f_score 0.390288\n",
      "=== total 2100 match 861\n",
      ">>> step 2200\n",
      "step 2100-2199, precision 0.244395, recall 0.865079, f_score 0.381119\n",
      "=== total 2200 match 892\n",
      ">>> step 2300\n",
      "step 2200-2299, precision 0.271682, recall 0.881356, f_score 0.415335\n",
      "=== total 2300 match 957\n",
      ">>> step 2400\n",
      "step 2300-2399, precision 0.260521, recall 0.881356, f_score 0.402166\n",
      "=== total 2400 match 998\n",
      ">>> step 2500\n",
      "step 2400-2499, precision 0.261745, recall 0.886364, f_score 0.404145\n",
      "=== total 2500 match 1043\n",
      ">>> step 2600\n",
      "step 2500-2599, precision 0.252311, recall 0.886364, f_score 0.392806\n",
      "=== total 2600 match 1082\n",
      ">>> step 2700\n",
      "step 2600-2699, precision 0.265679, recall 0.897059, f_score 0.409946\n",
      "=== total 2700 match 1148\n",
      ">>> step 2800\n",
      "step 2700-2799, precision 0.298860, recall 0.906173, f_score 0.449479\n",
      "=== total 2800 match 1228\n",
      ">>> step 2900\n",
      "step 2800-2899, precision 0.297667, recall 0.807860, f_score 0.435038\n",
      "=== total 2900 match 1243\n",
      ">>> step 3000\n",
      "step 2900-2999, precision 0.288837, recall 0.807860, f_score 0.425532\n",
      "=== total 3000 match 1281\n",
      ">>> step 3100\n",
      "step 3000-3099, precision 0.313510, recall 0.829126, f_score 0.454981\n",
      "=== total 3100 match 1362\n",
      ">>> step 3200\n",
      "step 3100-3199, precision 0.315417, recall 0.835206, f_score 0.457906\n",
      "=== total 3200 match 1414\n",
      ">>> step 3300\n",
      "step 3200-3299, precision 0.306108, recall 0.835206, f_score 0.448016\n",
      "=== total 3300 match 1457\n",
      ">>> step 3400\n",
      "step 3300-3399, precision 0.325688, recall 0.849573, f_score 0.470867\n",
      "=== total 3400 match 1526\n",
      ">>> step 3500\n",
      "step 3400-3499, precision 0.330380, recall 0.855738, f_score 0.476712\n",
      "=== total 3500 match 1580\n",
      ">>> step 3600\n",
      "step 3500-3599, precision 0.328020, recall 0.858748, f_score 0.474712\n",
      "=== total 3600 match 1631\n",
      ">>> step 3700\n",
      "step 3600-3699, precision 0.329970, recall 0.863354, f_score 0.477458\n",
      "=== total 3700 match 1685\n",
      ">>> step 3800\n",
      "step 3700-3799, precision 0.341324, recall 0.871720, f_score 0.490566\n",
      "=== total 3800 match 1752\n",
      ">>> step 3900\n",
      "step 3800-3899, precision 0.355068, recall 0.880435, f_score 0.506052\n",
      "=== total 3900 match 1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 4000\n",
      "step 3900-3999, precision 0.366914, recall 0.870440, f_score 0.516225\n",
      "=== total 4000 match 1886\n",
      ">>> step 4100\n",
      "step 4000-4099, precision 0.372590, recall 0.836257, f_score 0.515501\n",
      "=== total 4100 match 1919\n",
      ">>> step 4200\n",
      "step 4100-4199, precision 0.372589, recall 0.839817, f_score 0.516174\n",
      "=== total 4200 match 1970\n",
      ">>> step 4300\n",
      "step 4200-4299, precision 0.387695, recall 0.849198, f_score 0.532350\n",
      "=== total 4300 match 2048\n",
      ">>> step 4400\n",
      "step 4300-4399, precision 0.398201, recall 0.851215, f_score 0.542581\n",
      "=== total 4400 match 2112\n",
      ">>> step 4500\n",
      "step 4400-4499, precision 0.402603, recall 0.834297, f_score 0.543117\n",
      "=== total 4500 match 2151\n",
      ">>> step 4600\n",
      "step 4500-4599, precision 0.407942, recall 0.840149, f_score 0.549210\n",
      "=== total 4600 match 2216\n",
      ">>> step 4700\n",
      "step 4600-4699, precision 0.404058, recall 0.841912, f_score 0.546051\n",
      "=== total 4700 match 2267\n",
      ">>> step 4800\n",
      "step 4700-4799, precision 0.410853, recall 0.842756, f_score 0.552403\n",
      "=== total 4800 match 2322\n",
      ">>> step 4900\n",
      "step 4800-4899, precision 0.404409, recall 0.842756, f_score 0.546548\n",
      "=== total 4900 match 2359\n",
      ">>> step 5000\n",
      "step 4900-4999, precision 0.411377, recall 0.844332, f_score 0.553215\n",
      "=== total 5000 match 2426\n",
      ">>> step 5100\n",
      "step 5000-5099, precision 0.412072, recall 0.847682, f_score 0.554563\n",
      "=== total 5100 match 2485\n",
      ">>> step 5200\n",
      "step 5100-5199, precision 0.419443, recall 0.850556, f_score 0.561827\n",
      "=== total 5200 match 2551\n",
      ">>> step 5300\n",
      "step 5200-5299, precision 0.425954, recall 0.853211, f_score 0.568228\n",
      "=== total 5300 match 2620\n",
      ">>> step 5400\n",
      "step 5300-5399, precision 0.428678, recall 0.856716, f_score 0.571429\n",
      "=== total 5400 match 2678\n",
      ">>> step 5500\n",
      "step 5400-5499, precision 0.427470, recall 0.857143, f_score 0.570448\n",
      "=== total 5500 match 2723\n",
      ">>> step 5600\n",
      "step 5500-5599, precision 0.432170, recall 0.856023, f_score 0.574366\n",
      "=== total 5600 match 2779\n",
      ">>> step 5700\n",
      "step 5600-5699, precision 0.432957, recall 0.858642, f_score 0.575651\n",
      "=== total 5700 match 2834\n",
      ">>> step 5800\n",
      "step 5700-5799, precision 0.436702, recall 0.862398, f_score 0.579803\n",
      "=== total 5800 match 2899\n",
      ">>> step 5900\n",
      "step 5800-5899, precision 0.434206, recall 0.863421, f_score 0.577828\n",
      "=== total 5900 match 2941\n",
      ">>> step 6000\n",
      "step 5900-5999, precision 0.432569, recall 0.850232, f_score 0.573408\n",
      "=== total 6000 match 2966\n",
      ">>> step 6100\n",
      "step 6000-6099, precision 0.434330, recall 0.823043, f_score 0.568602\n",
      "=== total 6100 match 2977\n",
      ">>> step 6200\n",
      "step 6100-6199, precision 0.429568, recall 0.818873, f_score 0.563521\n",
      "=== total 6200 match 3010\n",
      ">>> step 6300\n",
      "step 6200-6299, precision 0.429642, recall 0.821807, f_score 0.564278\n",
      "=== total 6300 match 3070\n",
      ">>> step 6400\n",
      "step 6300-6399, precision 0.434976, recall 0.826586, f_score 0.570000\n",
      "=== total 6400 match 3145\n",
      ">>> step 6500\n",
      "step 6400-6499, precision 0.441607, recall 0.831672, f_score 0.576892\n",
      "=== total 6500 match 3211\n",
      ">>> step 6600\n",
      "step 6500-6599, precision 0.436845, recall 0.831672, f_score 0.572814\n",
      "=== total 6600 match 3246\n",
      ">>> step 6700\n",
      "step 6600-6699, precision 0.442435, recall 0.836467, f_score 0.578750\n",
      "=== total 6700 match 3318\n",
      ">>> step 6800\n",
      "step 6700-6799, precision 0.447524, recall 0.840997, f_score 0.584183\n",
      "=== total 6800 match 3392\n",
      ">>> step 6900\n",
      "step 6800-6899, precision 0.452787, recall 0.845283, f_score 0.589695\n",
      "=== total 6900 match 3463\n",
      ">>> step 7000\n",
      "step 6900-6999, precision 0.452557, recall 0.847340, f_score 0.590000\n",
      "=== total 7000 match 3520\n",
      ">>> step 7100\n",
      "step 7000-7099, precision 0.457556, recall 0.851372, f_score 0.595221\n",
      "=== total 7100 match 3593\n",
      ">>> step 7200\n",
      "step 7100-7199, precision 0.452643, recall 0.851372, f_score 0.591048\n",
      "=== total 7200 match 3632\n",
      ">>> step 7300\n",
      "step 7200-7299, precision 0.450746, recall 0.852669, f_score 0.589739\n",
      "=== total 7300 match 3685\n",
      ">>> step 7400\n",
      "step 7300-7399, precision 0.455706, recall 0.856500, f_score 0.594895\n",
      "=== total 7400 match 3759\n",
      ">>> step 7500\n",
      "step 7400-7499, precision 0.452870, recall 0.857000, f_score 0.592593\n",
      "=== total 7500 match 3798\n",
      ">>> step 7600\n",
      "step 7500-7599, precision 0.452801, recall 0.858829, f_score 0.592970\n",
      "=== total 7600 match 3856\n",
      ">>> step 7700\n",
      "step 7600-7699, precision 0.453109, recall 0.861017, f_score 0.593755\n",
      "=== total 7700 match 3924\n",
      ">>> step 7800\n",
      "step 7700-7799, precision 0.454682, recall 0.863203, f_score 0.595626\n",
      "=== total 7800 match 3983\n",
      ">>> step 7900\n",
      "step 7800-7899, precision 0.460458, recall 0.866883, f_score 0.601448\n",
      "=== total 7900 match 4059\n",
      ">>> step 8000\n",
      "step 7900-7999, precision 0.465555, recall 0.870312, f_score 0.606614\n",
      "=== total 8000 match 4137\n",
      ">>> step 8100\n",
      "step 8000-8099, precision 0.469077, recall 0.872953, f_score 0.610243\n",
      "=== total 8100 match 4204\n",
      ">>> step 8200\n",
      "step 8100-8199, precision 0.470340, recall 0.874836, f_score 0.611772\n",
      "=== total 8200 match 4265\n",
      ">>> step 8300\n",
      "step 8200-8299, precision 0.476541, recall 0.878338, f_score 0.617862\n",
      "=== total 8300 match 4348\n",
      ">>> step 8400\n",
      "step 8300-8399, precision 0.475878, recall 0.879816, f_score 0.617669\n",
      "=== total 8400 match 4415\n",
      ">>> step 8500\n",
      "step 8400-8499, precision 0.478823, recall 0.882136, f_score 0.620720\n",
      "=== total 8500 match 4486\n",
      ">>> step 8600\n",
      "step 8500-8599, precision 0.482335, recall 0.884507, f_score 0.624254\n",
      "=== total 8600 match 4557\n",
      ">>> step 8700\n",
      "step 8600-8699, precision 0.478610, recall 0.884785, f_score 0.621195\n",
      "=== total 8700 match 4605\n",
      ">>> step 8800\n",
      "step 8700-8799, precision 0.480856, recall 0.886785, f_score 0.623578\n",
      "=== total 8800 match 4675\n",
      ">>> step 8900\n",
      "step 8800-8899, precision 0.476695, recall 0.886874, f_score 0.620091\n",
      "=== total 8900 match 4720\n",
      ">>> step 9000\n",
      "step 8900-8999, precision 0.481258, recall 0.889530, f_score 0.624595\n",
      "=== total 9000 match 4802\n",
      ">>> step 9100\n",
      "step 9000-9099, precision 0.479472, recall 0.890080, f_score 0.623223\n",
      "=== total 9100 match 4847\n",
      ">>> step 9200\n",
      "step 9100-9199, precision 0.482815, recall 0.892146, f_score 0.626551\n",
      "=== total 9200 match 4917\n",
      ">>> step 9300\n",
      "step 9200-9299, precision 0.485772, recall 0.894135, f_score 0.629529\n",
      "=== total 9300 match 4990\n",
      ">>> step 9400\n",
      "step 9300-9399, precision 0.485856, recall 0.895370, f_score 0.629905\n",
      "=== total 9400 match 5055\n",
      ">>> step 9500\n",
      "step 9400-9499, precision 0.486814, recall 0.896725, f_score 0.631046\n",
      "=== total 9500 match 5119\n",
      ">>> step 9600\n",
      "step 9500-9599, precision 0.488803, recall 0.898191, f_score 0.633079\n",
      "=== total 9600 match 5180\n",
      ">>> step 9700\n",
      "step 9600-9699, precision 0.488847, recall 0.899334, f_score 0.633399\n",
      "=== total 9700 match 5245\n",
      ">>> step 9800\n",
      "step 9700-9799, precision 0.489734, recall 0.900589, f_score 0.634456\n",
      "=== total 9800 match 5309\n",
      ">>> step 9900\n",
      "step 9800-9899, precision 0.487960, recall 0.901069, f_score 0.633083\n",
      "=== total 9900 match 5357\n",
      ">>> step 10000\n",
      "step 9900-9999, precision 0.487913, recall 0.902081, f_score 0.633293\n",
      "=== total 10000 match 5419\n",
      ">>> step 10100\n",
      "step 10000-10099, precision 0.492456, recall 0.904206, f_score 0.637637\n",
      "=== total 10100 match 5501\n",
      ">>> step 10200\n",
      "step 10100-10199, precision 0.489715, recall 0.904365, f_score 0.635374\n",
      "=== total 10200 match 5542\n",
      ">>> step 10300\n",
      "step 10200-10299, precision 0.489024, recall 0.905187, f_score 0.634994\n",
      "=== total 10300 match 5603\n",
      ">>> step 10400\n",
      "step 10300-10399, precision 0.491543, recall 0.906727, f_score 0.637496\n",
      "=== total 10400 match 5676\n",
      ">>> step 10500\n",
      "step 10400-10499, precision 0.494085, recall 0.908219, f_score 0.640000\n",
      "=== total 10500 match 5748\n",
      ">>> step 10600\n",
      "step 10500-10599, precision 0.492116, recall 0.908219, f_score 0.638346\n",
      "=== total 10600 match 5771\n",
      ">>> step 10700\n",
      "step 10600-10699, precision 0.491710, recall 0.908424, f_score 0.638055\n",
      "=== total 10700 match 5790\n",
      ">>> step 10800\n",
      "step 10700-10799, precision 0.490355, recall 0.908424, f_score 0.636913\n",
      "=== total 10800 match 5806\n",
      ">>> step 10900\n",
      "step 10800-10899, precision 0.489343, recall 0.906399, f_score 0.635562\n",
      "=== total 10900 match 5818\n",
      ">>> step 11000\n",
      "step 10900-10999, precision 0.488755, recall 0.905534, f_score 0.634853\n",
      "=== total 11000 match 5825\n",
      ">>> step 11100\n",
      "step 11000-11099, precision 0.488173, recall 0.902694, f_score 0.633663\n",
      "=== total 11100 match 5834\n",
      ">>> step 11200\n",
      "step 11100-11199, precision 0.487513, recall 0.901328, f_score 0.632771\n",
      "=== total 11200 match 5846\n",
      ">>> step 11300\n",
      "step 11200-11299, precision 0.486348, recall 0.901328, f_score 0.631789\n",
      "=== total 11300 match 5860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 11400\n",
      "step 11300-11399, precision 0.485520, recall 0.901328, f_score 0.631089\n",
      "=== total 11400 match 5870\n",
      ">>> step 11500\n",
      "step 11400-11499, precision 0.484447, recall 0.901328, f_score 0.630182\n",
      "=== total 11500 match 5883\n",
      ">>> step 11600\n",
      "step 11500-11599, precision 0.483871, recall 0.901328, f_score 0.629695\n",
      "=== total 11600 match 5890\n",
      ">>> step 11700\n",
      "step 11600-11699, precision 0.482969, recall 0.901328, f_score 0.628931\n",
      "=== total 11700 match 5901\n",
      ">>> step 11800\n",
      "step 11700-11799, precision 0.481907, recall 0.901328, f_score 0.628030\n",
      "=== total 11800 match 5914\n",
      ">>> step 11900\n",
      "step 11800-11899, precision 0.480931, recall 0.901328, f_score 0.627201\n",
      "=== total 11900 match 5926\n",
      ">>> step 12000\n",
      "step 11900-11999, precision 0.480445, recall 0.901328, f_score 0.626787\n",
      "=== total 12000 match 5932\n",
      ">>> step 12100\n",
      "step 12000-12099, precision 0.479556, recall 0.901328, f_score 0.626030\n",
      "=== total 12100 match 5943\n",
      ">>> step 12200\n",
      "step 12100-12199, precision 0.478831, recall 0.899337, f_score 0.624931\n",
      "=== total 12200 match 5952\n",
      ">>> step 12300\n",
      "step 12200-12299, precision 0.477867, recall 0.899337, f_score 0.624110\n",
      "=== total 12300 match 5964\n",
      ">>> step 12400\n",
      "step 12300-12399, precision 0.477467, recall 0.899337, f_score 0.623769\n",
      "=== total 12400 match 5969\n",
      ">>> step 12500\n",
      "step 12400-12499, precision 0.476748, recall 0.899337, f_score 0.623155\n",
      "=== total 12500 match 5978\n",
      ">>> step 12600\n",
      "step 12500-12599, precision 0.475872, recall 0.899337, f_score 0.622407\n",
      "=== total 12600 match 5989\n",
      ">>> step 12700\n",
      "step 12600-12699, precision 0.475475, recall 0.897355, f_score 0.621592\n",
      "=== total 12700 match 5994\n",
      ">>> step 12800\n",
      "step 12700-12799, precision 0.474684, recall 0.897355, f_score 0.620915\n",
      "=== total 12800 match 6004\n",
      ">>> step 12900\n",
      "step 12800-12899, precision 0.473815, recall 0.897355, f_score 0.620172\n",
      "=== total 12900 match 6015\n",
      ">>> step 13000\n",
      "step 12900-12999, precision 0.472951, recall 0.897355, f_score 0.619431\n",
      "=== total 13000 match 6026\n",
      ">>> step 13100\n",
      "step 13000-13099, precision 0.475531, recall 0.895995, f_score 0.621313\n",
      "=== total 13100 match 6069\n",
      ">>> step 13200\n",
      "step 13100-13199, precision 0.474766, recall 0.893564, f_score 0.620075\n",
      "=== total 13200 match 6083\n",
      ">>> step 13300\n",
      "step 13200-13299, precision 0.475957, recall 0.891271, f_score 0.620535\n",
      "=== total 13300 match 6114\n",
      ">>> step 13400\n",
      "step 13300-13399, precision 0.474947, recall 0.891271, f_score 0.619676\n",
      "=== total 13400 match 6127\n",
      ">>> step 13500\n",
      "step 13400-13499, precision 0.475199, recall 0.890040, f_score 0.619593\n",
      "=== total 13500 match 6149\n",
      ">>> step 13600\n",
      "step 13500-13599, precision 0.474955, recall 0.885999, f_score 0.618404\n",
      "=== total 13600 match 6169\n",
      ">>> step 13700\n",
      "step 13600-13699, precision 0.475061, recall 0.885646, f_score 0.618407\n",
      "=== total 13700 match 6195\n",
      ">>> step 13800\n",
      "step 13700-13799, precision 0.473684, recall 0.885646, f_score 0.617240\n",
      "=== total 13800 match 6213\n",
      ">>> step 13900\n",
      "step 13800-13899, precision 0.472695, recall 0.885646, f_score 0.616400\n",
      "=== total 13900 match 6226\n",
      ">>> step 14000\n",
      "step 13900-13999, precision 0.472791, recall 0.885226, f_score 0.616380\n",
      "=== total 14000 match 6248\n",
      ">>> step 14100\n",
      "step 14000-14099, precision 0.473877, recall 0.884364, f_score 0.617092\n",
      "=== total 14100 match 6278\n",
      ">>> step 14200\n",
      "step 14100-14199, precision 0.475293, recall 0.885251, f_score 0.618508\n",
      "=== total 14200 match 6314\n",
      ">>> step 14300\n",
      "step 14200-14299, precision 0.474474, recall 0.880610, f_score 0.616680\n",
      "=== total 14300 match 6327\n",
      ">>> step 14400\n",
      "step 14300-14399, precision 0.473352, recall 0.880610, f_score 0.615732\n",
      "=== total 14400 match 6342\n",
      ">>> step 14500\n",
      "step 14400-14499, precision 0.473535, recall 0.881064, f_score 0.615998\n",
      "=== total 14500 match 6367\n",
      ">>> step 14600\n",
      "step 14500-14599, precision 0.472958, recall 0.873480, f_score 0.613648\n",
      "=== total 14600 match 6379\n",
      ">>> step 14700\n",
      "step 14600-14699, precision 0.474087, recall 0.866724, f_score 0.612916\n",
      "=== total 14700 match 6406\n",
      ">>> step 14800\n",
      "step 14700-14799, precision 0.475585, recall 0.863010, f_score 0.613232\n",
      "=== total 14800 match 6451\n",
      ">>> step 14900\n",
      "step 14800-14899, precision 0.475499, recall 0.859218, f_score 0.612200\n",
      "=== total 14900 match 6469\n",
      ">>> step 15000\n",
      "step 14900-14999, precision 0.477954, recall 0.858444, f_score 0.614033\n",
      "=== total 15000 match 6509\n",
      ">>> step 15100\n",
      "step 15000-15099, precision 0.478427, recall 0.859066, f_score 0.614583\n",
      "=== total 15100 match 6536\n",
      ">>> step 15200\n",
      "step 15100-15199, precision 0.482659, recall 0.861351, f_score 0.618655\n",
      "=== total 15200 match 6603\n",
      ">>> step 15300\n",
      "step 15200-15299, precision 0.482582, recall 0.861837, f_score 0.618716\n",
      "=== total 15300 match 6631\n",
      ">>> step 15400\n",
      "step 15300-15399, precision 0.484471, recall 0.854233, f_score 0.618286\n",
      "=== total 15400 match 6665\n",
      ">>> step 15500\n",
      "step 15400-15499, precision 0.488258, recall 0.855692, f_score 0.621747\n",
      "=== total 15500 match 6728\n",
      ">>> step 15600\n",
      "step 15500-15599, precision 0.489865, recall 0.850282, f_score 0.621609\n",
      "=== total 15600 match 6759\n",
      ">>> step 15700\n",
      "step 15600-15699, precision 0.492509, recall 0.851231, f_score 0.623988\n",
      "=== total 15700 match 6808\n",
      ">>> step 15800\n",
      "step 15700-15799, precision 0.494083, recall 0.848896, f_score 0.624619\n",
      "=== total 15800 match 6845\n",
      ">>> step 15900\n",
      "step 15800-15899, precision 0.493151, recall 0.848971, f_score 0.623894\n",
      "=== total 15900 match 6862\n",
      ">>> step 16000\n",
      "step 15900-15999, precision 0.494271, recall 0.849875, f_score 0.625034\n",
      "=== total 16000 match 6895\n",
      ">>> step 16100\n",
      "step 16000-16099, precision 0.495743, recall 0.848777, f_score 0.625911\n",
      "=== total 16100 match 6929\n",
      ">>> step 16200\n",
      "step 16100-16199, precision 0.497129, recall 0.847528, f_score 0.626674\n",
      "=== total 16200 match 6966\n",
      ">>> step 16300\n",
      "step 16200-16299, precision 0.498715, recall 0.847573, f_score 0.627945\n",
      "=== total 16300 match 7002\n",
      ">>> step 16400\n",
      "step 16300-16399, precision 0.501277, recall 0.849111, f_score 0.630396\n",
      "=== total 16400 match 7050\n",
      ">>> step 16500\n",
      "step 16400-16499, precision 0.501132, recall 0.842568, f_score 0.628470\n",
      "=== total 16500 match 7070\n",
      ">>> step 16600\n",
      "step 16500-16599, precision 0.504630, recall 0.843970, f_score 0.631607\n",
      "=== total 16600 match 7128\n",
      ">>> step 16700\n",
      "step 16600-16699, precision 0.505585, recall 0.844450, f_score 0.632489\n",
      "=== total 16700 match 7162\n",
      ">>> step 16800\n",
      "step 16700-16799, precision 0.506468, recall 0.839327, f_score 0.631734\n",
      "=== total 16800 match 7189\n",
      ">>> step 16900\n",
      "step 16800-16899, precision 0.505835, recall 0.839327, f_score 0.631241\n",
      "=== total 16900 match 7198\n",
      ">>> step 17000\n",
      "step 16900-16999, precision 0.504503, recall 0.836819, f_score 0.629495\n",
      "=== total 17000 match 7217\n",
      ">>> step 17100\n",
      "step 17000-17099, precision 0.503248, recall 0.833181, f_score 0.627488\n",
      "=== total 17100 match 7235\n",
      ">>> step 17200\n",
      "step 17100-17199, precision 0.501860, recall 0.830938, f_score 0.625773\n",
      "=== total 17200 match 7257\n",
      ">>> step 17300\n",
      "step 17200-17299, precision 0.500825, recall 0.830938, f_score 0.624968\n",
      "=== total 17300 match 7272\n",
      ">>> step 17400\n",
      "step 17300-17399, precision 0.499931, recall 0.830938, f_score 0.624272\n",
      "=== total 17400 match 7285\n",
      ">>> step 17500\n",
      "step 17400-17499, precision 0.499042, recall 0.829163, f_score 0.623077\n",
      "=== total 17500 match 7304\n",
      ">>> step 17600\n",
      "step 17500-17599, precision 0.498496, recall 0.826718, f_score 0.621961\n",
      "=== total 17600 match 7312\n",
      ">>> step 17700\n",
      "step 17600-17699, precision 0.496797, recall 0.826718, f_score 0.620637\n",
      "=== total 17700 match 7337\n",
      ">>> step 17800\n",
      "step 17700-17799, precision 0.495851, recall 0.823171, f_score 0.618898\n",
      "=== total 17800 match 7351\n",
      ">>> step 17900\n",
      "step 17800-17899, precision 0.494774, recall 0.819654, f_score 0.617064\n",
      "=== total 17900 match 7367\n",
      ">>> step 18000\n",
      "step 17900-17999, precision 0.494036, recall 0.812709, f_score 0.614516\n",
      "=== total 18000 match 7378\n",
      ">>> step 18100\n",
      "step 18000-18099, precision 0.492767, recall 0.811262, f_score 0.613120\n",
      "=== total 18100 match 7397\n",
      ">>> step 18200\n",
      "step 18100-18199, precision 0.491770, recall 0.809281, f_score 0.611782\n",
      "=== total 18200 match 7412\n",
      ">>> step 18300\n",
      "step 18200-18299, precision 0.490381, recall 0.809281, f_score 0.610706\n",
      "=== total 18300 match 7433\n",
      ">>> step 18400\n",
      "step 18300-18399, precision 0.488409, recall 0.809281, f_score 0.609175\n",
      "=== total 18400 match 7463\n",
      ">>> step 18500\n",
      "step 18400-18499, precision 0.488776, recall 0.808398, f_score 0.609210\n",
      "=== total 18500 match 7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 18600\n",
      "step 18500-18599, precision 0.488065, recall 0.807947, f_score 0.608529\n",
      "=== total 18600 match 7499\n",
      ">>> step 18700\n",
      "step 18600-18699, precision 0.488859, recall 0.806918, f_score 0.608854\n",
      "=== total 18700 match 7540\n",
      ">>> step 18800\n",
      "step 18700-18799, precision 0.487308, recall 0.806918, f_score 0.607649\n",
      "=== total 18800 match 7564\n",
      ">>> step 18900\n",
      "step 18800-18899, precision 0.486601, recall 0.802351, f_score 0.605802\n",
      "=== total 18900 match 7575\n",
      ">>> step 19000\n",
      "step 18900-18999, precision 0.486504, recall 0.799784, f_score 0.604994\n",
      "=== total 19000 match 7595\n",
      ">>> step 19100\n",
      "step 19000-19099, precision 0.485163, recall 0.799784, f_score 0.603956\n",
      "=== total 19100 match 7616\n",
      ">>> step 19200\n",
      "step 19100-19199, precision 0.484019, recall 0.799784, f_score 0.603068\n",
      "=== total 19200 match 7634\n",
      ">>> step 19300\n",
      "step 19200-19299, precision 0.485688, recall 0.801417, f_score 0.604828\n",
      "=== total 19300 match 7686\n",
      ">>> step 19400\n",
      "step 19300-19399, precision 0.486099, recall 0.802519, f_score 0.605460\n",
      "=== total 19400 match 7733\n",
      ">>> step 19500\n",
      "step 19400-19499, precision 0.488786, recall 0.804811, f_score 0.608196\n",
      "=== total 19500 match 7803\n",
      ">>> step 19600\n",
      "step 19500-19599, precision 0.489050, recall 0.803725, f_score 0.608090\n",
      "=== total 19600 match 7854\n",
      ">>> step 19700\n",
      "step 19600-19699, precision 0.489429, recall 0.804579, f_score 0.608627\n",
      "=== total 19700 match 7899\n",
      ">>> step 19800\n",
      "step 19700-19799, precision 0.488255, recall 0.804579, f_score 0.607718\n",
      "=== total 19800 match 7918\n",
      ">>> step 19900\n",
      "step 19800-19899, precision 0.488194, recall 0.804595, f_score 0.607676\n",
      "=== total 19900 match 7962\n",
      ">>> step 20000\n",
      "step 19900-19999, precision 0.488511, recall 0.805435, f_score 0.608162\n",
      "=== total 20000 match 8008\n",
      ">>> step 20100\n",
      "step 20000-20099, precision 0.488120, recall 0.803604, f_score 0.607336\n",
      "=== total 20100 match 8039\n",
      ">>> step 20200\n",
      "step 20100-20199, precision 0.487294, recall 0.803885, f_score 0.606776\n",
      "=== total 20200 match 8067\n",
      ">>> step 20300\n",
      "step 20200-20299, precision 0.486834, recall 0.799269, f_score 0.605101\n",
      "=== total 20300 match 8089\n",
      ">>> step 20400\n",
      "step 20300-20399, precision 0.486173, recall 0.790446, f_score 0.602049\n",
      "=== total 20400 match 8100\n",
      ">>> step 20500\n",
      "step 20400-20499, precision 0.486244, recall 0.790220, f_score 0.602038\n",
      "=== total 20500 match 8142\n",
      ">>> step 20600\n",
      "step 20500-20599, precision 0.485798, recall 0.787614, f_score 0.600939\n",
      "=== total 20600 match 8168\n",
      ">>> step 20700\n",
      "step 20600-20699, precision 0.484978, recall 0.783235, f_score 0.599035\n",
      "=== total 20700 match 8188\n",
      ">>> step 20800\n",
      "step 20700-20799, precision 0.485210, recall 0.777908, f_score 0.597646\n",
      "=== total 20800 match 8215\n",
      ">>> step 20900\n",
      "step 20800-20899, precision 0.484400, recall 0.774757, f_score 0.596101\n",
      "=== total 20900 match 8237\n",
      ">>> step 21000\n",
      "step 20900-20999, precision 0.483051, recall 0.774757, f_score 0.595078\n",
      "=== total 21000 match 8260\n",
      ">>> step 21100\n",
      "step 21000-21099, precision 0.483074, recall 0.774730, f_score 0.595088\n",
      "=== total 21100 match 8301\n",
      ">>> step 21200\n",
      "step 21100-21199, precision 0.482593, recall 0.772038, f_score 0.593928\n",
      "=== total 21200 match 8330\n",
      ">>> step 21300\n",
      "step 21200-21299, precision 0.484733, recall 0.770865, f_score 0.595196\n",
      "=== total 21300 match 8384\n",
      ">>> step 21400\n",
      "step 21300-21399, precision 0.484366, recall 0.768100, f_score 0.594094\n",
      "=== total 21400 match 8411\n",
      ">>> step 21500\n",
      "step 21400-21499, precision 0.486723, recall 0.768829, f_score 0.596083\n",
      "=== total 21500 match 8473\n",
      ">>> step 21600\n",
      "step 21500-21599, precision 0.489227, recall 0.769996, f_score 0.598310\n",
      "=== total 21600 match 8540\n",
      ">>> step 21700\n",
      "step 21600-21699, precision 0.488803, recall 0.767020, f_score 0.597094\n",
      "=== total 21700 match 8574\n",
      ">>> step 21800\n",
      "step 21700-21799, precision 0.489320, recall 0.766085, f_score 0.597195\n",
      "=== total 21800 match 8614\n",
      ">>> step 21900\n",
      "step 21800-21899, precision 0.488073, recall 0.766085, f_score 0.596265\n",
      "=== total 21900 match 8636\n",
      ">>> step 22000\n",
      "step 21900-21999, precision 0.488707, recall 0.767185, f_score 0.597072\n",
      "=== total 22000 match 8678\n",
      ">>> step 22100\n",
      "step 22000-22099, precision 0.488404, recall 0.767731, f_score 0.597011\n",
      "=== total 22100 match 8710\n",
      ">>> step 22200\n",
      "step 22100-22199, precision 0.488975, recall 0.768816, f_score 0.597765\n",
      "=== total 22200 match 8753\n",
      ">>> step 22300\n",
      "step 22200-22299, precision 0.490583, recall 0.770629, f_score 0.599515\n",
      "=== total 22300 match 8814\n",
      ">>> step 22400\n",
      "step 22300-22399, precision 0.491082, recall 0.771687, f_score 0.600207\n",
      "=== total 22400 match 8858\n",
      ">>> step 22500\n",
      "step 22400-22499, precision 0.491295, recall 0.772382, f_score 0.600577\n",
      "=== total 22500 match 8903\n",
      ">>> step 22600\n",
      "step 22500-22599, precision 0.493310, recall 0.774374, f_score 0.602684\n",
      "=== total 22600 match 8968\n",
      ">>> step 22700\n",
      "step 22600-22699, precision 0.492889, recall 0.774712, f_score 0.602472\n",
      "=== total 22700 match 9000\n",
      ">>> step 22800\n",
      "step 22700-22799, precision 0.493928, recall 0.776197, f_score 0.603697\n",
      "=== total 22800 match 9058\n",
      ">>> step 22900\n",
      "step 22800-22899, precision 0.496274, recall 0.777911, f_score 0.605967\n",
      "=== total 22900 match 9126\n",
      ">>> step 23000\n",
      "step 22900-22999, precision 0.495189, recall 0.777911, f_score 0.605158\n",
      "=== total 23000 match 9146\n",
      ">>> step 23100\n",
      "step 23000-23099, precision 0.494001, recall 0.777911, f_score 0.604270\n",
      "=== total 23100 match 9168\n",
      ">>> step 23200\n",
      "step 23100-23199, precision 0.494303, recall 0.778899, f_score 0.604793\n",
      "=== total 23200 match 9215\n",
      ">>> step 23300\n",
      "step 23200-23299, precision 0.496933, recall 0.781255, f_score 0.607472\n",
      "=== total 23300 match 9293\n",
      ">>> step 23400\n",
      "step 23300-23399, precision 0.498074, recall 0.782616, f_score 0.608735\n",
      "=== total 23400 match 9346\n",
      ">>> step 23500\n",
      "step 23400-23499, precision 0.499841, recall 0.784500, f_score 0.610625\n",
      "=== total 23500 match 9417\n",
      ">>> step 23600\n",
      "step 23500-23599, precision 0.502844, recall 0.786880, f_score 0.613585\n",
      "=== total 23600 match 9494\n",
      ">>> step 23700\n",
      "step 23600-23699, precision 0.505072, recall 0.788700, f_score 0.615797\n",
      "=== total 23700 match 9563\n",
      ">>> step 23800\n",
      "step 23700-23799, precision 0.506909, recall 0.790249, f_score 0.617634\n",
      "=== total 23800 match 9625\n",
      ">>> step 23900\n",
      "step 23800-23899, precision 0.505648, recall 0.790249, f_score 0.616697\n",
      "=== total 23900 match 9649\n",
      ">>> step 24000\n",
      "step 23900-23999, precision 0.506035, recall 0.791129, f_score 0.617253\n",
      "=== total 24000 match 9693\n",
      ">>> step 24100\n",
      "step 24000-24099, precision 0.504941, recall 0.791129, f_score 0.616438\n",
      "=== total 24100 match 9714\n",
      ">>> step 24200\n",
      "step 24100-24199, precision 0.505841, recall 0.792041, f_score 0.617386\n",
      "=== total 24200 match 9758\n",
      ">>> step 24300\n",
      "step 24200-24299, precision 0.507796, recall 0.793219, f_score 0.619199\n",
      "=== total 24300 match 9813\n",
      ">>> step 24400\n",
      "step 24300-24399, precision 0.507755, recall 0.794071, f_score 0.619427\n",
      "=== total 24400 match 9865\n",
      ">>> step 24500\n",
      "step 24400-24499, precision 0.507560, recall 0.794916, f_score 0.619540\n",
      "=== total 24500 match 9920\n",
      ">>> step 24600\n",
      "step 24500-24599, precision 0.509417, recall 0.796523, f_score 0.621410\n",
      "=== total 24600 match 9982\n",
      ">>> step 24700\n",
      "step 24600-24699, precision 0.509575, recall 0.797036, f_score 0.621684\n",
      "=== total 24700 match 10026\n",
      ">>> step 24800\n",
      "step 24700-24799, precision 0.509473, recall 0.797887, f_score 0.621867\n",
      "=== total 24800 match 10081\n",
      ">>> step 24900\n",
      "step 24800-24899, precision 0.511575, recall 0.799661, f_score 0.623971\n",
      "=== total 24900 match 10151\n",
      ">>> step 25000\n",
      "step 24900-24999, precision 0.510770, recall 0.799661, f_score 0.623372\n",
      "=== total 25000 match 10167\n",
      ">>> step 25100\n",
      "step 25000-25099, precision 0.510922, recall 0.800368, f_score 0.623700\n",
      "=== total 25100 match 10209\n",
      ">>> step 25200\n",
      "step 25100-25199, precision 0.511261, recall 0.801222, f_score 0.624211\n",
      "=== total 25200 match 10257\n",
      ">>> step 25300\n",
      "step 25200-25299, precision 0.513898, recall 0.803088, f_score 0.626742\n",
      "=== total 25300 match 10325\n",
      ">>> step 25400\n",
      "step 25300-25399, precision 0.514746, recall 0.804125, f_score 0.627688\n",
      "=== total 25400 match 10376\n",
      ">>> step 25500\n",
      "step 25400-25499, precision 0.517281, recall 0.805937, f_score 0.630124\n",
      "=== total 25500 match 10445\n",
      ">>> step 25600\n",
      "step 25500-25599, precision 0.519187, recall 0.807743, f_score 0.632090\n",
      "=== total 25600 match 10528\n",
      ">>> step 25700\n",
      "step 25600-25699, precision 0.519754, recall 0.808676, f_score 0.632796\n",
      "=== total 25700 match 10580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 25800\n",
      "step 25700-25799, precision 0.521249, recall 0.809816, f_score 0.634252\n",
      "=== total 25800 match 10636\n",
      ">>> step 25900\n",
      "step 25800-25899, precision 0.520417, recall 0.809816, f_score 0.633636\n",
      "=== total 25900 match 10653\n",
      ">>> step 26000\n",
      "step 25900-25999, precision 0.519876, recall 0.809017, f_score 0.632991\n",
      "=== total 26000 match 10666\n",
      ">>> step 26100\n",
      "step 26000-26099, precision 0.519659, recall 0.807653, f_score 0.632412\n",
      "=== total 26100 match 10682\n",
      ">>> step 26200\n",
      "step 26100-26199, precision 0.519319, recall 0.807653, f_score 0.632160\n",
      "=== total 26200 match 10689\n",
      ">>> step 26300\n",
      "step 26200-26299, precision 0.519432, recall 0.807318, f_score 0.632141\n",
      "=== total 26300 match 10704\n",
      ">>> step 26400\n",
      "step 26300-26399, precision 0.518608, recall 0.807318, f_score 0.631531\n",
      "=== total 26400 match 10721\n",
      ">>> step 26500\n",
      "step 26400-26499, precision 0.517594, recall 0.807318, f_score 0.630779\n",
      "=== total 26500 match 10742\n",
      ">>> step 26600\n",
      "step 26500-26599, precision 0.516873, recall 0.807318, f_score 0.630243\n",
      "=== total 26600 match 10757\n",
      ">>> step 26700\n",
      "step 26600-26699, precision 0.516105, recall 0.807318, f_score 0.629672\n",
      "=== total 26700 match 10773\n",
      ">>> step 26800\n",
      "step 26700-26799, precision 0.515818, recall 0.806265, f_score 0.629137\n",
      "=== total 26800 match 10779\n",
      ">>> step 26900\n",
      "step 26800-26899, precision 0.514958, recall 0.805680, f_score 0.628320\n",
      "=== total 26900 match 10797\n",
      ">>> step 27000\n",
      "step 26900-26999, precision 0.514053, recall 0.805680, f_score 0.627646\n",
      "=== total 27000 match 10816\n",
      ">>> step 27100\n",
      "step 27000-27099, precision 0.513294, recall 0.804049, f_score 0.626585\n",
      "=== total 27100 match 10832\n",
      ">>> step 27200\n",
      "step 27100-27199, precision 0.512962, recall 0.803236, f_score 0.626091\n",
      "=== total 27200 match 10839\n",
      ">>> step 27300\n",
      "step 27200-27299, precision 0.512442, recall 0.803236, f_score 0.625703\n",
      "=== total 27300 match 10850\n",
      ">>> step 27400\n",
      "step 27300-27399, precision 0.511829, recall 0.803236, f_score 0.625246\n",
      "=== total 27400 match 10863\n",
      ">>> step 27500\n",
      "step 27400-27499, precision 0.512383, recall 0.803972, f_score 0.625882\n",
      "=== total 27500 match 10902\n",
      ">>> step 27600\n",
      "step 27500-27599, precision 0.513341, recall 0.804756, f_score 0.626834\n",
      "=== total 27600 match 10944\n",
      ">>> step 27700\n",
      "step 27600-27699, precision 0.513171, recall 0.804861, f_score 0.626739\n",
      "=== total 27700 match 10971\n",
      ">>> step 27800\n",
      "step 27700-27799, precision 0.514701, recall 0.806025, f_score 0.628233\n",
      "=== total 27800 match 11020\n",
      ">>> step 27900\n",
      "step 27800-27899, precision 0.514842, recall 0.806493, f_score 0.628480\n",
      "=== total 27900 match 11050\n",
      ">>> step 28000\n",
      "step 27900-27999, precision 0.516835, recall 0.807909, f_score 0.630394\n",
      "=== total 28000 match 11108\n",
      ">>> step 28100\n",
      "step 28000-28099, precision 0.518462, recall 0.809091, f_score 0.631964\n",
      "=== total 28100 match 11158\n",
      ">>> step 28200\n",
      "step 28100-28199, precision 0.520546, recall 0.810548, f_score 0.633956\n",
      "=== total 28200 match 11219\n",
      ">>> step 28300\n",
      "step 28200-28299, precision 0.521878, recall 0.811594, f_score 0.635264\n",
      "=== total 28300 match 11267\n",
      ">>> step 28400\n",
      "step 28300-28399, precision 0.521092, recall 0.811594, f_score 0.634681\n",
      "=== total 28400 match 11284\n",
      ">>> step 28500\n",
      "step 28400-28499, precision 0.520492, recall 0.811594, f_score 0.634236\n",
      "=== total 28500 match 11297\n",
      ">>> step 28600\n",
      "step 28500-28599, precision 0.519848, recall 0.811594, f_score 0.633757\n",
      "=== total 28600 match 11311\n",
      ">>> step 28700\n",
      "step 28600-28699, precision 0.519343, recall 0.809249, f_score 0.632666\n",
      "=== total 28700 match 11322\n",
      ">>> step 28800\n",
      "step 28700-28799, precision 0.518701, recall 0.809249, f_score 0.632190\n",
      "=== total 28800 match 11336\n",
      ">>> step 28900\n",
      "step 28800-28899, precision 0.518107, recall 0.807692, f_score 0.631274\n",
      "=== total 28900 match 11349\n",
      ">>> step 29000\n",
      "step 28900-28999, precision 0.517469, recall 0.807692, f_score 0.630800\n",
      "=== total 29000 match 11363\n",
      ">>> step 29100\n",
      "step 29000-29099, precision 0.516878, recall 0.806253, f_score 0.629921\n",
      "=== total 29100 match 11376\n",
      ">>> step 29200\n",
      "step 29100-29199, precision 0.516466, recall 0.805506, f_score 0.629388\n",
      "=== total 29200 match 11387\n",
      ">>> step 29300\n",
      "step 29200-29299, precision 0.515696, recall 0.805506, f_score 0.628816\n",
      "=== total 29300 match 11404\n",
      ">>> step 29400\n",
      "step 29300-29399, precision 0.515732, recall 0.806037, f_score 0.629004\n",
      "=== total 29400 match 11442\n",
      ">>> step 29500\n",
      "step 29400-29499, precision 0.516548, recall 0.805952, f_score 0.629584\n",
      "=== total 29500 match 11482\n",
      ">>> step 29600\n",
      "step 29500-29599, precision 0.516670, recall 0.806478, f_score 0.629835\n",
      "=== total 29600 match 11518\n",
      ">>> step 29700\n",
      "step 29600-29699, precision 0.517068, recall 0.805942, f_score 0.629968\n",
      "=== total 29700 match 11542\n",
      ">>> step 29800\n",
      "step 29700-29799, precision 0.517018, recall 0.806386, f_score 0.630066\n",
      "=== total 29800 match 11576\n",
      ">>> step 29900\n",
      "step 29800-29899, precision 0.517307, recall 0.806985, f_score 0.630463\n",
      "=== total 29900 match 11614\n",
      ">>> step 30000\n",
      "step 29900-29999, precision 0.516770, recall 0.805064, f_score 0.629478\n",
      "=== total 30000 match 11628\n",
      ">>> step 30100\n",
      "step 30000-30099, precision 0.515927, recall 0.805064, f_score 0.628852\n",
      "=== total 30100 match 11647\n",
      ">>> step 30200\n",
      "step 30100-30199, precision 0.515936, recall 0.802719, f_score 0.628142\n",
      "=== total 30200 match 11672\n",
      ">>> step 30300\n",
      "step 30200-30299, precision 0.515273, recall 0.802719, f_score 0.627651\n",
      "=== total 30300 match 11687\n",
      ">>> step 30400\n",
      "step 30300-30399, precision 0.516206, recall 0.803292, f_score 0.628518\n",
      "=== total 30400 match 11724\n",
      ">>> step 30500\n",
      "step 30400-30499, precision 0.517953, recall 0.804589, f_score 0.630209\n",
      "=== total 30500 match 11781\n",
      ">>> step 30600\n",
      "step 30500-30599, precision 0.518941, recall 0.805064, f_score 0.631086\n",
      "=== total 30600 match 11826\n",
      ">>> step 30700\n",
      "step 30600-30699, precision 0.519252, recall 0.805726, f_score 0.631520\n",
      "=== total 30700 match 11869\n",
      ">>> step 30800\n",
      "step 30700-30799, precision 0.519493, recall 0.804240, f_score 0.631240\n",
      "=== total 30800 match 11902\n",
      ">>> step 30900\n",
      "step 30800-30899, precision 0.520539, recall 0.803876, f_score 0.631900\n",
      "=== total 30900 match 11953\n",
      ">>> step 31000\n",
      "step 30900-30999, precision 0.520164, recall 0.803560, f_score 0.631526\n",
      "=== total 31000 match 11977\n",
      ">>> step 31100\n",
      "step 31000-31099, precision 0.521887, recall 0.804894, f_score 0.633207\n",
      "=== total 31100 match 12039\n",
      ">>> step 31200\n",
      "step 31100-31199, precision 0.523266, recall 0.806086, f_score 0.634591\n",
      "=== total 31200 match 12099\n",
      ">>> step 31300\n",
      "step 31200-31299, precision 0.525495, recall 0.807775, f_score 0.636753\n",
      "=== total 31300 match 12179\n",
      ">>> step 31400\n",
      "step 31300-31399, precision 0.527288, recall 0.809076, f_score 0.638473\n",
      "=== total 31400 match 12240\n",
      ">>> step 31500\n",
      "step 31400-31499, precision 0.527576, recall 0.809601, f_score 0.638848\n",
      "=== total 31500 match 12275\n",
      ">>> step 31600\n",
      "step 31500-31599, precision 0.527447, recall 0.809601, f_score 0.638753\n",
      "=== total 31600 match 12278\n",
      ">>> step 31700\n",
      "step 31600-31699, precision 0.528418, recall 0.810360, f_score 0.639701\n",
      "=== total 31700 match 12316\n",
      ">>> step 31800\n",
      "step 31700-31799, precision 0.528467, recall 0.810045, f_score 0.639639\n",
      "=== total 31800 match 12330\n",
      ">>> step 31900\n",
      "step 31800-31899, precision 0.528082, recall 0.810045, f_score 0.639356\n",
      "=== total 31900 match 12339\n",
      ">>> step 32000\n",
      "step 31900-31999, precision 0.528636, recall 0.809789, f_score 0.639683\n",
      "=== total 32000 match 12362\n",
      ">>> step 32100\n",
      "step 32000-32099, precision 0.528465, recall 0.808487, f_score 0.639151\n",
      "=== total 32100 match 12366\n",
      ">>> step 32200\n",
      "step 32100-32199, precision 0.528399, recall 0.806810, f_score 0.638578\n",
      "=== total 32200 match 12377\n",
      ">>> step 32300\n",
      "step 32200-32299, precision 0.528781, recall 0.805774, f_score 0.638532\n",
      "=== total 32300 match 12404\n",
      ">>> step 32400\n",
      "step 32300-32399, precision 0.528521, recall 0.805798, f_score 0.638350\n",
      "=== total 32400 match 12412\n",
      ">>> step 32500\n",
      "step 32400-32499, precision 0.528265, recall 0.805798, f_score 0.638163\n",
      "=== total 32500 match 12418\n",
      ">>> step 32600\n",
      "step 32500-32599, precision 0.527798, recall 0.805798, f_score 0.637822\n",
      "=== total 32600 match 12429\n",
      ">>> step 32700\n",
      "step 32600-32699, precision 0.527501, recall 0.805600, f_score 0.637543\n",
      "=== total 32700 match 12436\n",
      ">>> step 32800\n",
      "step 32700-32799, precision 0.527077, recall 0.805402, f_score 0.637172\n",
      "=== total 32800 match 12446\n",
      ">>> step 32900\n",
      "step 32800-32899, precision 0.526738, recall 0.805402, f_score 0.636924\n",
      "=== total 32900 match 12454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 33000\n",
      "step 32900-32999, precision 0.526274, recall 0.805402, f_score 0.636584\n",
      "=== total 33000 match 12465\n",
      ">>> step 33100\n",
      "step 33000-33099, precision 0.526020, recall 0.805402, f_score 0.636399\n",
      "=== total 33100 match 12471\n",
      ">>> step 33200\n",
      "step 33100-33199, precision 0.525725, recall 0.805402, f_score 0.636183\n",
      "=== total 33200 match 12478\n",
      ">>> step 33300\n",
      "step 33200-33299, precision 0.525431, recall 0.804020, f_score 0.635536\n",
      "=== total 33300 match 12485\n",
      ">>> step 33400\n",
      "step 33300-33399, precision 0.525220, recall 0.802643, f_score 0.634951\n",
      "=== total 33400 match 12490\n",
      ">>> step 33500\n",
      "step 33400-33499, precision 0.525010, recall 0.800586, f_score 0.634153\n",
      "=== total 33500 match 12495\n",
      ">>> step 33600\n",
      "step 33500-33599, precision 0.524674, recall 0.800586, f_score 0.633908\n",
      "=== total 33600 match 12503\n",
      ">>> step 33700\n",
      "step 33600-33699, precision 0.524339, recall 0.800586, f_score 0.633663\n",
      "=== total 33700 match 12511\n",
      ">>> step 33800\n",
      "step 33700-33799, precision 0.524087, recall 0.800586, f_score 0.633480\n",
      "=== total 33800 match 12517\n",
      ">>> step 33900\n",
      "step 33800-33899, precision 0.523794, recall 0.800586, f_score 0.633266\n",
      "=== total 33900 match 12524\n",
      ">>> step 34000\n",
      "step 33900-33999, precision 0.523293, recall 0.800586, f_score 0.632899\n",
      "=== total 34000 match 12536\n",
      ">>> step 34100\n",
      "step 34000-34099, precision 0.523039, recall 0.798758, f_score 0.632142\n",
      "=== total 34100 match 12544\n",
      ">>> step 34200\n",
      "step 34100-34199, precision 0.522744, recall 0.796940, f_score 0.631356\n",
      "=== total 34200 match 12553\n",
      ">>> step 34300\n",
      "step 34200-34299, precision 0.522203, recall 0.796940, f_score 0.630962\n",
      "=== total 34300 match 12566\n",
      ">>> step 34400\n",
      "step 34300-34399, precision 0.521909, recall 0.796384, f_score 0.630573\n",
      "=== total 34400 match 12575\n",
      ">>> step 34500\n",
      "step 34400-34499, precision 0.521760, recall 0.796557, f_score 0.630518\n",
      "=== total 34500 match 12592\n",
      ">>> step 34600\n",
      "step 34500-34599, precision 0.521304, recall 0.796557, f_score 0.630186\n",
      "=== total 34600 match 12603\n",
      ">>> step 34700\n",
      "step 34600-34699, precision 0.521584, recall 0.795867, f_score 0.630174\n",
      "=== total 34700 match 12625\n",
      ">>> step 34800\n",
      "step 34700-34799, precision 0.521130, recall 0.795867, f_score 0.629842\n",
      "=== total 34800 match 12636\n",
      ">>> step 34900\n",
      "step 34800-34899, precision 0.520860, recall 0.796039, f_score 0.629699\n",
      "=== total 34900 match 12656\n",
      ">>> step 35000\n",
      "step 34900-34999, precision 0.521842, recall 0.796971, f_score 0.630708\n",
      "=== total 35000 match 12705\n",
      ">>> step 35100\n",
      "step 35000-35099, precision 0.522554, recall 0.797534, f_score 0.631404\n",
      "=== total 35100 match 12747\n",
      ">>> step 35200\n",
      "step 35100-35199, precision 0.522631, recall 0.797848, f_score 0.631559\n",
      "=== total 35200 match 12770\n",
      ">>> step 35300\n",
      "step 35200-35299, precision 0.522473, recall 0.796948, f_score 0.631161\n",
      "=== total 35300 match 12793\n",
      ">>> step 35400\n",
      "step 35300-35399, precision 0.522543, recall 0.796362, f_score 0.631029\n",
      "=== total 35400 match 12820\n",
      ">>> step 35500\n",
      "step 35400-35499, precision 0.522396, recall 0.794738, f_score 0.630411\n",
      "=== total 35500 match 12837\n",
      ">>> step 35600\n",
      "step 35500-35599, precision 0.522057, recall 0.793989, f_score 0.629929\n",
      "=== total 35600 match 12853\n",
      ">>> step 35700\n",
      "step 35600-35699, precision 0.522620, recall 0.794597, f_score 0.630529\n",
      "=== total 35700 match 12887\n",
      ">>> step 35800\n",
      "step 35700-35799, precision 0.522214, recall 0.794597, f_score 0.630234\n",
      "=== total 35800 match 12897\n",
      ">>> step 35900\n",
      "step 35800-35899, precision 0.521729, recall 0.794597, f_score 0.629881\n",
      "=== total 35900 match 12909\n",
      ">>> step 36000\n",
      "step 35900-35999, precision 0.521366, recall 0.794597, f_score 0.629616\n",
      "=== total 36000 match 12918\n",
      ">>> step 36100\n",
      "step 36000-36099, precision 0.521400, recall 0.794935, f_score 0.629747\n",
      "=== total 36100 match 12944\n",
      ">>> step 36200\n",
      "step 36100-36199, precision 0.520796, recall 0.794935, f_score 0.629307\n",
      "=== total 36200 match 12959\n",
      ">>> step 36300\n",
      "step 36200-36299, precision 0.520533, recall 0.795104, f_score 0.629167\n",
      "=== total 36300 match 12979\n",
      ">>> step 36400\n",
      "step 36300-36399, precision 0.520246, recall 0.795152, f_score 0.628973\n",
      "=== total 36400 match 12990\n",
      ">>> step 36500\n",
      "step 36400-36499, precision 0.519991, recall 0.795273, f_score 0.628824\n",
      "=== total 36500 match 13006\n",
      ">>> step 36600\n",
      "step 36500-36599, precision 0.520107, recall 0.795609, f_score 0.629014\n",
      "=== total 36600 match 13030\n",
      ">>> step 36700\n",
      "step 36600-36699, precision 0.519589, recall 0.795609, f_score 0.628635\n",
      "=== total 36700 match 13043\n",
      ">>> step 36800\n",
      "step 36700-36799, precision 0.519111, recall 0.795609, f_score 0.628285\n",
      "=== total 36800 match 13055\n",
      ">>> step 36900\n",
      "step 36800-36899, precision 0.518833, recall 0.793188, f_score 0.627326\n",
      "=== total 36900 match 13062\n",
      ">>> step 37000\n",
      "step 36900-36999, precision 0.518674, recall 0.789124, f_score 0.625935\n",
      "=== total 37000 match 13066\n",
      ">>> step 37100\n",
      "step 37000-37099, precision 0.518317, recall 0.786743, f_score 0.624925\n",
      "=== total 37100 match 13075\n",
      ">>> step 37200\n",
      "step 37100-37199, precision 0.518000, recall 0.784829, f_score 0.624091\n",
      "=== total 37200 match 13083\n",
      ">>> step 37300\n",
      "step 37200-37299, precision 0.517763, recall 0.782022, f_score 0.623029\n",
      "=== total 37300 match 13089\n",
      ">>> step 37400\n",
      "step 37300-37399, precision 0.517447, recall 0.779682, f_score 0.622057\n",
      "=== total 37400 match 13097\n",
      ">>> step 37500\n",
      "step 37400-37499, precision 0.517170, recall 0.775489, f_score 0.620519\n",
      "=== total 37500 match 13104\n",
      ">>> step 37600\n",
      "step 37500-37599, precision 0.516894, recall 0.772748, f_score 0.619442\n",
      "=== total 37600 match 13111\n",
      ">>> step 37700\n",
      "step 37600-37699, precision 0.516737, recall 0.769065, f_score 0.618142\n",
      "=== total 37700 match 13115\n",
      ">>> step 37800\n",
      "step 37700-37799, precision 0.516461, recall 0.766629, f_score 0.617157\n",
      "=== total 37800 match 13122\n",
      ">>> step 37900\n",
      "step 37800-37899, precision 0.515910, recall 0.766629, f_score 0.616764\n",
      "=== total 37900 match 13136\n",
      ">>> step 38000\n",
      "step 37900-37999, precision 0.515344, recall 0.767174, f_score 0.616535\n",
      "=== total 38000 match 13197\n",
      ">>> step 38100\n",
      "step 38000-38099, precision 0.515014, recall 0.767829, f_score 0.616510\n",
      "=== total 38100 match 13254\n",
      ">>> step 38200\n",
      "step 38100-38199, precision 0.514919, recall 0.768480, f_score 0.616652\n",
      "=== total 38200 match 13305\n",
      ">>> step 38300\n",
      "step 38200-38299, precision 0.512953, recall 0.768480, f_score 0.615240\n",
      "=== total 38300 match 13356\n",
      ">>> step 38400\n",
      "step 38300-38399, precision 0.512637, recall 0.769128, f_score 0.615219\n",
      "=== total 38400 match 13413\n",
      ">>> step 38500\n",
      "step 38400-38499, precision 0.510940, recall 0.769463, f_score 0.614102\n",
      "=== total 38500 match 13483\n",
      ">>> step 38600\n",
      "step 38500-38599, precision 0.509466, recall 0.769463, f_score 0.613037\n",
      "=== total 38600 match 13522\n",
      ">>> step 38700\n",
      "step 38600-38699, precision 0.508526, recall 0.769463, f_score 0.612356\n",
      "=== total 38700 match 13547\n",
      ">>> step 38800\n",
      "step 38700-38799, precision 0.507178, recall 0.769463, f_score 0.611377\n",
      "=== total 38800 match 13583\n",
      ">>> step 38900\n",
      "step 38800-38899, precision 0.506457, recall 0.769797, f_score 0.610959\n",
      "=== total 38900 match 13628\n",
      ">>> step 39000\n",
      "step 38900-38999, precision 0.506360, recall 0.770437, f_score 0.611089\n",
      "=== total 39000 match 13680\n",
      ">>> step 39100\n",
      "step 39000-39099, precision 0.505215, recall 0.770437, f_score 0.610255\n",
      "=== total 39100 match 13711\n",
      ">>> step 39200\n",
      "step 39100-39199, precision 0.503818, recall 0.770437, f_score 0.609235\n",
      "=== total 39200 match 13749\n",
      ">>> step 39300\n",
      "step 39200-39299, precision 0.503154, recall 0.770769, f_score 0.608852\n",
      "=== total 39300 match 13793\n",
      ">>> step 39400\n",
      "step 39300-39399, precision 0.502207, recall 0.770769, f_score 0.608158\n",
      "=== total 39400 match 13819\n",
      ">>> step 39500\n",
      "step 39400-39499, precision 0.501370, recall 0.769963, f_score 0.607294\n",
      "=== total 39500 match 13866\n",
      ">>> step 39600\n",
      "step 39500-39599, precision 0.500144, recall 0.769793, f_score 0.606341\n",
      "=== total 39600 match 13900\n",
      ">>> step 39700\n",
      "step 39600-39699, precision 0.499140, recall 0.768942, f_score 0.605339\n",
      "=== total 39700 match 13948\n",
      ">>> step 39800\n",
      "step 39700-39799, precision 0.499143, recall 0.769654, f_score 0.605562\n",
      "=== total 39800 match 14004\n",
      ">>> step 39900\n",
      "step 39800-39899, precision 0.499110, recall 0.770067, f_score 0.605665\n",
      "=== total 39900 match 14051\n",
      ">>> step 40000\n",
      "step 39900-39999, precision 0.498969, recall 0.769880, f_score 0.605504\n",
      "=== total 40000 match 14067\n",
      ">>> step 40100\n",
      "step 40000-40099, precision 0.498759, recall 0.769306, f_score 0.605171\n",
      "=== total 40100 match 14101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 40200\n",
      "step 40100-40199, precision 0.498517, recall 0.769936, f_score 0.605188\n",
      "=== total 40200 match 14158\n",
      ">>> step 40300\n",
      "step 40200-40299, precision 0.497747, recall 0.770261, f_score 0.604721\n",
      "=== total 40300 match 14206\n",
      ">>> step 40400\n",
      "step 40300-40399, precision 0.497686, recall 0.770885, f_score 0.604867\n",
      "=== total 40400 match 14258\n",
      ">>> step 40500\n",
      "step 40400-40499, precision 0.496502, recall 0.770885, f_score 0.603992\n",
      "=== total 40500 match 14292\n",
      ">>> step 40600\n",
      "step 40500-40599, precision 0.496444, recall 0.771398, f_score 0.604107\n",
      "=== total 40600 match 14342\n",
      ">>> step 40700\n",
      "step 40600-40699, precision 0.495511, recall 0.771398, f_score 0.603415\n",
      "=== total 40700 match 14369\n",
      ">>> step 40800\n",
      "step 40700-40799, precision 0.495458, recall 0.772015, f_score 0.603565\n",
      "=== total 40800 match 14421\n",
      ">>> step 40900\n",
      "step 40800-40899, precision 0.494532, recall 0.772015, f_score 0.602877\n",
      "=== total 40900 match 14448\n",
      ">>> step 41000\n",
      "step 40900-40999, precision 0.493167, recall 0.772015, f_score 0.601862\n",
      "=== total 41000 match 14488\n",
      ">>> step 41100\n",
      "step 41000-41099, precision 0.492185, recall 0.772089, f_score 0.601152\n",
      "=== total 41100 match 14523\n",
      ">>> step 41200\n",
      "step 41100-41199, precision 0.491437, recall 0.772727, f_score 0.600787\n",
      "=== total 41200 match 14598\n",
      ">>> step 41300\n",
      "step 41200-41299, precision 0.490173, recall 0.772947, f_score 0.599908\n",
      "=== total 41300 match 14654\n",
      ">>> step 41400\n",
      "step 41300-41399, precision 0.488872, recall 0.772947, f_score 0.598933\n",
      "=== total 41400 match 14693\n",
      ">>> step 41500\n",
      "step 41400-41499, precision 0.488314, recall 0.773557, f_score 0.598696\n",
      "=== total 41500 match 14761\n",
      ">>> step 41600\n",
      "step 41500-41599, precision 0.487793, recall 0.774162, f_score 0.598486\n",
      "=== total 41600 match 14828\n",
      ">>> step 41700\n",
      "step 41600-41699, precision 0.486677, recall 0.774162, f_score 0.597645\n",
      "=== total 41700 match 14862\n",
      ">>> step 41800\n",
      "step 41700-41799, precision 0.485783, recall 0.774263, f_score 0.597000\n",
      "=== total 41800 match 14912\n",
      ">>> step 41900\n",
      "step 41800-41899, precision 0.485590, recall 0.774696, f_score 0.596983\n",
      "=== total 41900 match 14955\n",
      ">>> step 42000\n",
      "step 41900-41999, precision 0.485208, recall 0.775176, f_score 0.596836\n",
      "=== total 42000 match 15008\n",
      ">>> step 42100\n",
      "step 42000-42099, precision 0.484176, recall 0.775176, f_score 0.596055\n",
      "=== total 42100 match 15040\n",
      ">>> step 42200\n",
      "step 42100-42199, precision 0.483753, recall 0.775486, f_score 0.595826\n",
      "=== total 42200 match 15080\n",
      ">>> step 42300\n",
      "step 42200-42299, precision 0.483256, recall 0.775653, f_score 0.595498\n",
      "=== total 42300 match 15110\n",
      ">>> step 42400\n",
      "step 42300-42399, precision 0.482977, recall 0.776081, f_score 0.595412\n",
      "=== total 42400 match 15156\n",
      ">>> step 42500\n",
      "step 42400-42499, precision 0.481706, recall 0.776081, f_score 0.594445\n",
      "=== total 42500 match 15196\n",
      ">>> step 42600\n",
      "step 42500-42599, precision 0.480504, recall 0.776081, f_score 0.593530\n",
      "=== total 42600 match 15234\n",
      ">>> step 42700\n",
      "step 42600-42699, precision 0.480474, recall 0.776673, f_score 0.593679\n",
      "=== total 42700 match 15287\n",
      ">>> step 42800\n",
      "step 42700-42799, precision 0.480506, recall 0.777262, f_score 0.593876\n",
      "=== total 42800 match 15338\n",
      ">>> step 42900\n",
      "step 42800-42899, precision 0.479350, recall 0.777262, f_score 0.592992\n",
      "=== total 42900 match 15375\n",
      ">>> step 43000\n",
      "step 42900-42999, precision 0.477920, recall 0.777262, f_score 0.591897\n",
      "=== total 43000 match 15421\n",
      ">>> step 43100\n",
      "step 43000-43099, precision 0.477586, recall 0.777567, f_score 0.591729\n",
      "=== total 43100 match 15459\n",
      ">>> step 43200\n",
      "step 43100-43199, precision 0.477166, recall 0.778151, f_score 0.591575\n",
      "=== total 43200 match 15525\n",
      ">>> step 43300\n",
      "step 43200-43299, precision 0.476872, recall 0.778732, f_score 0.591517\n",
      "=== total 43300 match 15587\n",
      ">>> step 43400\n",
      "step 43300-43399, precision 0.475803, recall 0.778732, f_score 0.590694\n",
      "=== total 43400 match 15622\n",
      ">>> step 43500\n",
      "step 43400-43499, precision 0.474497, recall 0.778732, f_score 0.589687\n",
      "=== total 43500 match 15665\n",
      ">>> step 43600\n",
      "step 43500-43599, precision 0.474548, recall 0.779310, f_score 0.589892\n",
      "=== total 43600 match 15716\n",
      ">>> step 43700\n",
      "step 43600-43699, precision 0.474749, recall 0.779885, f_score 0.590212\n",
      "=== total 43700 match 15762\n",
      ">>> step 43800\n",
      "step 43700-43799, precision 0.476070, recall 0.780715, f_score 0.591470\n",
      "=== total 43800 match 15817\n",
      ">>> step 43900\n",
      "step 43800-43899, precision 0.477177, recall 0.781743, f_score 0.592619\n",
      "=== total 43900 match 15883\n",
      ">>> step 44000\n",
      "step 43900-43999, precision 0.476008, recall 0.781743, f_score 0.591716\n",
      "=== total 44000 match 15922\n",
      ">>> step 44100\n",
      "step 44000-44099, precision 0.475000, recall 0.781788, f_score 0.590950\n",
      "=== total 44100 match 15960\n",
      ">>> step 44200\n",
      "step 44100-44199, precision 0.476066, recall 0.782760, f_score 0.592052\n",
      "=== total 44200 match 16023\n",
      ">>> step 44300\n",
      "step 44200-44299, precision 0.476019, recall 0.783134, f_score 0.592123\n",
      "=== total 44300 match 16075\n",
      ">>> step 44400\n",
      "step 44300-44399, precision 0.474984, recall 0.783134, f_score 0.591322\n",
      "=== total 44400 match 16110\n",
      ">>> step 44500\n",
      "step 44400-44499, precision 0.474190, recall 0.783134, f_score 0.590706\n",
      "=== total 44500 match 16137\n",
      ">>> step 44600\n",
      "step 44500-44599, precision 0.473368, recall 0.783134, f_score 0.590068\n",
      "=== total 44600 match 16165\n",
      ">>> step 44700\n",
      "step 44600-44699, precision 0.472567, recall 0.783245, f_score 0.589476\n",
      "=== total 44700 match 16203\n",
      ">>> step 44800\n",
      "step 44700-44799, precision 0.472231, recall 0.783709, f_score 0.589346\n",
      "=== total 44800 match 16259\n",
      ">>> step 44900\n",
      "step 44800-44899, precision 0.472794, recall 0.784545, f_score 0.590021\n",
      "=== total 44900 match 16320\n",
      ">>> step 45000\n",
      "step 44900-44999, precision 0.471552, recall 0.784545, f_score 0.589053\n",
      "=== total 45000 match 16363\n",
      ">>> step 45100\n",
      "step 45000-45099, precision 0.470746, recall 0.784545, f_score 0.588424\n",
      "=== total 45100 match 16391\n",
      ">>> step 45200\n",
      "step 45100-45199, precision 0.470860, recall 0.784910, f_score 0.588616\n",
      "=== total 45200 match 16438\n",
      ">>> step 45300\n",
      "step 45200-45299, precision 0.470174, recall 0.784910, f_score 0.588079\n",
      "=== total 45300 match 16462\n",
      ">>> step 45400\n",
      "step 45300-45399, precision 0.470816, recall 0.785534, f_score 0.588756\n",
      "=== total 45400 match 16516\n",
      ">>> step 45500\n",
      "step 45400-45499, precision 0.469764, recall 0.785534, f_score 0.587933\n",
      "=== total 45500 match 16553\n",
      ">>> step 45600\n",
      "step 45500-45599, precision 0.470744, recall 0.786483, f_score 0.588966\n",
      "=== total 45600 match 16612\n",
      ">>> step 45700\n",
      "step 45600-45699, precision 0.471706, recall 0.787451, f_score 0.589991\n",
      "=== total 45700 match 16682\n",
      ">>> step 45800\n",
      "step 45700-45799, precision 0.471766, recall 0.788003, f_score 0.590192\n",
      "=== total 45800 match 16735\n",
      ">>> step 45900\n",
      "step 45800-45899, precision 0.470809, recall 0.788003, f_score 0.589443\n",
      "=== total 45900 match 16769\n",
      ">>> step 46000\n",
      "step 45900-45999, precision 0.471494, recall 0.788605, f_score 0.590148\n",
      "=== total 46000 match 16821\n",
      ">>> step 46100\n",
      "step 46000-46099, precision 0.472472, recall 0.789651, f_score 0.591207\n",
      "=== total 46100 match 16892\n",
      ">>> step 46200\n",
      "step 46100-46199, precision 0.472250, recall 0.790190, f_score 0.591184\n",
      "=== total 46200 match 16955\n",
      ">>> step 46300\n",
      "step 46200-46299, precision 0.472682, recall 0.790995, f_score 0.591748\n",
      "=== total 46300 match 17022\n",
      ">>> step 46400\n",
      "step 46300-46399, precision 0.471942, recall 0.791221, f_score 0.591231\n",
      "=== total 46400 match 17072\n",
      ">>> step 46500\n",
      "step 46400-46499, precision 0.472618, recall 0.791997, f_score 0.591978\n",
      "=== total 46500 match 17128\n",
      ">>> step 46600\n",
      "step 46500-46599, precision 0.471764, recall 0.791997, f_score 0.591308\n",
      "=== total 46600 match 17159\n",
      ">>> step 46700\n",
      "step 46600-46699, precision 0.470941, recall 0.791997, f_score 0.590660\n",
      "=== total 46700 match 17189\n",
      ">>> step 46800\n",
      "step 46700-46799, precision 0.471846, recall 0.793009, f_score 0.591654\n",
      "=== total 46800 match 17262\n",
      ">>> step 46900\n",
      "step 46800-46899, precision 0.471163, recall 0.793009, f_score 0.591117\n",
      "=== total 46900 match 17287\n",
      ">>> step 47000\n",
      "step 46900-46999, precision 0.470184, recall 0.793009, f_score 0.590346\n",
      "=== total 47000 match 17323\n",
      ">>> step 47100\n",
      "step 47000-47099, precision 0.470290, recall 0.792233, f_score 0.590214\n",
      "=== total 47100 match 17351\n",
      ">>> step 47200\n",
      "step 47100-47199, precision 0.470751, recall 0.792256, f_score 0.590583\n",
      "=== total 47200 match 17385\n",
      ">>> step 47300\n",
      "step 47200-47299, precision 0.469819, recall 0.792259, f_score 0.589850\n",
      "=== total 47300 match 17428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 47400\n",
      "step 47300-47399, precision 0.469038, recall 0.792259, f_score 0.589234\n",
      "=== total 47400 match 17457\n",
      ">>> step 47500\n",
      "step 47400-47499, precision 0.469937, recall 0.793260, f_score 0.590220\n",
      "=== total 47500 match 17530\n",
      ">>> step 47600\n",
      "step 47500-47599, precision 0.469161, recall 0.793260, f_score 0.589608\n",
      "=== total 47600 match 17559\n",
      ">>> step 47700\n",
      "step 47600-47699, precision 0.469690, recall 0.793917, f_score 0.590207\n",
      "=== total 47700 match 17618\n",
      ">>> step 47800\n",
      "step 47700-47799, precision 0.468865, recall 0.793917, f_score 0.589555\n",
      "=== total 47800 match 17649\n",
      ">>> step 47900\n",
      "step 47800-47899, precision 0.469722, recall 0.794710, f_score 0.590451\n",
      "=== total 47900 match 17719\n",
      ">>> step 48000\n",
      "step 47900-47999, precision 0.469330, recall 0.794361, f_score 0.590046\n",
      "=== total 48000 match 17770\n",
      ">>> step 48100\n",
      "step 48000-48099, precision 0.468460, recall 0.794361, f_score 0.589358\n",
      "=== total 48100 match 17803\n",
      ">>> step 48200\n",
      "step 48100-48199, precision 0.468600, recall 0.794986, f_score 0.589640\n",
      "=== total 48200 match 17866\n",
      ">>> step 48300\n",
      "step 48200-48299, precision 0.467900, recall 0.795025, f_score 0.589096\n",
      "=== total 48300 match 17897\n",
      ">>> step 48400\n",
      "step 48300-48399, precision 0.468827, recall 0.795955, f_score 0.590086\n",
      "=== total 48400 match 17964\n",
      ">>> step 48500\n",
      "step 48400-48499, precision 0.467993, recall 0.795955, f_score 0.589425\n",
      "=== total 48500 match 17996\n",
      ">>> step 48600\n",
      "step 48500-48599, precision 0.467396, recall 0.795955, f_score 0.588951\n",
      "=== total 48600 match 18019\n",
      ">>> step 48700\n",
      "step 48600-48699, precision 0.468315, recall 0.796857, f_score 0.589928\n",
      "=== total 48700 match 18084\n",
      ">>> step 48800\n",
      "step 48700-48799, precision 0.468497, recall 0.797429, f_score 0.590229\n",
      "=== total 48800 match 18141\n",
      ">>> step 48900\n",
      "step 48800-48899, precision 0.468396, recall 0.797865, f_score 0.590268\n",
      "=== total 48900 match 18194\n",
      ">>> step 49000\n",
      "step 48900-48999, precision 0.467574, recall 0.797865, f_score 0.589615\n",
      "=== total 49000 match 18226\n",
      ">>> step 49100\n",
      "step 49000-49099, precision 0.466805, recall 0.797865, f_score 0.589004\n",
      "=== total 49100 match 18256\n",
      ">>> step 49200\n",
      "step 49100-49199, precision 0.466751, recall 0.798111, f_score 0.589027\n",
      "=== total 49200 match 18286\n",
      ">>> step 49300\n",
      "step 49200-49299, precision 0.467295, recall 0.798826, f_score 0.589655\n",
      "=== total 49300 match 18346\n",
      ">>> step 49400\n",
      "step 49300-49399, precision 0.466431, recall 0.798826, f_score 0.588967\n",
      "=== total 49400 match 18380\n",
      ">>> step 49500\n",
      "step 49400-49499, precision 0.465924, recall 0.798826, f_score 0.588562\n",
      "=== total 49500 match 18400\n",
      ">>> step 49600\n",
      "step 49500-49599, precision 0.465266, recall 0.798826, f_score 0.588038\n",
      "=== total 49600 match 18426\n",
      ">>> step 49700\n",
      "step 49600-49699, precision 0.464907, recall 0.795585, f_score 0.586871\n",
      "=== total 49700 match 18451\n",
      ">>> step 49800\n",
      "step 49700-49799, precision 0.465335, recall 0.796267, f_score 0.587397\n",
      "=== total 49800 match 18520\n",
      ">>> step 49900\n",
      "step 49800-49899, precision 0.464849, recall 0.796645, f_score 0.587112\n",
      "=== total 49900 match 18591\n",
      ">>> step 50000\n",
      "step 49900-49999, precision 0.465276, recall 0.797280, f_score 0.587626\n",
      "=== total 50000 match 18647\n",
      ">>> step 50100\n",
      "step 50000-50099, precision 0.464454, recall 0.797280, f_score 0.586970\n",
      "=== total 50100 match 18680\n",
      ">>> step 50200\n",
      "step 50100-50199, precision 0.463660, recall 0.797280, f_score 0.586335\n",
      "=== total 50200 match 18712\n",
      ">>> step 50300\n",
      "step 50200-50299, precision 0.463736, recall 0.797763, f_score 0.586526\n",
      "=== total 50300 match 18765\n",
      ">>> step 50400\n",
      "step 50300-50399, precision 0.462675, recall 0.797763, f_score 0.585678\n",
      "=== total 50400 match 18808\n",
      ">>> step 50500\n",
      "step 50400-50499, precision 0.462991, recall 0.796313, f_score 0.585539\n",
      "=== total 50500 match 18847\n",
      ">>> step 50600\n",
      "step 50500-50599, precision 0.462060, recall 0.796313, f_score 0.584794\n",
      "=== total 50600 match 18885\n",
      ">>> step 50700\n",
      "step 50600-50699, precision 0.461823, recall 0.796684, f_score 0.584704\n",
      "=== total 50700 match 18938\n",
      ">>> step 50800\n",
      "step 50700-50799, precision 0.461810, recall 0.797183, f_score 0.584828\n",
      "=== total 50800 match 18997\n",
      ">>> step 50900\n",
      "step 50800-50899, precision 0.462071, recall 0.797716, f_score 0.585181\n",
      "=== total 50900 match 19049\n",
      ">>> step 51000\n",
      "step 50900-50999, precision 0.461079, recall 0.797716, f_score 0.584385\n",
      "=== total 51000 match 19090\n",
      ">>> step 51100\n",
      "step 51000-51099, precision 0.460380, recall 0.797716, f_score 0.583823\n",
      "=== total 51100 match 19119\n",
      ">>> step 51200\n",
      "step 51100-51199, precision 0.460439, recall 0.798192, f_score 0.583998\n",
      "=== total 51200 match 19173\n",
      ">>> step 51300\n",
      "step 51200-51299, precision 0.459528, recall 0.798192, f_score 0.583265\n",
      "=== total 51300 match 19211\n",
      ">>> step 51400\n",
      "step 51300-51399, precision 0.460429, recall 0.799100, f_score 0.584233\n",
      "=== total 51400 match 19282\n",
      ">>> step 51500\n",
      "step 51400-51499, precision 0.459524, recall 0.799100, f_score 0.583503\n",
      "=== total 51500 match 19320\n",
      ">>> step 51600\n",
      "step 51500-51599, precision 0.460277, recall 0.799892, f_score 0.584321\n",
      "=== total 51600 match 19384\n",
      ">>> step 51700\n",
      "step 51600-51699, precision 0.460977, recall 0.800785, f_score 0.585124\n",
      "=== total 51700 match 19463\n",
      ">>> step 51800\n",
      "step 51700-51799, precision 0.460126, recall 0.800785, f_score 0.584438\n",
      "=== total 51800 match 19499\n",
      ">>> step 51900\n",
      "step 51800-51899, precision 0.459396, recall 0.800785, f_score 0.583849\n",
      "=== total 51900 match 19530\n",
      ">>> step 52000\n",
      "step 51900-51999, precision 0.460118, recall 0.801671, f_score 0.584667\n",
      "=== total 52000 match 19608\n",
      ">>> step 52100\n",
      "step 52000-52099, precision 0.460976, recall 0.802548, f_score 0.585593\n",
      "=== total 52100 match 19680\n",
      ">>> step 52200\n",
      "step 52100-52199, precision 0.460892, recall 0.803001, f_score 0.585645\n",
      "=== total 52200 match 19740\n",
      ">>> step 52300\n",
      "step 52200-52299, precision 0.461484, recall 0.803692, f_score 0.586308\n",
      "=== total 52300 match 19810\n",
      ">>> step 52400\n",
      "step 52300-52399, precision 0.460802, recall 0.803691, f_score 0.585756\n",
      "=== total 52400 match 19848\n",
      ">>> step 52500\n",
      "step 52400-52499, precision 0.461376, recall 0.804097, f_score 0.586328\n",
      "=== total 52500 match 19910\n",
      ">>> step 52600\n",
      "step 52500-52599, precision 0.460929, recall 0.804024, f_score 0.585947\n",
      "=== total 52600 match 19938\n",
      ">>> step 52700\n",
      "step 52600-52699, precision 0.460167, recall 0.804024, f_score 0.585332\n",
      "=== total 52700 match 19971\n",
      ">>> step 52800\n",
      "step 52700-52799, precision 0.461009, recall 0.804878, f_score 0.586239\n",
      "=== total 52800 match 20043\n",
      ">>> step 52900\n",
      "step 52800-52899, precision 0.460182, recall 0.804878, f_score 0.585570\n",
      "=== total 52900 match 20079\n",
      ">>> step 53000\n",
      "step 52900-52999, precision 0.459313, recall 0.804878, f_score 0.584866\n",
      "=== total 53000 match 20117\n",
      ">>> step 53100\n",
      "step 53000-53099, precision 0.459124, recall 0.805024, f_score 0.584751\n",
      "=== total 53100 match 20171\n",
      ">>> step 53200\n",
      "step 53100-53199, precision 0.458801, recall 0.805030, f_score 0.584490\n",
      "=== total 53200 match 20231\n",
      ">>> step 53300\n",
      "step 53200-53299, precision 0.458952, recall 0.805469, f_score 0.584728\n",
      "=== total 53300 match 20281\n",
      ">>> step 53400\n",
      "step 53300-53399, precision 0.458139, recall 0.805469, f_score 0.584068\n",
      "=== total 53400 match 20317\n",
      ">>> step 53500\n",
      "step 53400-53499, precision 0.458380, recall 0.806020, f_score 0.584409\n",
      "=== total 53500 match 20387\n",
      ">>> step 53600\n",
      "step 53500-53599, precision 0.457893, recall 0.803989, f_score 0.583479\n",
      "=== total 53600 match 20424\n",
      ">>> step 53700\n",
      "step 53600-53699, precision 0.457621, recall 0.804135, f_score 0.583297\n",
      "=== total 53700 match 20482\n",
      ">>> step 53800\n",
      "step 53700-53799, precision 0.457889, recall 0.804772, f_score 0.583682\n",
      "=== total 53800 match 20553\n",
      ">>> step 53900\n",
      "step 53800-53899, precision 0.457387, recall 0.804988, f_score 0.583331\n",
      "=== total 53900 match 20604\n",
      ">>> step 54000\n",
      "step 53900-53999, precision 0.456909, recall 0.805205, f_score 0.582999\n",
      "=== total 54000 match 20654\n",
      ">>> step 54100\n",
      "step 54000-54099, precision 0.457179, recall 0.805752, f_score 0.583362\n",
      "=== total 54100 match 20714\n",
      ">>> step 54200\n",
      "step 54100-54199, precision 0.456985, recall 0.806032, f_score 0.583277\n",
      "=== total 54200 match 20760\n",
      ">>> step 54300\n",
      "step 54200-54299, precision 0.457139, recall 0.806558, f_score 0.583540\n",
      "=== total 54300 match 20823\n",
      ">>> step 54400\n",
      "step 54300-54399, precision 0.456437, recall 0.806558, f_score 0.582968\n",
      "=== total 54400 match 20855\n",
      ">>> step 54500\n",
      "step 54400-54499, precision 0.457191, recall 0.807374, f_score 0.583796\n",
      "=== total 54500 match 20930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 54600\n",
      "step 54500-54599, precision 0.456558, recall 0.807374, f_score 0.583280\n",
      "=== total 54600 match 20959\n",
      ">>> step 54700\n",
      "step 54600-54699, precision 0.455884, recall 0.807374, f_score 0.582729\n",
      "=== total 54700 match 20990\n",
      ">>> step 54800\n",
      "step 54700-54799, precision 0.456743, recall 0.808183, f_score 0.583642\n",
      "=== total 54800 match 21060\n",
      ">>> step 54900\n",
      "step 54800-54899, precision 0.457553, recall 0.808986, f_score 0.584512\n",
      "=== total 54900 match 21132\n",
      ">>> step 55000\n",
      "step 54900-54999, precision 0.456861, recall 0.808986, f_score 0.583947\n",
      "=== total 55000 match 21164\n",
      ">>> step 55100\n",
      "step 55000-55099, precision 0.456149, recall 0.808986, f_score 0.583366\n",
      "=== total 55100 match 21197\n",
      ">>> step 55200\n",
      "step 55100-55199, precision 0.455619, recall 0.809178, f_score 0.582982\n",
      "=== total 55200 match 21248\n",
      ">>> step 55300\n",
      "step 55200-55299, precision 0.455041, recall 0.807907, f_score 0.582179\n",
      "=== total 55300 match 21286\n",
      ">>> step 55400\n",
      "step 55300-55399, precision 0.454366, recall 0.806325, f_score 0.581216\n",
      "=== total 55400 match 21322\n",
      ">>> step 55500\n",
      "step 55400-55499, precision 0.454303, recall 0.806661, f_score 0.581251\n",
      "=== total 55500 match 21380\n",
      ">>> step 55600\n",
      "step 55500-55599, precision 0.455046, recall 0.806633, f_score 0.581852\n",
      "=== total 55600 match 21433\n",
      ">>> step 55700\n",
      "step 55600-55699, precision 0.454219, recall 0.806633, f_score 0.581176\n",
      "=== total 55700 match 21472\n",
      ">>> step 55800\n",
      "step 55700-55799, precision 0.453890, recall 0.806665, f_score 0.580914\n",
      "=== total 55800 match 21492\n",
      ">>> step 55900\n",
      "step 55800-55899, precision 0.454385, recall 0.806112, f_score 0.581176\n",
      "=== total 55900 match 21539\n",
      ">>> step 56000\n",
      "step 55900-55999, precision 0.454137, recall 0.804948, f_score 0.580670\n",
      "=== total 56000 match 21564\n",
      ">>> step 56100\n",
      "step 56000-56099, precision 0.454166, recall 0.805060, f_score 0.580724\n",
      "=== total 56100 match 21578\n",
      ">>> step 56200\n",
      "step 56100-56199, precision 0.454461, recall 0.805474, f_score 0.581072\n",
      "=== total 56200 match 21630\n",
      ">>> step 56300\n",
      "step 56200-56299, precision 0.454440, recall 0.805474, f_score 0.581055\n",
      "=== total 56300 match 21631\n",
      ">>> step 56400\n",
      "step 56300-56399, precision 0.454558, recall 0.805681, f_score 0.581205\n",
      "=== total 56400 match 21654\n",
      ">>> step 56500\n",
      "step 56400-56499, precision 0.454675, recall 0.805914, f_score 0.581362\n",
      "=== total 56500 match 21699\n",
      ">>> step 56600\n",
      "step 56500-56599, precision 0.454612, recall 0.805914, f_score 0.581310\n",
      "=== total 56600 match 21702\n",
      ">>> step 56700\n",
      "step 56600-56699, precision 0.454688, recall 0.806120, f_score 0.581425\n",
      "=== total 56700 match 21727\n",
      ">>> step 56800\n",
      "step 56700-56799, precision 0.454917, recall 0.806515, f_score 0.581716\n",
      "=== total 56800 match 21771\n",
      ">>> step 56900\n",
      "step 56800-56899, precision 0.455271, recall 0.806711, f_score 0.582056\n",
      "=== total 56900 match 21809\n",
      ">>> step 57000\n",
      "step 56900-56999, precision 0.455278, recall 0.804614, f_score 0.581515\n",
      "=== total 57000 match 21835\n",
      ">>> step 57100\n",
      "step 57000-57099, precision 0.454928, recall 0.804657, f_score 0.581241\n",
      "=== total 57100 match 21876\n",
      ">>> step 57200\n",
      "step 57100-57199, precision 0.455118, recall 0.804742, f_score 0.581418\n",
      "=== total 57200 match 21924\n",
      ">>> step 57300\n",
      "step 57200-57299, precision 0.455195, recall 0.804199, f_score 0.581339\n",
      "=== total 57300 match 21962\n",
      ">>> step 57400\n",
      "step 57300-57399, precision 0.454794, recall 0.802826, f_score 0.580653\n",
      "=== total 57400 match 21988\n",
      ">>> step 57500\n",
      "step 57400-57499, precision 0.455185, recall 0.803221, f_score 0.581075\n",
      "=== total 57500 match 22024\n",
      ">>> step 57600\n",
      "step 57500-57599, precision 0.455288, recall 0.802015, f_score 0.580843\n",
      "=== total 57600 match 22030\n",
      ">>> step 57700\n",
      "step 57600-57699, precision 0.455061, recall 0.802015, f_score 0.580658\n",
      "=== total 57700 match 22041\n",
      ">>> step 57800\n",
      "step 57700-57799, precision 0.455197, recall 0.801053, f_score 0.580516\n",
      "=== total 57800 match 22052\n",
      ">>> step 57900\n",
      "step 57800-57899, precision 0.455023, recall 0.801260, f_score 0.580429\n",
      "=== total 57900 match 22089\n",
      ">>> step 58000\n",
      "step 57900-57999, precision 0.455471, recall 0.801178, f_score 0.580772\n",
      "=== total 58000 match 22109\n",
      ">>> step 58100\n",
      "step 58000-58099, precision 0.455399, recall 0.801016, f_score 0.580671\n",
      "=== total 58100 match 22152\n",
      ">>> step 58200\n",
      "step 58100-58199, precision 0.455369, recall 0.801222, f_score 0.580701\n",
      "=== total 58200 match 22182\n",
      ">>> step 58300\n",
      "step 58200-58299, precision 0.455250, recall 0.800111, f_score 0.580312\n",
      "=== total 58300 match 22201\n",
      ">>> step 58400\n",
      "step 58300-58399, precision 0.454995, recall 0.799921, f_score 0.580055\n",
      "=== total 58400 match 22231\n",
      ">>> step 58500\n",
      "step 58400-58499, precision 0.454864, recall 0.799605, f_score 0.579865\n",
      "=== total 58500 match 22255\n",
      ">>> step 58600\n",
      "step 58500-58599, precision 0.454880, recall 0.799259, f_score 0.579787\n",
      "=== total 58600 match 22285\n",
      ">>> step 58700\n",
      "step 58600-58699, precision 0.454860, recall 0.799259, f_score 0.579771\n",
      "=== total 58700 match 22286\n",
      ">>> step 58800\n",
      "step 58700-58799, precision 0.454664, recall 0.796277, f_score 0.578826\n",
      "=== total 58800 match 22300\n",
      ">>> step 58900\n",
      "step 58800-58899, precision 0.454480, recall 0.796277, f_score 0.578677\n",
      "=== total 58900 match 22309\n",
      ">>> step 59000\n",
      "step 58900-58999, precision 0.454472, recall 0.796485, f_score 0.578725\n",
      "=== total 59000 match 22338\n",
      ">>> step 59100\n",
      "step 59000-59099, precision 0.455004, recall 0.797138, f_score 0.579329\n",
      "=== total 59100 match 22402\n",
      ">>> step 59200\n",
      "step 59100-59199, precision 0.454939, recall 0.797280, f_score 0.579313\n",
      "=== total 59200 match 22425\n",
      ">>> step 59300\n",
      "step 59200-59299, precision 0.454878, recall 0.797280, f_score 0.579264\n",
      "=== total 59300 match 22428\n",
      ">>> step 59400\n",
      "step 59300-59399, precision 0.455137, recall 0.797502, f_score 0.579533\n",
      "=== total 59400 match 22446\n",
      ">>> step 59500\n",
      "step 59400-59499, precision 0.455096, recall 0.797502, f_score 0.579500\n",
      "=== total 59500 match 22448\n",
      ">>> step 59600\n",
      "step 59500-59599, precision 0.455096, recall 0.796631, f_score 0.579270\n",
      "=== total 59600 match 22448\n",
      ">>> step 59700\n",
      "step 59600-59699, precision 0.454995, recall 0.796631, f_score 0.579188\n",
      "=== total 59700 match 22453\n",
      ">>> step 59800\n",
      "step 59700-59799, precision 0.454954, recall 0.796631, f_score 0.579155\n",
      "=== total 59800 match 22455\n",
      ">>> step 59900\n",
      "step 59800-59899, precision 0.454894, recall 0.796197, f_score 0.578991\n",
      "=== total 59900 match 22458\n",
      ">>> step 60000\n",
      "step 59900-59999, precision 0.454894, recall 0.795329, f_score 0.578761\n",
      "=== total 60000 match 22458\n",
      ">>> step 60100\n",
      "step 60000-60099, precision 0.454853, recall 0.794463, f_score 0.578499\n",
      "=== total 60100 match 22460\n",
      ">>> step 60200\n",
      "step 60100-60199, precision 0.454732, recall 0.794463, f_score 0.578401\n",
      "=== total 60200 match 22466\n",
      ">>> step 60300\n",
      "step 60200-60299, precision 0.454732, recall 0.794463, f_score 0.578401\n",
      "=== total 60300 match 22466\n",
      ">>> step 60400\n",
      "step 60300-60399, precision 0.454930, recall 0.793928, f_score 0.578419\n",
      "=== total 60400 match 22476\n",
      ">>> step 60500\n",
      "step 60400-60499, precision 0.454647, recall 0.793928, f_score 0.578190\n",
      "=== total 60500 match 22490\n",
      ">>> step 60600\n",
      "step 60500-60599, precision 0.454606, recall 0.793928, f_score 0.578157\n",
      "=== total 60600 match 22492\n",
      ">>> step 60700\n",
      "step 60600-60699, precision 0.454852, recall 0.793181, f_score 0.578158\n",
      "=== total 60700 match 22504\n",
      ">>> step 60800\n",
      "step 60700-60799, precision 0.454772, recall 0.793181, f_score 0.578093\n",
      "=== total 60800 match 22508\n",
      ">>> step 60900\n",
      "step 60800-60899, precision 0.454840, recall 0.793250, f_score 0.578166\n",
      "=== total 60900 match 22531\n",
      ">>> step 61000\n",
      "step 60900-60999, precision 0.454840, recall 0.793250, f_score 0.578166\n",
      "=== total 61000 match 22531\n",
      ">>> step 61100\n",
      "step 61000-61099, precision 0.454638, recall 0.793250, f_score 0.578003\n",
      "=== total 61100 match 22541\n",
      ">>> step 61200\n",
      "step 61100-61199, precision 0.454654, recall 0.793554, f_score 0.578097\n",
      "=== total 61200 match 22582\n",
      ">>> step 61300\n",
      "step 61200-61299, precision 0.454871, recall 0.793857, f_score 0.578353\n",
      "=== total 61300 match 22613\n",
      ">>> step 61400\n",
      "step 61300-61399, precision 0.454606, recall 0.793369, f_score 0.578009\n",
      "=== total 61400 match 22635\n",
      ">>> step 61500\n",
      "step 61400-61499, precision 0.454586, recall 0.793345, f_score 0.577986\n",
      "=== total 61500 match 22658\n",
      ">>> step 61600\n",
      "step 61500-61599, precision 0.454405, recall 0.793345, f_score 0.577840\n",
      "=== total 61600 match 22667\n",
      ">>> step 61700\n",
      "step 61600-61699, precision 0.454669, recall 0.793715, f_score 0.578152\n",
      "=== total 61700 match 22722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 61800\n",
      "step 61700-61799, precision 0.454933, recall 0.793701, f_score 0.578361\n",
      "=== total 61800 match 22766\n",
      ">>> step 61900\n",
      "step 61800-61899, precision 0.454829, recall 0.793703, f_score 0.578278\n",
      "=== total 61900 match 22780\n",
      ">>> step 62000\n",
      "step 61900-61999, precision 0.455509, recall 0.793543, f_score 0.578785\n",
      "=== total 62000 match 22825\n",
      ">>> step 62100\n",
      "step 62000-62099, precision 0.455668, recall 0.792914, f_score 0.578745\n",
      "=== total 62100 match 22839\n",
      ">>> step 62200\n",
      "step 62100-62199, precision 0.455449, recall 0.792914, f_score 0.578568\n",
      "=== total 62200 match 22850\n",
      ">>> step 62300\n",
      "step 62200-62299, precision 0.455547, recall 0.792126, f_score 0.578438\n",
      "=== total 62300 match 22878\n",
      ">>> step 62400\n",
      "step 62300-62399, precision 0.455653, recall 0.792195, f_score 0.578542\n",
      "=== total 62400 match 22899\n",
      ">>> step 62500\n",
      "step 62400-62499, precision 0.455375, recall 0.792195, f_score 0.578317\n",
      "=== total 62500 match 22913\n",
      ">>> step 62600\n",
      "step 62500-62599, precision 0.454700, recall 0.792195, f_score 0.577773\n",
      "=== total 62600 match 22947\n",
      ">>> step 62700\n",
      "step 62600-62699, precision 0.455415, recall 0.792001, f_score 0.578298\n",
      "=== total 62700 match 23001\n",
      ">>> step 62800\n",
      "step 62700-62799, precision 0.455383, recall 0.791613, f_score 0.578169\n",
      "=== total 62800 match 23007\n",
      ">>> step 62900\n",
      "step 62800-62899, precision 0.455846, recall 0.792210, f_score 0.578701\n",
      "=== total 62900 match 23067\n",
      ">>> step 63000\n",
      "step 62900-62999, precision 0.456524, recall 0.792803, f_score 0.579405\n",
      "=== total 63000 match 23116\n",
      ">>> step 63100\n",
      "step 63000-63099, precision 0.456298, recall 0.792612, f_score 0.579173\n",
      "=== total 63100 match 23134\n",
      ">>> step 63200\n",
      "step 63100-63199, precision 0.456064, recall 0.792604, f_score 0.578982\n",
      "=== total 63200 match 23170\n",
      ">>> step 63300\n",
      "step 63200-63299, precision 0.456477, recall 0.793008, f_score 0.579422\n",
      "=== total 63300 match 23206\n",
      ">>> step 63400\n",
      "step 63300-63399, precision 0.456359, recall 0.793008, f_score 0.579327\n",
      "=== total 63400 match 23212\n",
      ">>> step 63500\n",
      "step 63400-63499, precision 0.456162, recall 0.793008, f_score 0.579169\n",
      "=== total 63500 match 23222\n",
      ">>> step 63600\n",
      "step 63500-63599, precision 0.456063, recall 0.792848, f_score 0.579047\n",
      "=== total 63600 match 23238\n",
      ">>> step 63700\n",
      "step 63600-63699, precision 0.456243, recall 0.791688, f_score 0.578881\n",
      "=== total 63700 match 23299\n",
      ">>> step 63800\n",
      "step 63700-63799, precision 0.455569, recall 0.791890, f_score 0.578392\n",
      "=== total 63800 match 23362\n",
      ">>> step 63900\n",
      "step 63800-63899, precision 0.455485, recall 0.792202, f_score 0.578408\n",
      "=== total 63900 match 23419\n",
      ">>> step 64000\n",
      "step 63900-63999, precision 0.454844, recall 0.792202, f_score 0.577891\n",
      "=== total 64000 match 23452\n",
      ">>> step 64100\n",
      "step 64000-64099, precision 0.454263, recall 0.792202, f_score 0.577422\n",
      "=== total 64100 match 23482\n",
      ">>> step 64200\n",
      "step 64100-64199, precision 0.453934, recall 0.792202, f_score 0.577156\n",
      "=== total 64200 match 23499\n",
      ">>> step 64300\n",
      "step 64200-64299, precision 0.453741, recall 0.792202, f_score 0.577000\n",
      "=== total 64300 match 23509\n",
      ">>> step 64400\n",
      "step 64300-64399, precision 0.453592, recall 0.791920, f_score 0.576805\n",
      "=== total 64400 match 23552\n",
      ">>> step 64500\n",
      "step 64400-64499, precision 0.453226, recall 0.791920, f_score 0.576509\n",
      "=== total 64500 match 23571\n",
      ">>> step 64600\n",
      "step 64500-64599, precision 0.452746, recall 0.791920, f_score 0.576120\n",
      "=== total 64600 match 23596\n",
      ">>> step 64700\n",
      "step 64600-64699, precision 0.452339, recall 0.792120, f_score 0.575843\n",
      "=== total 64700 match 23646\n",
      ">>> step 64800\n",
      "step 64700-64799, precision 0.452278, recall 0.792357, f_score 0.575857\n",
      "=== total 64800 match 23700\n",
      ">>> step 64900\n",
      "step 64800-64899, precision 0.451764, recall 0.792357, f_score 0.575440\n",
      "=== total 64900 match 23727\n",
      ">>> step 65000\n",
      "step 64900-64999, precision 0.451846, recall 0.792740, f_score 0.575607\n",
      "=== total 65000 match 23778\n",
      ">>> step 65100\n",
      "step 65000-65099, precision 0.451501, recall 0.792938, f_score 0.575379\n",
      "=== total 65100 match 23825\n",
      ">>> step 65200\n",
      "step 65100-65199, precision 0.451708, recall 0.793172, f_score 0.575609\n",
      "=== total 65200 match 23865\n",
      ">>> step 65300\n",
      "step 65200-65299, precision 0.451520, recall 0.793461, f_score 0.575532\n",
      "=== total 65300 match 23917\n",
      ">>> step 65400\n",
      "step 65300-65399, precision 0.450521, recall 0.793461, f_score 0.574721\n",
      "=== total 65400 match 23970\n",
      ">>> step 65500\n",
      "step 65400-65499, precision 0.450650, recall 0.793619, f_score 0.574867\n",
      "=== total 65500 match 24012\n",
      ">>> step 65600\n",
      "step 65500-65599, precision 0.450275, recall 0.793619, f_score 0.574561\n",
      "=== total 65600 match 24032\n",
      ">>> step 65700\n",
      "step 65600-65699, precision 0.450353, recall 0.793907, f_score 0.574700\n",
      "=== total 65700 match 24070\n",
      ">>> step 65800\n",
      "step 65700-65799, precision 0.450147, recall 0.793907, f_score 0.574533\n",
      "=== total 65800 match 24081\n",
      ">>> step 65900\n",
      "step 65800-65899, precision 0.449699, recall 0.793907, f_score 0.574168\n",
      "=== total 65900 match 24105\n",
      ">>> step 66000\n",
      "step 65900-65999, precision 0.449623, recall 0.794103, f_score 0.574157\n",
      "=== total 66000 match 24138\n",
      ">>> step 66100\n",
      "step 66000-66099, precision 0.448823, recall 0.794103, f_score 0.573505\n",
      "=== total 66100 match 24181\n",
      ">>> step 66200\n",
      "step 66100-66199, precision 0.448490, recall 0.794103, f_score 0.573232\n",
      "=== total 66200 match 24199\n",
      ">>> step 66300\n",
      "step 66200-66299, precision 0.447982, recall 0.794152, f_score 0.572830\n",
      "=== total 66300 match 24251\n",
      ">>> step 66400\n",
      "step 66300-66399, precision 0.447575, recall 0.794152, f_score 0.572498\n",
      "=== total 66400 match 24273\n",
      ">>> step 66500\n",
      "step 66400-66499, precision 0.447624, recall 0.794455, f_score 0.572616\n",
      "=== total 66500 match 24324\n",
      ">>> step 66600\n",
      "step 66500-66599, precision 0.447790, recall 0.794649, f_score 0.572802\n",
      "=== total 66600 match 24344\n",
      ">>> step 66700\n",
      "step 66600-66699, precision 0.447422, recall 0.794649, f_score 0.572501\n",
      "=== total 66700 match 24364\n",
      ">>> step 66800\n",
      "step 66700-66799, precision 0.447482, recall 0.794514, f_score 0.572515\n",
      "=== total 66800 match 24401\n",
      ">>> step 66900\n",
      "step 66800-66899, precision 0.446877, recall 0.794514, f_score 0.572020\n",
      "=== total 66900 match 24434\n",
      ">>> step 67000\n",
      "step 66900-66999, precision 0.446567, recall 0.794514, f_score 0.571765\n",
      "=== total 67000 match 24451\n",
      ">>> step 67100\n",
      "step 67000-67099, precision 0.446129, recall 0.794514, f_score 0.571406\n",
      "=== total 67100 match 24475\n",
      ">>> step 67200\n",
      "step 67100-67199, precision 0.446289, recall 0.794814, f_score 0.571615\n",
      "=== total 67200 match 24520\n",
      ">>> step 67300\n",
      "step 67200-67299, precision 0.446126, recall 0.795186, f_score 0.571577\n",
      "=== total 67300 match 24585\n",
      ">>> step 67400\n",
      "step 67300-67399, precision 0.445745, recall 0.795186, f_score 0.571265\n",
      "=== total 67400 match 24606\n",
      ">>> step 67500\n",
      "step 67400-67499, precision 0.445432, recall 0.794944, f_score 0.570946\n",
      "=== total 67500 match 24639\n",
      ">>> step 67600\n",
      "step 67500-67599, precision 0.445115, recall 0.794520, f_score 0.570576\n",
      "=== total 67600 match 24688\n",
      ">>> step 67700\n",
      "step 67600-67699, precision 0.444467, recall 0.794520, f_score 0.570043\n",
      "=== total 67700 match 24724\n",
      ">>> step 67800\n",
      "step 67700-67799, precision 0.444790, recall 0.794533, f_score 0.570312\n",
      "=== total 67800 match 24769\n",
      ">>> step 67900\n",
      "step 67800-67899, precision 0.445004, recall 0.794774, f_score 0.570550\n",
      "=== total 67900 match 24811\n",
      ">>> step 68000\n",
      "step 67900-67999, precision 0.444963, recall 0.794881, f_score 0.570544\n",
      "=== total 68000 match 24847\n",
      ">>> step 68100\n",
      "step 68000-68099, precision 0.444927, recall 0.792147, f_score 0.569809\n",
      "=== total 68100 match 24849\n",
      ">>> step 68200\n",
      "step 68100-68199, precision 0.444498, recall 0.792147, f_score 0.569457\n",
      "=== total 68200 match 24873\n",
      ">>> step 68300\n",
      "step 68200-68299, precision 0.444552, recall 0.792159, f_score 0.569503\n",
      "=== total 68300 match 24906\n",
      ">>> step 68400\n",
      "step 68300-68399, precision 0.444694, recall 0.792515, f_score 0.569712\n",
      "=== total 68400 match 24952\n",
      ">>> step 68500\n",
      "step 68400-68499, precision 0.444355, recall 0.792515, f_score 0.569434\n",
      "=== total 68500 match 24971\n",
      ">>> step 68600\n",
      "step 68500-68599, precision 0.444249, recall 0.792515, f_score 0.569347\n",
      "=== total 68600 match 24977\n",
      ">>> step 68700\n",
      "step 68600-68699, precision 0.443929, recall 0.792515, f_score 0.569084\n",
      "=== total 68700 match 24995\n",
      ">>> step 68800\n",
      "step 68700-68799, precision 0.444294, recall 0.792899, f_score 0.569483\n",
      "=== total 68800 match 25033\n",
      ">>> step 68900\n",
      "step 68800-68899, precision 0.444223, recall 0.790083, f_score 0.568697\n",
      "=== total 68900 match 25037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 69000\n",
      "step 68900-68999, precision 0.443868, recall 0.790083, f_score 0.568406\n",
      "=== total 69000 match 25057\n",
      ">>> step 69100\n",
      "step 69000-69099, precision 0.443567, recall 0.790083, f_score 0.568159\n",
      "=== total 69100 match 25074\n",
      ">>> step 69200\n",
      "step 69100-69199, precision 0.443431, recall 0.790277, f_score 0.568098\n",
      "=== total 69200 match 25111\n",
      ">>> step 69300\n",
      "step 69200-69299, precision 0.443378, recall 0.787482, f_score 0.567331\n",
      "=== total 69300 match 25114\n",
      ">>> step 69400\n",
      "step 69300-69399, precision 0.443804, recall 0.787962, f_score 0.567804\n",
      "=== total 69400 match 25162\n",
      ">>> step 69500\n",
      "step 69400-69499, precision 0.443698, recall 0.785855, f_score 0.567169\n",
      "=== total 69500 match 25168\n",
      ">>> step 69600\n",
      "step 69500-69599, precision 0.443823, recall 0.785433, f_score 0.567161\n",
      "=== total 69600 match 25197\n",
      ">>> step 69700\n",
      "step 69600-69699, precision 0.443814, recall 0.784978, f_score 0.567036\n",
      "=== total 69700 match 25220\n",
      ">>> step 69800\n",
      "step 69700-69799, precision 0.443779, recall 0.782837, f_score 0.566447\n",
      "=== total 69800 match 25222\n",
      ">>> step 69900\n",
      "step 69800-69899, precision 0.443392, recall 0.782837, f_score 0.566132\n",
      "=== total 69900 match 25244\n",
      ">>> step 70000\n",
      "step 69900-69999, precision 0.443600, recall 0.781154, f_score 0.565861\n",
      "=== total 70000 match 25266\n",
      ">>> step 70100\n",
      "step 70000-70099, precision 0.443530, recall 0.781154, f_score 0.565803\n",
      "=== total 70100 match 25270\n",
      ">>> step 70200\n",
      "step 70100-70199, precision 0.443526, recall 0.778650, f_score 0.565142\n",
      "=== total 70200 match 25277\n",
      ">>> step 70300\n",
      "step 70200-70299, precision 0.443438, recall 0.778650, f_score 0.565071\n",
      "=== total 70300 match 25282\n",
      ">>> step 70400\n",
      "step 70300-70399, precision 0.443245, recall 0.778650, f_score 0.564914\n",
      "=== total 70400 match 25293\n",
      ">>> step 70500\n",
      "step 70400-70499, precision 0.443212, recall 0.778896, f_score 0.564952\n",
      "=== total 70500 match 25331\n",
      ">>> step 70600\n",
      "step 70500-70599, precision 0.442890, recall 0.779049, f_score 0.564730\n",
      "=== total 70600 match 25372\n",
      ">>> step 70700\n",
      "step 70600-70699, precision 0.442541, recall 0.779049, f_score 0.564446\n",
      "=== total 70700 match 25392\n",
      ">>> step 70800\n",
      "step 70700-70799, precision 0.442645, recall 0.779239, f_score 0.564581\n",
      "=== total 70800 match 25438\n",
      ">>> step 70900\n",
      "step 70800-70899, precision 0.442245, recall 0.779239, f_score 0.564255\n",
      "=== total 70900 match 25461\n",
      ">>> step 71000\n",
      "step 70900-70999, precision 0.442106, recall 0.776552, f_score 0.563437\n",
      "=== total 71000 match 25469\n",
      ">>> step 71100\n",
      "step 71000-71099, precision 0.442261, recall 0.776194, f_score 0.563468\n",
      "=== total 71100 match 25494\n",
      ">>> step 71200\n",
      "step 71100-71199, precision 0.442018, recall 0.776194, f_score 0.563271\n",
      "=== total 71200 match 25508\n",
      ">>> step 71300\n",
      "step 71200-71299, precision 0.442408, recall 0.775611, f_score 0.563434\n",
      "=== total 71300 match 25533\n",
      ">>> step 71400\n",
      "step 71300-71399, precision 0.442304, recall 0.775611, f_score 0.563349\n",
      "=== total 71400 match 25539\n",
      ">>> step 71500\n",
      "step 71400-71499, precision 0.442235, recall 0.773063, f_score 0.562620\n",
      "=== total 71500 match 25543\n",
      ">>> step 71600\n",
      "step 71500-71599, precision 0.441975, recall 0.772957, f_score 0.562382\n",
      "=== total 71600 match 25558\n",
      ">>> step 71700\n",
      "step 71600-71699, precision 0.441664, recall 0.772957, f_score 0.562130\n",
      "=== total 71700 match 25576\n",
      ">>> step 71800\n",
      "step 71700-71799, precision 0.441336, recall 0.772957, f_score 0.561864\n",
      "=== total 71800 match 25595\n",
      ">>> step 71900\n",
      "step 71800-71899, precision 0.441216, recall 0.770322, f_score 0.561069\n",
      "=== total 71900 match 25602\n",
      ">>> step 72000\n",
      "step 71900-71999, precision 0.441181, recall 0.767704, f_score 0.560345\n",
      "=== total 72000 match 25604\n",
      ">>> step 72100\n",
      "step 72000-72099, precision 0.440837, recall 0.767704, f_score 0.560067\n",
      "=== total 72100 match 25624\n",
      ">>> step 72200\n",
      "step 72100-72199, precision 0.440510, recall 0.767704, f_score 0.559804\n",
      "=== total 72200 match 25643\n",
      ">>> step 72300\n",
      "step 72200-72299, precision 0.440848, recall 0.767775, f_score 0.560095\n",
      "=== total 72300 match 25671\n",
      ">>> step 72400\n",
      "step 72300-72399, precision 0.441416, recall 0.767478, f_score 0.560474\n",
      "=== total 72400 match 25715\n",
      ">>> step 72500\n",
      "step 72400-72499, precision 0.441175, recall 0.767478, f_score 0.560280\n",
      "=== total 72500 match 25729\n",
      ">>> step 72600\n",
      "step 72500-72599, precision 0.441245, recall 0.765988, f_score 0.559939\n",
      "=== total 72600 match 25734\n",
      ">>> step 72700\n",
      "step 72600-72699, precision 0.441171, recall 0.765423, f_score 0.559728\n",
      "=== total 72700 match 25761\n",
      ">>> step 72800\n",
      "step 72700-72799, precision 0.440971, recall 0.764678, f_score 0.559368\n",
      "=== total 72800 match 25784\n",
      ">>> step 72900\n",
      "step 72800-72899, precision 0.440851, recall 0.762882, f_score 0.558791\n",
      "=== total 72900 match 25791\n",
      ">>> step 73000\n",
      "step 72900-72999, precision 0.440442, recall 0.762882, f_score 0.558462\n",
      "=== total 73000 match 25815\n",
      ">>> step 73100\n",
      "step 73000-73099, precision 0.440101, recall 0.762882, f_score 0.558187\n",
      "=== total 73100 match 25835\n",
      ">>> step 73200\n",
      "step 73100-73199, precision 0.439811, recall 0.762882, f_score 0.557955\n",
      "=== total 73200 match 25852\n",
      ">>> step 73300\n",
      "step 73200-73299, precision 0.439420, recall 0.762882, f_score 0.557640\n",
      "=== total 73300 match 25875\n",
      ">>> step 73400\n",
      "step 73300-73399, precision 0.439030, recall 0.762882, f_score 0.557326\n",
      "=== total 73400 match 25898\n",
      ">>> step 73500\n",
      "step 73400-73499, precision 0.439063, recall 0.763280, f_score 0.557458\n",
      "=== total 73500 match 25953\n",
      ">>> step 73600\n",
      "step 73500-73599, precision 0.438842, recall 0.762472, f_score 0.557065\n",
      "=== total 73600 match 25982\n",
      ">>> step 73700\n",
      "step 73600-73699, precision 0.438851, recall 0.761749, f_score 0.556879\n",
      "=== total 73700 match 26002\n",
      ">>> step 73800\n",
      "step 73700-73799, precision 0.438485, recall 0.761358, f_score 0.556479\n",
      "=== total 73800 match 26026\n",
      ">>> step 73900\n",
      "step 73800-73899, precision 0.438371, recall 0.760674, f_score 0.556205\n",
      "=== total 73900 match 26051\n",
      ">>> step 74000\n",
      "step 73900-73999, precision 0.438219, recall 0.758502, f_score 0.555502\n",
      "=== total 74000 match 26060\n",
      ">>> step 74100\n",
      "step 74000-74099, precision 0.438374, recall 0.758653, f_score 0.555666\n",
      "=== total 74100 match 26101\n",
      ">>> step 74200\n",
      "step 74100-74199, precision 0.438222, recall 0.757209, f_score 0.555157\n",
      "=== total 74200 match 26126\n",
      ">>> step 74300\n",
      "step 74200-74299, precision 0.438264, recall 0.757428, f_score 0.555249\n",
      "=== total 74300 match 26176\n",
      ">>> step 74400\n",
      "step 74300-74399, precision 0.438144, recall 0.755942, f_score 0.554753\n",
      "=== total 74400 match 26206\n",
      ">>> step 74500\n",
      "step 74400-74499, precision 0.437848, recall 0.755659, f_score 0.554440\n",
      "=== total 74500 match 26226\n",
      ">>> step 74600\n",
      "step 74500-74599, precision 0.438126, recall 0.755894, f_score 0.554726\n",
      "=== total 74600 match 26271\n",
      ">>> step 74700\n",
      "step 74600-74699, precision 0.438244, recall 0.756133, f_score 0.554884\n",
      "=== total 74700 match 26305\n",
      ">>> step 74800\n",
      "step 74700-74799, precision 0.437994, recall 0.756133, f_score 0.554684\n",
      "=== total 74800 match 26320\n",
      ">>> step 74900\n",
      "step 74800-74899, precision 0.437981, recall 0.754938, f_score 0.554352\n",
      "=== total 74900 match 26355\n",
      ">>> step 75000\n",
      "step 74900-74999, precision 0.437699, recall 0.754938, f_score 0.554126\n",
      "=== total 75000 match 26372\n",
      ">>> step 75100\n",
      "step 75000-75099, precision 0.437351, recall 0.754938, f_score 0.553847\n",
      "=== total 75100 match 26393\n",
      ">>> step 75200\n",
      "step 75100-75199, precision 0.436970, recall 0.754938, f_score 0.553541\n",
      "=== total 75200 match 26416\n",
      ">>> step 75300\n",
      "step 75200-75299, precision 0.436789, recall 0.755162, f_score 0.553456\n",
      "=== total 75300 match 26459\n",
      ">>> step 75400\n",
      "step 75300-75399, precision 0.436344, recall 0.755162, f_score 0.553099\n",
      "=== total 75400 match 26486\n",
      ">>> step 75500\n",
      "step 75400-75499, precision 0.436452, recall 0.755577, f_score 0.553297\n",
      "=== total 75500 match 26539\n",
      ">>> step 75600\n",
      "step 75500-75599, precision 0.435957, recall 0.755133, f_score 0.552780\n",
      "=== total 75600 match 26576\n",
      ">>> step 75700\n",
      "step 75600-75699, precision 0.435646, recall 0.753757, f_score 0.552161\n",
      "=== total 75700 match 26595\n",
      ">>> step 75800\n",
      "step 75700-75799, precision 0.435666, recall 0.753394, f_score 0.552080\n",
      "=== total 75800 match 26619\n",
      ">>> step 75900\n",
      "step 75800-75899, precision 0.435344, recall 0.753410, f_score 0.551826\n",
      "=== total 75900 match 26641\n",
      ">>> step 76000\n",
      "step 75900-75999, precision 0.435156, recall 0.753113, f_score 0.551595\n",
      "=== total 76000 match 26687\n",
      ">>> step 76100\n",
      "step 76000-76099, precision 0.434960, recall 0.753113, f_score 0.551438\n",
      "=== total 76100 match 26699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 76200\n",
      "step 76100-76199, precision 0.434949, recall 0.751390, f_score 0.550966\n",
      "=== total 76200 match 26725\n",
      ">>> step 76300\n",
      "step 76200-76299, precision 0.434737, recall 0.751390, f_score 0.550796\n",
      "=== total 76300 match 26738\n",
      ">>> step 76400\n",
      "step 76300-76399, precision 0.434493, recall 0.751390, f_score 0.550600\n",
      "=== total 76400 match 26753\n",
      ">>> step 76500\n",
      "step 76400-76499, precision 0.434153, recall 0.751390, f_score 0.550327\n",
      "=== total 76500 match 26774\n",
      ">>> step 76600\n",
      "step 76500-76599, precision 0.433731, recall 0.751050, f_score 0.549897\n",
      "=== total 76600 match 26800\n",
      ">>> step 76700\n",
      "step 76600-76699, precision 0.433634, recall 0.748969, f_score 0.549261\n",
      "=== total 76700 match 26806\n",
      ">>> step 76800\n",
      "step 76700-76799, precision 0.433392, recall 0.746564, f_score 0.548418\n",
      "=== total 76800 match 26821\n",
      ">>> step 76900\n",
      "step 76800-76899, precision 0.433117, recall 0.746564, f_score 0.548198\n",
      "=== total 76900 match 26838\n",
      ">>> step 77000\n",
      "step 76900-76999, precision 0.432698, recall 0.746564, f_score 0.547863\n",
      "=== total 77000 match 26864\n",
      ">>> step 77100\n",
      "step 77000-77099, precision 0.433283, recall 0.747214, f_score 0.548506\n",
      "=== total 77100 match 26927\n",
      ">>> step 77200\n",
      "step 77100-77199, precision 0.433025, recall 0.747214, f_score 0.548300\n",
      "=== total 77200 match 26943\n",
      ">>> step 77300\n",
      "step 77200-77299, precision 0.433802, recall 0.748021, f_score 0.549140\n",
      "=== total 77300 match 27010\n",
      ">>> step 77400\n",
      "step 77300-77399, precision 0.433705, recall 0.748438, f_score 0.549175\n",
      "=== total 77400 match 27076\n",
      ">>> step 77500\n",
      "step 77400-77499, precision 0.434025, recall 0.749014, f_score 0.549586\n",
      "=== total 77500 match 27139\n",
      ">>> step 77600\n",
      "step 77500-77599, precision 0.434125, recall 0.749238, f_score 0.549726\n",
      "=== total 77600 match 27165\n",
      ">>> step 77700\n",
      "step 77600-77699, precision 0.433758, recall 0.749238, f_score 0.549432\n",
      "=== total 77700 match 27188\n",
      ">>> step 77800\n",
      "step 77700-77799, precision 0.434290, recall 0.749937, f_score 0.550046\n",
      "=== total 77800 match 27256\n",
      ">>> step 77900\n",
      "step 77800-77899, precision 0.433955, recall 0.749937, f_score 0.549778\n",
      "=== total 77900 match 27277\n",
      ">>> step 78000\n",
      "step 77900-77999, precision 0.433765, recall 0.749937, f_score 0.549625\n",
      "=== total 78000 match 27289\n",
      ">>> step 78100\n",
      "step 78000-78099, precision 0.433479, recall 0.749937, f_score 0.549395\n",
      "=== total 78100 match 27307\n",
      ">>> step 78200\n",
      "step 78100-78199, precision 0.433587, recall 0.749905, f_score 0.549474\n",
      "=== total 78200 match 27344\n",
      ">>> step 78300\n",
      "step 78200-78299, precision 0.434304, recall 0.750694, f_score 0.550261\n",
      "=== total 78300 match 27414\n",
      ">>> step 78400\n",
      "step 78300-78399, precision 0.434003, recall 0.750694, f_score 0.550020\n",
      "=== total 78400 match 27433\n",
      ">>> step 78500\n",
      "step 78400-78499, precision 0.433718, recall 0.750694, f_score 0.549791\n",
      "=== total 78500 match 27451\n",
      ">>> step 78600\n",
      "step 78500-78599, precision 0.433465, recall 0.750850, f_score 0.549630\n",
      "=== total 78600 match 27497\n",
      ">>> step 78700\n",
      "step 78600-78699, precision 0.434333, recall 0.751429, f_score 0.550482\n",
      "=== total 78700 match 27548\n",
      ">>> step 78800\n",
      "step 78700-78799, precision 0.433892, recall 0.751429, f_score 0.550128\n",
      "=== total 78800 match 27576\n",
      ">>> step 78900\n",
      "step 78800-78899, precision 0.434712, recall 0.752207, f_score 0.550995\n",
      "=== total 78900 match 27639\n",
      ">>> step 79000\n",
      "step 78900-78999, precision 0.434765, recall 0.751484, f_score 0.550844\n",
      "=== total 79000 match 27654\n",
      ">>> step 79100\n",
      "step 79000-79099, precision 0.435294, recall 0.751620, f_score 0.551305\n",
      "=== total 79100 match 27710\n",
      ">>> step 79200\n",
      "step 79100-79199, precision 0.435032, recall 0.751636, f_score 0.551099\n",
      "=== total 79200 match 27729\n",
      ">>> step 79300\n",
      "step 79200-79299, precision 0.435444, recall 0.752222, f_score 0.551587\n",
      "=== total 79300 match 27790\n",
      ">>> step 79400\n",
      "step 79300-79399, precision 0.435391, recall 0.752407, f_score 0.551594\n",
      "=== total 79400 match 27821\n",
      ">>> step 79500\n",
      "step 79400-79499, precision 0.435555, recall 0.752558, f_score 0.551766\n",
      "=== total 79500 match 27861\n",
      ">>> step 79600\n",
      "step 79500-79599, precision 0.435336, recall 0.752558, f_score 0.551591\n",
      "=== total 79600 match 27875\n",
      ">>> step 79700\n",
      "step 79600-79699, precision 0.435055, recall 0.752558, f_score 0.551365\n",
      "=== total 79700 match 27893\n",
      ">>> step 79800\n",
      "step 79700-79799, precision 0.434619, recall 0.752558, f_score 0.551015\n",
      "=== total 79800 match 27921\n",
      ">>> step 79900\n",
      "step 79800-79899, precision 0.434678, recall 0.752956, f_score 0.551169\n",
      "=== total 79900 match 27977\n",
      ">>> step 80000\n",
      "step 79900-79999, precision 0.434539, recall 0.752956, f_score 0.551057\n",
      "=== total 80000 match 27986\n",
      ">>> step 80100\n",
      "step 80000-80099, precision 0.434651, recall 0.753323, f_score 0.551245\n",
      "=== total 80100 match 28034\n",
      ">>> step 80200\n",
      "step 80100-80199, precision 0.434428, recall 0.753354, f_score 0.551074\n",
      "=== total 80200 match 28053\n",
      ">>> step 80300\n",
      "step 80200-80299, precision 0.434537, recall 0.753810, f_score 0.551284\n",
      "=== total 80300 match 28115\n",
      ">>> step 80400\n",
      "step 80300-80399, precision 0.434710, recall 0.754128, f_score 0.551508\n",
      "=== total 80400 match 28159\n",
      ">>> step 80500\n",
      "step 80400-80499, precision 0.434661, recall 0.754079, f_score 0.551456\n",
      "=== total 80500 match 28176\n",
      ">>> step 80600\n",
      "step 80500-80599, precision 0.434706, recall 0.753395, f_score 0.551309\n",
      "=== total 80600 match 28203\n",
      ">>> step 80700\n",
      "step 80600-80699, precision 0.434459, recall 0.753395, f_score 0.551110\n",
      "=== total 80700 match 28219\n",
      ">>> step 80800\n",
      "step 80700-80799, precision 0.434795, recall 0.753940, f_score 0.551526\n",
      "=== total 80800 match 28280\n",
      ">>> step 80900\n",
      "step 80800-80899, precision 0.434798, recall 0.754151, f_score 0.551585\n",
      "=== total 80900 match 28312\n",
      ">>> step 81000\n",
      "step 80900-80999, precision 0.434414, recall 0.754151, f_score 0.551276\n",
      "=== total 81000 match 28337\n",
      ">>> step 81100\n",
      "step 81000-81099, precision 0.434093, recall 0.754151, f_score 0.551017\n",
      "=== total 81100 match 28358\n",
      ">>> step 81200\n",
      "step 81100-81199, precision 0.434452, recall 0.754722, f_score 0.551459\n",
      "=== total 81200 match 28422\n",
      ">>> step 81300\n",
      "step 81200-81299, precision 0.435124, recall 0.755469, f_score 0.552200\n",
      "=== total 81300 match 28493\n",
      ">>> step 81400\n",
      "step 81300-81399, precision 0.434911, recall 0.755469, f_score 0.552028\n",
      "=== total 81400 match 28507\n",
      ">>> step 81500\n",
      "step 81400-81499, precision 0.434545, recall 0.755469, f_score 0.551733\n",
      "=== total 81500 match 28531\n",
      ">>> step 81600\n",
      "step 81500-81599, precision 0.434649, recall 0.755856, f_score 0.551920\n",
      "=== total 81600 match 28584\n",
      ">>> step 81700\n",
      "step 81600-81699, precision 0.435271, recall 0.756596, f_score 0.552619\n",
      "=== total 81700 match 28658\n",
      ">>> step 81800\n",
      "step 81700-81799, precision 0.434983, recall 0.756596, f_score 0.552387\n",
      "=== total 81800 match 28677\n",
      ">>> step 81900\n",
      "step 81800-81899, precision 0.435276, recall 0.756551, f_score 0.552611\n",
      "=== total 81900 match 28722\n",
      ">>> step 82000\n",
      "step 81900-81999, precision 0.435386, recall 0.756073, f_score 0.552572\n",
      "=== total 82000 match 28740\n",
      ">>> step 82100\n",
      "step 82000-82099, precision 0.435587, recall 0.756235, f_score 0.552777\n",
      "=== total 82100 match 28752\n",
      ">>> step 82200\n",
      "step 82100-82199, precision 0.435653, recall 0.756352, f_score 0.552862\n",
      "=== total 82200 match 28766\n",
      ">>> step 82300\n",
      "step 82200-82299, precision 0.435623, recall 0.756352, f_score 0.552838\n",
      "=== total 82300 match 28768\n",
      ">>> step 82400\n",
      "step 82300-82399, precision 0.435921, recall 0.756609, f_score 0.553147\n",
      "=== total 82400 match 28824\n",
      ">>> step 82500\n",
      "step 82400-82499, precision 0.435861, recall 0.756472, f_score 0.553061\n",
      "=== total 82500 match 28828\n",
      ">>> step 82600\n",
      "step 82500-82599, precision 0.435737, recall 0.755242, f_score 0.552632\n",
      "=== total 82600 match 28850\n",
      ">>> step 82700\n",
      "step 82600-82699, precision 0.435910, recall 0.754840, f_score 0.552664\n",
      "=== total 82700 match 28889\n",
      ">>> step 82800\n",
      "step 82700-82799, precision 0.435563, recall 0.753711, f_score 0.552082\n",
      "=== total 82800 match 28912\n",
      ">>> step 82900\n",
      "step 82800-82899, precision 0.435915, recall 0.754079, f_score 0.552464\n",
      "=== total 82900 match 28946\n",
      ">>> step 83000\n",
      "step 82900-82999, precision 0.435834, recall 0.753073, f_score 0.552128\n",
      "=== total 83000 match 28956\n",
      ">>> step 83100\n",
      "step 83000-83099, precision 0.435804, recall 0.753073, f_score 0.552104\n",
      "=== total 83100 match 28958\n",
      ">>> step 83200\n",
      "step 83100-83199, precision 0.435787, recall 0.752668, f_score 0.551982\n",
      "=== total 83200 match 28966\n",
      ">>> step 83300\n",
      "step 83200-83299, precision 0.435828, recall 0.752443, f_score 0.551955\n",
      "=== total 83300 match 28977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 83400\n",
      "step 83300-83399, precision 0.435927, recall 0.751873, f_score 0.551881\n",
      "=== total 83400 match 29014\n",
      ">>> step 83500\n",
      "step 83400-83499, precision 0.435681, recall 0.751411, f_score 0.551559\n",
      "=== total 83500 match 29035\n",
      ">>> step 83600\n",
      "step 83500-83599, precision 0.435725, recall 0.750890, f_score 0.551454\n",
      "=== total 83600 match 29055\n",
      ">>> step 83700\n",
      "step 83600-83699, precision 0.435635, recall 0.750890, f_score 0.551382\n",
      "=== total 83700 match 29061\n",
      ">>> step 83800\n",
      "step 83700-83799, precision 0.435770, recall 0.750370, f_score 0.551349\n",
      "=== total 83800 match 29075\n",
      ">>> step 83900\n",
      "step 83800-83899, precision 0.435802, recall 0.749497, f_score 0.551139\n",
      "=== total 83900 match 29082\n",
      ">>> step 84000\n",
      "step 83900-83999, precision 0.435949, recall 0.749690, f_score 0.551309\n",
      "=== total 84000 match 29102\n",
      ">>> step 84100\n",
      "step 84000-84099, precision 0.436299, recall 0.750059, f_score 0.551688\n",
      "=== total 84100 match 29136\n",
      ">>> step 84200\n",
      "step 84100-84199, precision 0.436316, recall 0.748088, f_score 0.551168\n",
      "=== total 84200 match 29144\n",
      ">>> step 84300\n",
      "step 84200-84299, precision 0.436286, recall 0.746200, f_score 0.550631\n",
      "=== total 84300 match 29146\n",
      ">>> step 84400\n",
      "step 84300-84399, precision 0.436241, recall 0.744758, f_score 0.550202\n",
      "=== total 84400 match 29149\n",
      ">>> step 84500\n",
      "step 84400-84499, precision 0.436241, recall 0.742931, f_score 0.549703\n",
      "=== total 84500 match 29149\n",
      ">>> step 84600\n",
      "step 84500-84599, precision 0.436241, recall 0.742584, f_score 0.549608\n",
      "=== total 84600 match 29149\n",
      ">>> step 84700\n",
      "step 84600-84699, precision 0.436241, recall 0.742584, f_score 0.549608\n",
      "=== total 84700 match 29149\n",
      ">>> step 84800\n",
      "step 84700-84799, precision 0.436557, recall 0.741558, f_score 0.549577\n",
      "=== total 84800 match 29176\n",
      ">>> step 84900\n",
      "step 84800-84899, precision 0.436768, recall 0.741337, f_score 0.549683\n",
      "=== total 84900 match 29194\n",
      ">>> step 85000\n",
      "step 84900-84999, precision 0.436776, recall 0.740334, f_score 0.549414\n",
      "=== total 85000 match 29198\n",
      ">>> step 85100\n",
      "step 85000-85099, precision 0.437089, recall 0.738791, f_score 0.549236\n",
      "=== total 85100 match 29216\n",
      ">>> step 85200\n",
      "step 85100-85199, precision 0.437271, recall 0.737252, f_score 0.548953\n",
      "=== total 85200 match 29229\n",
      ">>> step 85300\n",
      "step 85200-85299, precision 0.437244, recall 0.735700, f_score 0.548501\n",
      "=== total 85300 match 29240\n",
      ">>> step 85400\n",
      "step 85300-85399, precision 0.437301, recall 0.733762, f_score 0.548006\n",
      "=== total 85400 match 29243\n",
      ">>> step 85500\n",
      "step 85400-85499, precision 0.437421, recall 0.732063, f_score 0.547626\n",
      "=== total 85500 match 29251\n",
      ">>> step 85600\n",
      "step 85500-85599, precision 0.437376, recall 0.732063, f_score 0.547591\n",
      "=== total 85600 match 29254\n",
      ">>> step 85700\n",
      "step 85600-85699, precision 0.437376, recall 0.730225, f_score 0.547075\n",
      "=== total 85700 match 29254\n",
      ">>> step 85800\n",
      "step 85700-85799, precision 0.437376, recall 0.730225, f_score 0.547075\n",
      "=== total 85800 match 29254\n",
      ">>> step 85900\n",
      "step 85800-85899, precision 0.437376, recall 0.729143, f_score 0.546772\n",
      "=== total 85900 match 29254\n",
      ">>> step 86000\n",
      "step 85900-85999, precision 0.437331, recall 0.728064, f_score 0.546433\n",
      "=== total 86000 match 29257\n",
      ">>> step 86100\n",
      "step 86000-86099, precision 0.437331, recall 0.727526, f_score 0.546281\n",
      "=== total 86100 match 29257\n",
      ">>> step 86200\n",
      "step 86100-86199, precision 0.437257, recall 0.725464, f_score 0.545641\n",
      "=== total 86200 match 29262\n",
      ">>> step 86300\n",
      "step 86200-86299, precision 0.437216, recall 0.724863, f_score 0.545439\n",
      "=== total 86300 match 29267\n",
      ">>> step 86400\n",
      "step 86300-86399, precision 0.437186, recall 0.723919, f_score 0.545149\n",
      "=== total 86400 match 29269\n",
      ">>> step 86500\n",
      "step 86400-86499, precision 0.437246, recall 0.722272, f_score 0.544727\n",
      "=== total 86500 match 29281\n",
      ">>> step 86600\n",
      "step 86500-86599, precision 0.437246, recall 0.722272, f_score 0.544727\n",
      "=== total 86600 match 29281\n",
      ">>> step 86700\n",
      "step 86600-86699, precision 0.437141, recall 0.720241, f_score 0.544068\n",
      "=== total 86700 match 29288\n",
      ">>> step 86800\n",
      "step 86700-86799, precision 0.437150, recall 0.718333, f_score 0.543529\n",
      "=== total 86800 match 29292\n",
      ">>> step 86900\n",
      "step 86800-86899, precision 0.437150, recall 0.717568, f_score 0.543310\n",
      "=== total 86900 match 29292\n",
      ">>> step 87000\n",
      "step 86900-86999, precision 0.437182, recall 0.715307, f_score 0.542685\n",
      "=== total 87000 match 29299\n",
      ">>> step 87100\n",
      "step 87000-87099, precision 0.437142, recall 0.714684, f_score 0.542475\n",
      "=== total 87100 match 29304\n",
      ">>> step 87200\n",
      "step 87100-87199, precision 0.437219, recall 0.712562, f_score 0.541921\n",
      "=== total 87200 match 29308\n",
      ">>> step 87300\n",
      "step 87200-87299, precision 0.437649, recall 0.711680, f_score 0.541996\n",
      "=== total 87300 match 29334\n",
      ">>> step 87400\n",
      "step 87300-87399, precision 0.437681, recall 0.711626, f_score 0.542005\n",
      "=== total 87400 match 29341\n",
      ">>> step 87500\n",
      "step 87400-87499, precision 0.437681, recall 0.709660, f_score 0.541434\n",
      "=== total 87500 match 29341\n",
      ">>> step 87600\n",
      "step 87500-87599, precision 0.437547, recall 0.709660, f_score 0.541331\n",
      "=== total 87600 match 29350\n",
      ">>> step 87700\n",
      "step 87600-87699, precision 0.437517, recall 0.708681, f_score 0.541023\n",
      "=== total 87700 match 29352\n",
      ">>> step 87800\n",
      "step 87700-87799, precision 0.437728, recall 0.707297, f_score 0.540780\n",
      "=== total 87800 match 29363\n",
      ">>> step 87900\n",
      "step 87800-87899, precision 0.437609, recall 0.707297, f_score 0.540689\n",
      "=== total 87900 match 29371\n",
      ">>> step 88000\n",
      "step 87900-87999, precision 0.437594, recall 0.705317, f_score 0.540099\n",
      "=== total 88000 match 29372\n",
      ">>> step 88100\n",
      "step 88000-88099, precision 0.437598, recall 0.703404, f_score 0.539540\n",
      "=== total 88100 match 29374\n",
      ">>> step 88200\n",
      "step 88100-88199, precision 0.438237, recall 0.703394, f_score 0.540023\n",
      "=== total 88200 match 29411\n",
      ">>> step 88300\n",
      "step 88200-88299, precision 0.438256, recall 0.701917, f_score 0.539601\n",
      "=== total 88300 match 29412\n",
      ">>> step 88400\n",
      "step 88300-88399, precision 0.438269, recall 0.700707, f_score 0.539253\n",
      "=== total 88400 match 29418\n",
      ">>> step 88500\n",
      "step 88400-88499, precision 0.438273, recall 0.698862, f_score 0.538709\n",
      "=== total 88500 match 29420\n",
      ">>> step 88600\n",
      "step 88500-88599, precision 0.438267, recall 0.697307, f_score 0.538242\n",
      "=== total 88600 match 29425\n",
      ">>> step 88700\n",
      "step 88600-88699, precision 0.438177, recall 0.697307, f_score 0.538174\n",
      "=== total 88700 match 29431\n",
      ">>> step 88800\n",
      "step 88700-88799, precision 0.438148, recall 0.696328, f_score 0.537860\n",
      "=== total 88800 match 29433\n",
      ">>> step 88900\n",
      "step 88800-88899, precision 0.438103, recall 0.695915, f_score 0.537703\n",
      "=== total 88900 match 29436\n",
      ">>> step 89000\n",
      "step 88900-88999, precision 0.438043, recall 0.695352, f_score 0.537490\n",
      "=== total 89000 match 29440\n",
      ">>> step 89100\n",
      "step 89000-89099, precision 0.438977, recall 0.695781, f_score 0.538321\n",
      "=== total 89100 match 29489\n",
      ">>> step 89200\n",
      "step 89100-89199, precision 0.438918, recall 0.694884, f_score 0.538008\n",
      "=== total 89200 match 29493\n",
      ">>> step 89300\n",
      "step 89200-89299, precision 0.439091, recall 0.694650, f_score 0.538067\n",
      "=== total 89300 match 29511\n",
      ">>> step 89400\n",
      "step 89300-89399, precision 0.439175, recall 0.693819, f_score 0.537881\n",
      "=== total 89400 match 29519\n",
      ">>> step 89500\n",
      "step 89400-89499, precision 0.439162, recall 0.693884, f_score 0.537890\n",
      "=== total 89500 match 29529\n",
      ">>> step 89600\n",
      "step 89500-89599, precision 0.439977, recall 0.693772, f_score 0.538468\n",
      "=== total 89600 match 29572\n",
      ">>> step 89700\n",
      "step 89600-89699, precision 0.439970, recall 0.692734, f_score 0.538150\n",
      "=== total 89700 match 29577\n",
      ">>> step 89800\n",
      "step 89700-89799, precision 0.439940, recall 0.691556, f_score 0.537772\n",
      "=== total 89800 match 29579\n",
      ">>> step 89900\n",
      "step 89800-89899, precision 0.440130, recall 0.690253, f_score 0.537519\n",
      "=== total 89900 match 29589\n",
      ">>> step 90000\n",
      "step 89900-89999, precision 0.440477, recall 0.690586, f_score 0.537878\n",
      "=== total 90000 match 29627\n",
      ">>> step 90100\n",
      "step 90000-90099, precision 0.441139, recall 0.691179, f_score 0.538552\n",
      "=== total 90100 match 29680\n",
      ">>> step 90200\n",
      "step 90100-90199, precision 0.441181, recall 0.689728, f_score 0.538142\n",
      "=== total 90200 match 29693\n",
      ">>> step 90300\n",
      "step 90200-90299, precision 0.441284, recall 0.689907, f_score 0.538274\n",
      "=== total 90300 match 29711\n",
      ">>> step 90400\n",
      "step 90300-90399, precision 0.441939, recall 0.690490, f_score 0.538938\n",
      "=== total 90400 match 29753\n",
      ">>> step 90500\n",
      "step 90400-90499, precision 0.441850, recall 0.690490, f_score 0.538871\n",
      "=== total 90500 match 29759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 90600\n",
      "step 90500-90599, precision 0.442038, recall 0.690492, f_score 0.539013\n",
      "=== total 90600 match 29787\n",
      ">>> step 90700\n",
      "step 90600-90699, precision 0.442840, recall 0.691302, f_score 0.539855\n",
      "=== total 90700 match 29846\n",
      ">>> step 90800\n",
      "step 90700-90799, precision 0.443128, recall 0.691641, f_score 0.540172\n",
      "=== total 90800 match 29874\n",
      ">>> step 90900\n",
      "step 90800-90899, precision 0.443113, recall 0.689867, f_score 0.539620\n",
      "=== total 90900 match 29884\n",
      ">>> step 91000\n",
      "step 90900-90999, precision 0.443103, recall 0.689402, f_score 0.539470\n",
      "=== total 91000 match 29905\n",
      ">>> step 91100\n",
      "step 91000-91099, precision 0.443572, recall 0.688813, f_score 0.539637\n",
      "=== total 91100 match 29941\n",
      ">>> step 91200\n",
      "step 91100-91199, precision 0.443721, recall 0.687296, f_score 0.539281\n",
      "=== total 91200 match 29958\n",
      ">>> step 91300\n",
      "step 91200-91299, precision 0.443989, recall 0.687406, f_score 0.539512\n",
      "=== total 91300 match 29985\n",
      ">>> step 91400\n",
      "step 91300-91399, precision 0.444500, recall 0.687890, f_score 0.540038\n",
      "=== total 91400 match 30018\n",
      ">>> step 91500\n",
      "step 91400-91499, precision 0.444470, recall 0.686333, f_score 0.539536\n",
      "=== total 91500 match 30020\n",
      ">>> step 91600\n",
      "step 91500-91599, precision 0.444411, recall 0.686333, f_score 0.539493\n",
      "=== total 91600 match 30024\n",
      ">>> step 91700\n",
      "step 91600-91699, precision 0.444718, recall 0.686598, f_score 0.539800\n",
      "=== total 91700 match 30055\n",
      ">>> step 91800\n",
      "step 91700-91799, precision 0.445013, recall 0.686913, f_score 0.540115\n",
      "=== total 91800 match 30089\n",
      ">>> step 91900\n",
      "step 91800-91899, precision 0.445127, recall 0.686626, f_score 0.540110\n",
      "=== total 91900 match 30115\n",
      ">>> step 92000\n",
      "step 91900-91999, precision 0.445388, recall 0.686439, f_score 0.540245\n",
      "=== total 92000 match 30140\n",
      ">>> step 92100\n",
      "step 92000-92099, precision 0.445329, recall 0.685528, f_score 0.539919\n",
      "=== total 92100 match 30144\n",
      ">>> step 92200\n",
      "step 92100-92199, precision 0.446067, recall 0.686179, f_score 0.540663\n",
      "=== total 92200 match 30195\n",
      ">>> step 92300\n",
      "step 92200-92299, precision 0.446078, recall 0.686227, f_score 0.540686\n",
      "=== total 92300 match 30201\n",
      ">>> step 92400\n",
      "step 92300-92399, precision 0.446202, recall 0.685717, f_score 0.540619\n",
      "=== total 92400 match 30224\n",
      ">>> step 92500\n",
      "step 92400-92499, precision 0.446238, recall 0.683901, f_score 0.540080\n",
      "=== total 92500 match 30235\n",
      ">>> step 92600\n",
      "step 92500-92599, precision 0.446046, recall 0.683762, f_score 0.539896\n",
      "=== total 92600 match 30248\n",
      ">>> step 92700\n",
      "step 92600-92699, precision 0.446597, recall 0.684282, f_score 0.540462\n",
      "=== total 92700 match 30298\n",
      ">>> step 92800\n",
      "step 92700-92799, precision 0.447155, recall 0.684566, f_score 0.540959\n",
      "=== total 92800 match 30334\n",
      ">>> step 92900\n",
      "step 92800-92899, precision 0.447097, recall 0.684773, f_score 0.540981\n",
      "=== total 92900 match 30367\n",
      ">>> step 93000\n",
      "step 92900-92999, precision 0.446671, recall 0.684773, f_score 0.540669\n",
      "=== total 93000 match 30396\n",
      ">>> step 93100\n",
      "step 93000-93099, precision 0.446348, recall 0.684773, f_score 0.540432\n",
      "=== total 93100 match 30418\n",
      ">>> step 93200\n",
      "step 93100-93199, precision 0.446100, recall 0.684980, f_score 0.540315\n",
      "=== total 93200 match 30464\n",
      ">>> step 93300\n",
      "step 93200-93299, precision 0.445618, recall 0.684980, f_score 0.539961\n",
      "=== total 93300 match 30497\n",
      ">>> step 93400\n",
      "step 93300-93399, precision 0.445627, recall 0.685345, f_score 0.540081\n",
      "=== total 93400 match 30548\n",
      ">>> step 93500\n",
      "step 93400-93499, precision 0.445066, recall 0.685376, f_score 0.539678\n",
      "=== total 93500 match 30591\n",
      ">>> step 93600\n",
      "step 93500-93599, precision 0.444778, recall 0.685582, f_score 0.539530\n",
      "=== total 93600 match 30640\n",
      ">>> step 93700\n",
      "step 93600-93699, precision 0.444434, recall 0.685586, f_score 0.539278\n",
      "=== total 93700 match 30684\n",
      ">>> step 93800\n",
      "step 93700-93799, precision 0.444173, recall 0.685586, f_score 0.539086\n",
      "=== total 93800 match 30702\n",
      ">>> step 93900\n",
      "step 93800-93899, precision 0.444260, recall 0.685931, f_score 0.539257\n",
      "=== total 93900 match 30750\n",
      ">>> step 94000\n",
      "step 93900-93999, precision 0.444044, recall 0.685931, f_score 0.539097\n",
      "=== total 94000 match 30765\n",
      ">>> step 94100\n",
      "step 94000-94099, precision 0.443731, recall 0.686136, f_score 0.538930\n",
      "=== total 94100 match 30816\n",
      ">>> step 94200\n",
      "step 94100-94199, precision 0.443691, recall 0.685939, f_score 0.538840\n",
      "=== total 94200 match 30830\n",
      ">>> step 94300\n",
      "step 94200-94299, precision 0.443566, recall 0.685943, f_score 0.538749\n",
      "=== total 94300 match 30859\n",
      ">>> step 94400\n",
      "step 94300-94399, precision 0.443140, recall 0.686148, f_score 0.538498\n",
      "=== total 94400 match 30918\n",
      ">>> step 94500\n",
      "step 94400-94499, precision 0.442725, recall 0.686148, f_score 0.538191\n",
      "=== total 94500 match 30947\n",
      ">>> step 94600\n",
      "step 94500-94599, precision 0.442396, recall 0.686148, f_score 0.537948\n",
      "=== total 94600 match 30970\n",
      ">>> step 94700\n",
      "step 94600-94699, precision 0.441882, recall 0.686148, f_score 0.537568\n",
      "=== total 94700 match 31006\n",
      ">>> step 94800\n",
      "step 94700-94799, precision 0.441547, recall 0.686352, f_score 0.537382\n",
      "=== total 94800 match 31059\n",
      ">>> step 94900\n",
      "step 94800-94899, precision 0.440880, recall 0.686352, f_score 0.536888\n",
      "=== total 94900 match 31106\n",
      ">>> step 95000\n",
      "step 94900-94999, precision 0.440946, recall 0.686744, f_score 0.537057\n",
      "=== total 95000 match 31158\n",
      ">>> step 95100\n",
      "step 95000-95099, precision 0.440928, recall 0.687135, f_score 0.537163\n",
      "=== total 95100 match 31216\n",
      ">>> step 95200\n",
      "step 95100-95199, precision 0.440518, recall 0.687135, f_score 0.536859\n",
      "=== total 95200 match 31245\n",
      ">>> step 95300\n",
      "step 95200-95299, precision 0.440585, recall 0.687525, f_score 0.537028\n",
      "=== total 95300 match 31297\n",
      ">>> step 95400\n",
      "step 95300-95399, precision 0.440610, recall 0.687914, f_score 0.537165\n",
      "=== total 95400 match 31352\n",
      ">>> step 95500\n",
      "step 95400-95499, precision 0.440546, recall 0.688252, f_score 0.537221\n",
      "=== total 95500 match 31411\n",
      ">>> step 95600\n",
      "step 95500-95599, precision 0.440086, recall 0.688205, f_score 0.536864\n",
      "=== total 95600 match 31462\n",
      ">>> step 95700\n",
      "step 95600-95699, precision 0.440126, recall 0.688592, f_score 0.537011\n",
      "=== total 95700 match 31516\n",
      ">>> step 95800\n",
      "step 95700-95799, precision 0.439833, recall 0.688592, f_score 0.536793\n",
      "=== total 95800 match 31537\n",
      ">>> step 95900\n",
      "step 95800-95899, precision 0.439470, recall 0.688592, f_score 0.536523\n",
      "=== total 95900 match 31563\n",
      ">>> step 96000\n",
      "step 95900-95999, precision 0.439317, recall 0.688592, f_score 0.536409\n",
      "=== total 96000 match 31574\n",
      ">>> step 96100\n",
      "step 96000-96099, precision 0.439107, recall 0.688808, f_score 0.536318\n",
      "=== total 96100 match 31621\n",
      ">>> step 96200\n",
      "step 96100-96199, precision 0.438848, recall 0.688994, f_score 0.536181\n",
      "=== total 96200 match 31667\n",
      ">>> step 96300\n",
      "step 96200-96299, precision 0.438663, recall 0.689194, f_score 0.536103\n",
      "=== total 96300 match 31710\n",
      ">>> step 96400\n",
      "step 96300-96399, precision 0.438373, recall 0.689194, f_score 0.535886\n",
      "=== total 96400 match 31731\n",
      ">>> step 96500\n",
      "step 96400-96499, precision 0.438167, recall 0.688852, f_score 0.535629\n",
      "=== total 96500 match 31771\n",
      ">>> step 96600\n",
      "step 96500-96599, precision 0.437891, recall 0.688852, f_score 0.535423\n",
      "=== total 96600 match 31791\n",
      ">>> step 96700\n",
      "step 96600-96699, precision 0.437643, recall 0.688852, f_score 0.535238\n",
      "=== total 96700 match 31809\n",
      ">>> step 96800\n",
      "step 96700-96799, precision 0.437327, recall 0.687701, f_score 0.534654\n",
      "=== total 96800 match 31848\n",
      ">>> step 96900\n",
      "step 96800-96899, precision 0.437039, recall 0.687701, f_score 0.534438\n",
      "=== total 96900 match 31869\n",
      ">>> step 97000\n",
      "step 96900-96999, precision 0.436868, recall 0.688101, f_score 0.534431\n",
      "=== total 97000 match 31941\n",
      ">>> step 97100\n",
      "step 97000-97099, precision 0.436857, recall 0.688451, f_score 0.534529\n",
      "=== total 97100 match 31999\n",
      ">>> step 97200\n",
      "step 97100-97199, precision 0.436353, recall 0.688451, f_score 0.534151\n",
      "=== total 97200 match 32036\n",
      ">>> step 97300\n",
      "step 97200-97299, precision 0.436774, recall 0.688774, f_score 0.534564\n",
      "=== total 97300 match 32099\n",
      ">>> step 97400\n",
      "step 97300-97399, precision 0.436352, recall 0.688774, f_score 0.534248\n",
      "=== total 97400 match 32130\n",
      ">>> step 97500\n",
      "step 97400-97499, precision 0.436007, recall 0.688924, f_score 0.534034\n",
      "=== total 97500 match 32183\n",
      ">>> step 97600\n",
      "step 97500-97599, precision 0.435785, recall 0.688858, f_score 0.533848\n",
      "=== total 97600 match 32220\n",
      ">>> step 97700\n",
      "step 97600-97699, precision 0.435393, recall 0.688741, f_score 0.533518\n",
      "=== total 97700 match 32272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 97800\n",
      "step 97700-97799, precision 0.435675, recall 0.689304, f_score 0.533899\n",
      "=== total 97800 match 32336\n",
      ">>> step 97900\n",
      "step 97800-97899, precision 0.435560, recall 0.689650, f_score 0.533916\n",
      "=== total 97900 match 32402\n",
      ">>> step 98000\n",
      "step 97900-97999, precision 0.435090, recall 0.689650, f_score 0.533563\n",
      "=== total 98000 match 32437\n",
      ">>> step 98100\n",
      "step 98000-98099, precision 0.434754, recall 0.689650, f_score 0.533311\n",
      "=== total 98100 match 32462\n",
      ">>> step 98200\n",
      "step 98100-98199, precision 0.434554, recall 0.689650, f_score 0.533160\n",
      "=== total 98200 match 32477\n",
      ">>> step 98300\n",
      "step 98200-98299, precision 0.434393, recall 0.689650, f_score 0.533039\n",
      "=== total 98300 match 32489\n",
      ">>> step 98400\n",
      "step 98300-98399, precision 0.434471, recall 0.690044, f_score 0.533215\n",
      "=== total 98400 match 32543\n",
      ">>> step 98500\n",
      "step 98400-98499, precision 0.434124, recall 0.690044, f_score 0.532954\n",
      "=== total 98500 match 32569\n",
      ">>> step 98600\n",
      "step 98500-98599, precision 0.434633, recall 0.690652, f_score 0.533519\n",
      "=== total 98600 match 32639\n",
      ">>> step 98700\n",
      "step 98600-98699, precision 0.435039, recall 0.691161, f_score 0.533976\n",
      "=== total 98700 match 32712\n",
      ">>> step 98800\n",
      "step 98700-98799, precision 0.434548, recall 0.691161, f_score 0.533606\n",
      "=== total 98800 match 32749\n",
      ">>> step 98900\n",
      "step 98800-98899, precision 0.435158, recall 0.691909, f_score 0.534289\n",
      "=== total 98900 match 32818\n",
      ">>> step 99000\n",
      "step 98900-98999, precision 0.434925, recall 0.692282, f_score 0.534225\n",
      "=== total 99000 match 32893\n",
      ">>> step 99100\n",
      "step 99000-99099, precision 0.434655, recall 0.692653, f_score 0.534131\n",
      "=== total 99100 match 32971\n",
      ">>> step 99200\n",
      "step 99100-99199, precision 0.435168, recall 0.693394, f_score 0.534739\n",
      "=== total 99200 match 33047\n",
      ">>> step 99300\n",
      "step 99200-99299, precision 0.435033, recall 0.693778, f_score 0.534751\n",
      "=== total 99300 match 33117\n",
      ">>> step 99400\n",
      "step 99300-99399, precision 0.435105, recall 0.693313, f_score 0.534667\n",
      "=== total 99400 match 33169\n",
      ">>> step 99500\n",
      "step 99400-99499, precision 0.434489, recall 0.693313, f_score 0.534202\n",
      "=== total 99500 match 33216\n",
      ">>> step 99600\n",
      "step 99500-99599, precision 0.434346, recall 0.693313, f_score 0.534093\n",
      "=== total 99600 match 33227\n",
      ">>> step 99700\n",
      "step 99600-99699, precision 0.434058, recall 0.693313, f_score 0.533876\n",
      "=== total 99700 match 33249\n",
      ">>> step 99800\n",
      "step 99700-99799, precision 0.433654, recall 0.693246, f_score 0.533550\n",
      "=== total 99800 match 33280\n",
      ">>> step 99900\n",
      "step 99800-99899, precision 0.433483, recall 0.692991, f_score 0.533346\n",
      "=== total 99900 match 33300\n",
      ">>> step 100000\n",
      "step 99900-99999, precision 0.433132, recall 0.692991, f_score 0.533080\n",
      "=== total 100000 match 33327\n",
      ">>> step 100100\n",
      "step 100000-100099, precision 0.433099, recall 0.692311, f_score 0.532854\n",
      "=== total 100100 match 33348\n",
      ">>> step 100200\n",
      "step 100100-100199, precision 0.432840, recall 0.692311, f_score 0.532657\n",
      "=== total 100200 match 33368\n",
      ">>> step 100300\n",
      "step 100200-100299, precision 0.432464, recall 0.692311, f_score 0.532373\n",
      "=== total 100300 match 33397\n",
      ">>> step 100400\n",
      "step 100300-100399, precision 0.432854, recall 0.691899, f_score 0.532546\n",
      "=== total 100400 match 33427\n",
      ">>> step 100500\n",
      "step 100400-100499, precision 0.432517, recall 0.691899, f_score 0.532291\n",
      "=== total 100500 match 33453\n",
      ">>> step 100600\n",
      "step 100500-100599, precision 0.432387, recall 0.691183, f_score 0.531981\n",
      "=== total 100600 match 33470\n",
      ">>> step 100700\n",
      "step 100600-100699, precision 0.432505, recall 0.691566, f_score 0.532183\n",
      "=== total 100700 match 33521\n",
      ">>> step 100800\n",
      "step 100700-100799, precision 0.432118, recall 0.691566, f_score 0.531890\n",
      "=== total 100800 match 33551\n",
      ">>> step 100900\n",
      "step 100800-100899, precision 0.431591, recall 0.691566, f_score 0.531491\n",
      "=== total 100900 match 33592\n",
      ">>> step 101000\n",
      "step 100900-100999, precision 0.431528, recall 0.691802, f_score 0.531512\n",
      "=== total 101000 match 33634\n",
      ">>> step 101100\n",
      "step 101000-101099, precision 0.431261, recall 0.691949, f_score 0.531353\n",
      "=== total 101100 match 33678\n",
      ">>> step 101200\n",
      "step 101100-101199, precision 0.431086, recall 0.691140, f_score 0.530982\n",
      "=== total 101200 match 33694\n",
      ">>> step 101300\n",
      "step 101200-101299, precision 0.431299, recall 0.691284, f_score 0.531186\n",
      "=== total 101300 match 33726\n",
      ">>> step 101400\n",
      "step 101300-101399, precision 0.430903, recall 0.691284, f_score 0.530886\n",
      "=== total 101400 match 33757\n",
      ">>> step 101500\n",
      "step 101400-101499, precision 0.430546, recall 0.691284, f_score 0.530614\n",
      "=== total 101500 match 33785\n",
      ">>> step 101600\n",
      "step 101500-101599, precision 0.430190, recall 0.691284, f_score 0.530344\n",
      "=== total 101600 match 33813\n",
      ">>> step 101700\n",
      "step 101600-101699, precision 0.430117, recall 0.690872, f_score 0.530167\n",
      "=== total 101700 match 33821\n",
      ">>> step 101800\n",
      "step 101700-101799, precision 0.429875, recall 0.690971, f_score 0.530013\n",
      "=== total 101800 match 33861\n",
      ">>> step 101900\n",
      "step 101800-101899, precision 0.430315, recall 0.691510, f_score 0.530505\n",
      "=== total 101900 match 33917\n",
      ">>> step 102000\n",
      "step 101900-101999, precision 0.430864, recall 0.692064, f_score 0.531086\n",
      "=== total 102000 match 33962\n",
      ">>> step 102100\n",
      "step 102000-102099, precision 0.430484, recall 0.692064, f_score 0.530797\n",
      "=== total 102100 match 33992\n",
      ">>> step 102200\n",
      "step 102100-102199, precision 0.431081, recall 0.692507, f_score 0.531381\n",
      "=== total 102200 match 34047\n",
      ">>> step 102300\n",
      "step 102200-102299, precision 0.431576, recall 0.693058, f_score 0.531919\n",
      "=== total 102300 match 34096\n",
      ">>> step 102400\n",
      "step 102300-102399, precision 0.431802, recall 0.693386, f_score 0.532188\n",
      "=== total 102400 match 34136\n",
      ">>> step 102500\n",
      "step 102400-102499, precision 0.431911, recall 0.693591, f_score 0.532331\n",
      "=== total 102500 match 34176\n",
      ">>> step 102600\n",
      "step 102500-102599, precision 0.431879, recall 0.693792, f_score 0.532365\n",
      "=== total 102600 match 34211\n",
      ">>> step 102700\n",
      "step 102600-102699, precision 0.431425, recall 0.693792, f_score 0.532020\n",
      "=== total 102700 match 34247\n",
      ">>> step 102800\n",
      "step 102700-102799, precision 0.430984, recall 0.693792, f_score 0.531685\n",
      "=== total 102800 match 34282\n",
      ">>> step 102900\n",
      "step 102800-102899, precision 0.430632, recall 0.693792, f_score 0.531417\n",
      "=== total 102900 match 34310\n",
      ">>> step 103000\n",
      "step 102900-102999, precision 0.430863, recall 0.694166, f_score 0.531702\n",
      "=== total 103000 match 34352\n",
      ">>> step 103100\n",
      "step 103000-103099, precision 0.430399, recall 0.694166, f_score 0.531349\n",
      "=== total 103100 match 34389\n",
      ">>> step 103200\n",
      "step 103100-103199, precision 0.430227, recall 0.694352, f_score 0.531272\n",
      "=== total 103200 match 34433\n",
      ">>> step 103300\n",
      "step 103200-103299, precision 0.430150, recall 0.694610, f_score 0.531289\n",
      "=== total 103300 match 34481\n",
      ">>> step 103400\n",
      "step 103300-103399, precision 0.429933, recall 0.694724, f_score 0.531157\n",
      "=== total 103400 match 34517\n",
      ">>> step 103500\n",
      "step 103400-103499, precision 0.429572, recall 0.694724, f_score 0.530882\n",
      "=== total 103500 match 34546\n",
      ">>> step 103600\n",
      "step 103500-103599, precision 0.430049, recall 0.695211, f_score 0.531388\n",
      "=== total 103600 match 34603\n",
      ">>> step 103700\n",
      "step 103600-103699, precision 0.429764, recall 0.695211, f_score 0.531170\n",
      "=== total 103700 match 34626\n",
      ">>> step 103800\n",
      "step 103700-103799, precision 0.429906, recall 0.695581, f_score 0.531387\n",
      "=== total 103800 match 34675\n",
      ">>> step 103900\n",
      "step 103800-103899, precision 0.430048, recall 0.695950, f_score 0.531603\n",
      "=== total 103900 match 34724\n",
      ">>> step 104000\n",
      "step 103900-103999, precision 0.429690, recall 0.695950, f_score 0.531329\n",
      "=== total 104000 match 34753\n",
      ">>> step 104100\n",
      "step 104000-104099, precision 0.430282, recall 0.696610, f_score 0.531975\n",
      "=== total 104100 match 34819\n",
      ">>> step 104200\n",
      "step 104100-104199, precision 0.429974, recall 0.696610, f_score 0.531739\n",
      "=== total 104200 match 34844\n",
      ">>> step 104300\n",
      "step 104200-104299, precision 0.429792, recall 0.696794, f_score 0.531653\n",
      "=== total 104300 match 34889\n",
      ">>> step 104400\n",
      "step 104300-104399, precision 0.429398, recall 0.696794, f_score 0.531351\n",
      "=== total 104400 match 34921\n",
      ">>> step 104500\n",
      "step 104400-104499, precision 0.429564, recall 0.697160, f_score 0.531585\n",
      "=== total 104500 match 34968\n",
      ">>> step 104600\n",
      "step 104500-104599, precision 0.430052, recall 0.697684, f_score 0.532111\n",
      "=== total 104600 match 35026\n",
      ">>> step 104700\n",
      "step 104600-104699, precision 0.430156, recall 0.698048, f_score 0.532296\n",
      "=== total 104700 match 35078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 104800\n",
      "step 104700-104799, precision 0.429739, recall 0.698048, f_score 0.531977\n",
      "=== total 104800 match 35112\n",
      ">>> step 104900\n",
      "step 104800-104899, precision 0.429409, recall 0.698048, f_score 0.531724\n",
      "=== total 104900 match 35139\n",
      ">>> step 105000\n",
      "step 104900-104999, precision 0.429018, recall 0.698048, f_score 0.531424\n",
      "=== total 105000 match 35171\n",
      ">>> step 105100\n",
      "step 105000-105099, precision 0.428653, recall 0.698048, f_score 0.531144\n",
      "=== total 105100 match 35201\n",
      ">>> step 105200\n",
      "step 105100-105199, precision 0.428782, recall 0.698410, f_score 0.531348\n",
      "=== total 105200 match 35251\n",
      ">>> step 105300\n",
      "step 105200-105299, precision 0.428418, recall 0.698410, f_score 0.531068\n",
      "=== total 105300 match 35281\n",
      ">>> step 105400\n",
      "step 105300-105399, precision 0.429016, recall 0.699060, f_score 0.531716\n",
      "=== total 105400 match 35346\n",
      ">>> step 105500\n",
      "step 105400-105499, precision 0.429217, recall 0.699217, f_score 0.531916\n",
      "=== total 105500 match 35383\n",
      ">>> step 105600\n",
      "step 105500-105599, precision 0.429305, recall 0.699522, f_score 0.532072\n",
      "=== total 105600 match 35427\n",
      ">>> step 105700\n",
      "step 105600-105699, precision 0.429522, recall 0.699821, f_score 0.532325\n",
      "=== total 105700 match 35465\n",
      ">>> step 105800\n",
      "step 105700-105799, precision 0.429750, recall 0.700119, f_score 0.532586\n",
      "=== total 105800 match 35502\n",
      ">>> step 105900\n",
      "step 105800-105899, precision 0.430407, recall 0.700806, f_score 0.533289\n",
      "=== total 105900 match 35564\n",
      ">>> step 106000\n",
      "step 105900-105999, precision 0.430986, recall 0.701306, f_score 0.533878\n",
      "=== total 106000 match 35623\n",
      ">>> step 106100\n",
      "step 106000-106099, precision 0.431159, recall 0.701661, f_score 0.534114\n",
      "=== total 106100 match 35669\n",
      ">>> step 106200\n",
      "step 106100-106199, precision 0.431264, recall 0.701618, f_score 0.534182\n",
      "=== total 106200 match 35702\n",
      ">>> step 106300\n",
      "step 106200-106299, precision 0.431354, recall 0.701657, f_score 0.534262\n",
      "=== total 106300 match 35734\n",
      ">>> step 106400\n",
      "step 106300-106399, precision 0.430968, recall 0.701657, f_score 0.533966\n",
      "=== total 106400 match 35766\n",
      ">>> step 106500\n",
      "step 106400-106499, precision 0.430619, recall 0.701657, f_score 0.533698\n",
      "=== total 106500 match 35795\n",
      ">>> step 106600\n",
      "step 106500-106599, precision 0.430198, recall 0.701657, f_score 0.533375\n",
      "=== total 106600 match 35830\n",
      ">>> step 106700\n",
      "step 106600-106699, precision 0.430299, recall 0.702010, f_score 0.533554\n",
      "=== total 106700 match 35882\n",
      ">>> step 106800\n",
      "step 106700-106799, precision 0.429880, recall 0.702010, f_score 0.533232\n",
      "=== total 106800 match 35917\n",
      ">>> step 106900\n",
      "step 106800-106899, precision 0.429398, recall 0.702077, f_score 0.532880\n",
      "=== total 106900 match 35969\n",
      ">>> step 107000\n",
      "step 106900-106999, precision 0.429361, recall 0.702225, f_score 0.532895\n",
      "=== total 107000 match 36014\n",
      ">>> step 107100\n",
      "step 107000-107099, precision 0.429039, recall 0.702225, f_score 0.532647\n",
      "=== total 107100 match 36041\n",
      ">>> step 107200\n",
      "step 107100-107199, precision 0.429042, recall 0.702412, f_score 0.532703\n",
      "=== total 107200 match 36106\n",
      ">>> step 107300\n",
      "step 107200-107299, precision 0.428793, recall 0.702502, f_score 0.532536\n",
      "=== total 107300 match 36148\n",
      ">>> step 107400\n",
      "step 107300-107399, precision 0.428907, recall 0.702852, f_score 0.532725\n",
      "=== total 107400 match 36199\n",
      ">>> step 107500\n",
      "step 107400-107499, precision 0.428938, recall 0.703201, f_score 0.532849\n",
      "=== total 107500 match 36257\n",
      ">>> step 107600\n",
      "step 107500-107599, precision 0.428524, recall 0.703201, f_score 0.532530\n",
      "=== total 107600 match 36292\n",
      ">>> step 107700\n",
      "step 107600-107699, precision 0.428799, recall 0.703633, f_score 0.532866\n",
      "=== total 107700 match 36355\n",
      ">>> step 107800\n",
      "step 107700-107799, precision 0.428595, recall 0.703781, f_score 0.532750\n",
      "=== total 107800 match 36398\n",
      ">>> step 107900\n",
      "step 107800-107899, precision 0.428575, recall 0.703954, f_score 0.532785\n",
      "=== total 107900 match 36430\n",
      ">>> step 108000\n",
      "step 107900-107999, precision 0.428317, recall 0.703954, f_score 0.532585\n",
      "=== total 108000 match 36452\n",
      ">>> step 108100\n",
      "step 108000-108099, precision 0.428407, recall 0.704301, f_score 0.532754\n",
      "=== total 108100 match 36505\n",
      ">>> step 108200\n",
      "step 108100-108199, precision 0.428415, recall 0.704726, f_score 0.532882\n",
      "=== total 108200 match 36579\n",
      ">>> step 108300\n",
      "step 108200-108299, precision 0.428540, recall 0.705071, f_score 0.533077\n",
      "=== total 108300 match 36629\n",
      ">>> step 108400\n",
      "step 108300-108399, precision 0.428271, recall 0.705071, f_score 0.532869\n",
      "=== total 108400 match 36652\n",
      ">>> step 108500\n",
      "step 108400-108499, precision 0.427979, recall 0.705071, f_score 0.532643\n",
      "=== total 108500 match 36677\n",
      ">>> step 108600\n",
      "step 108500-108599, precision 0.427595, recall 0.705071, f_score 0.532345\n",
      "=== total 108600 match 36710\n",
      ">>> step 108700\n",
      "step 108600-108699, precision 0.427565, recall 0.705323, f_score 0.532394\n",
      "=== total 108700 match 36757\n",
      ">>> step 108800\n",
      "step 108700-108799, precision 0.427360, recall 0.705415, f_score 0.532261\n",
      "=== total 108800 match 36791\n",
      ">>> step 108900\n",
      "step 108800-108899, precision 0.427331, recall 0.705453, f_score 0.532249\n",
      "=== total 108900 match 36845\n",
      ">>> step 109000\n",
      "step 108900-108999, precision 0.427236, recall 0.705732, f_score 0.532256\n",
      "=== total 109000 match 36914\n",
      ">>> step 109100\n",
      "step 109000-109099, precision 0.427486, recall 0.706195, f_score 0.532581\n",
      "=== total 109100 match 36986\n",
      ">>> step 109200\n",
      "step 109100-109199, precision 0.427047, recall 0.706195, f_score 0.532240\n",
      "=== total 109200 match 37024\n",
      ">>> step 109300\n",
      "step 109200-109299, precision 0.427435, recall 0.706538, f_score 0.532639\n",
      "=== total 109300 match 37091\n",
      ">>> step 109400\n",
      "step 109300-109399, precision 0.427741, recall 0.707012, f_score 0.533012\n",
      "=== total 109400 match 37172\n",
      ">>> step 109500\n",
      "step 109400-109499, precision 0.427919, recall 0.707455, f_score 0.533275\n",
      "=== total 109500 match 37236\n",
      ">>> step 109600\n",
      "step 109500-109599, precision 0.427886, recall 0.707766, f_score 0.533338\n",
      "=== total 109600 match 37295\n",
      ">>> step 109700\n",
      "step 109600-109699, precision 0.427718, recall 0.707816, f_score 0.533222\n",
      "=== total 109700 match 37347\n",
      ">>> step 109800\n",
      "step 109700-109799, precision 0.428079, recall 0.708379, f_score 0.533662\n",
      "=== total 109800 match 37423\n",
      ">>> step 109900\n",
      "step 109800-109899, precision 0.427690, recall 0.708379, f_score 0.533360\n",
      "=== total 109900 match 37457\n",
      ">>> step 110000\n",
      "step 109900-109999, precision 0.427360, recall 0.708379, f_score 0.533103\n",
      "=== total 110000 match 37486\n",
      ">>> step 110100\n",
      "step 110000-110099, precision 0.427223, recall 0.707972, f_score 0.532881\n",
      "=== total 110100 match 37498\n",
      ">>> step 110200\n",
      "step 110100-110199, precision 0.427120, recall 0.707972, f_score 0.532801\n",
      "=== total 110200 match 37507\n",
      ">>> step 110300\n",
      "step 110200-110299, precision 0.426984, recall 0.707566, f_score 0.532580\n",
      "=== total 110300 match 37519\n",
      ">>> step 110400\n",
      "step 110300-110399, precision 0.426870, recall 0.706785, f_score 0.532270\n",
      "=== total 110400 match 37529\n",
      ">>> step 110500\n",
      "step 110400-110499, precision 0.426745, recall 0.706287, f_score 0.532031\n",
      "=== total 110500 match 37540\n",
      ">>> step 110600\n",
      "step 110500-110599, precision 0.426665, recall 0.705665, f_score 0.531793\n",
      "=== total 110600 match 37547\n",
      ">>> step 110700\n",
      "step 110600-110699, precision 0.426540, recall 0.705199, f_score 0.531564\n",
      "=== total 110700 match 37558\n",
      ">>> step 110800\n",
      "step 110700-110799, precision 0.426415, recall 0.704455, f_score 0.531255\n",
      "=== total 110800 match 37569\n",
      ">>> step 110900\n",
      "step 110800-110899, precision 0.426291, recall 0.703774, f_score 0.530965\n",
      "=== total 110900 match 37580\n",
      ">>> step 111000\n",
      "step 110900-110999, precision 0.426143, recall 0.703588, f_score 0.530798\n",
      "=== total 111000 match 37593\n",
      ">>> step 111100\n",
      "step 111000-111099, precision 0.426041, recall 0.703588, f_score 0.530718\n",
      "=== total 111100 match 37602\n",
      ">>> step 111200\n",
      "step 111100-111199, precision 0.425939, recall 0.703187, f_score 0.530525\n",
      "=== total 111200 match 37611\n",
      ">>> step 111300\n",
      "step 111200-111299, precision 0.425860, recall 0.701647, f_score 0.530025\n",
      "=== total 111300 match 37618\n",
      ">>> step 111400\n",
      "step 111300-111399, precision 0.425781, recall 0.700879, f_score 0.529744\n",
      "=== total 111400 match 37625\n",
      ">>> step 111500\n",
      "step 111400-111499, precision 0.425634, recall 0.700114, f_score 0.529412\n",
      "=== total 111500 match 37638\n",
      ">>> step 111600\n",
      "step 111500-111599, precision 0.425588, recall 0.698953, f_score 0.529045\n",
      "=== total 111600 match 37642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 111700\n",
      "step 111600-111699, precision 0.425487, recall 0.698191, f_score 0.528748\n",
      "=== total 111700 match 37651\n",
      ">>> step 111800\n",
      "step 111700-111799, precision 0.425362, recall 0.698191, f_score 0.528652\n",
      "=== total 111800 match 37662\n",
      ">>> step 111900\n",
      "step 111800-111899, precision 0.425295, recall 0.698191, f_score 0.528599\n",
      "=== total 111900 match 37668\n",
      ">>> step 112000\n",
      "step 111900-111999, precision 0.425315, recall 0.697437, f_score 0.528398\n",
      "=== total 112000 match 37678\n",
      ">>> step 112100\n",
      "step 112000-112099, precision 0.425475, recall 0.697452, f_score 0.528527\n",
      "=== total 112100 match 37699\n",
      ">>> step 112200\n",
      "step 112100-112199, precision 0.425765, recall 0.697529, f_score 0.528773\n",
      "=== total 112200 match 37725\n",
      ">>> step 112300\n",
      "step 112200-112299, precision 0.425747, recall 0.696031, f_score 0.528327\n",
      "=== total 112300 match 37729\n",
      ">>> step 112400\n",
      "step 112300-112399, precision 0.425623, recall 0.695428, f_score 0.528058\n",
      "=== total 112400 match 37740\n",
      ">>> step 112500\n",
      "step 112400-112499, precision 0.425454, recall 0.694766, f_score 0.527737\n",
      "=== total 112500 match 37755\n",
      ">>> step 112600\n",
      "step 112500-112599, precision 0.425390, recall 0.694119, f_score 0.527501\n",
      "=== total 112600 match 37763\n",
      ">>> step 112700\n",
      "step 112600-112699, precision 0.425379, recall 0.692563, f_score 0.527043\n",
      "=== total 112700 match 37764\n",
      ">>> step 112800\n",
      "step 112700-112799, precision 0.425247, recall 0.691831, f_score 0.526730\n",
      "=== total 112800 match 37778\n",
      ">>> step 112900\n",
      "step 112800-112899, precision 0.425124, recall 0.691831, f_score 0.526635\n",
      "=== total 112900 match 37789\n",
      ">>> step 113000\n",
      "step 112900-112999, precision 0.425079, recall 0.691831, f_score 0.526600\n",
      "=== total 113000 match 37793\n",
      ">>> step 113100\n",
      "step 113000-113099, precision 0.424989, recall 0.690236, f_score 0.526069\n",
      "=== total 113100 match 37808\n",
      ">>> step 113200\n",
      "step 113100-113199, precision 0.425352, recall 0.690365, f_score 0.526385\n",
      "=== total 113200 match 37851\n",
      ">>> step 113300\n",
      "step 113200-113299, precision 0.425282, recall 0.689102, f_score 0.525963\n",
      "=== total 113300 match 37869\n",
      ">>> step 113400\n",
      "step 113300-113399, precision 0.425560, recall 0.688698, f_score 0.526058\n",
      "=== total 113400 match 37903\n",
      ">>> step 113500\n",
      "step 113400-113499, precision 0.425425, recall 0.686997, f_score 0.525459\n",
      "=== total 113500 match 37915\n",
      ">>> step 113600\n",
      "step 113500-113599, precision 0.425869, recall 0.687067, f_score 0.525818\n",
      "=== total 113600 match 37960\n",
      ">>> step 113700\n",
      "step 113600-113699, precision 0.425780, recall 0.687067, f_score 0.525749\n",
      "=== total 113700 match 37968\n",
      ">>> step 113800\n",
      "step 113700-113799, precision 0.425735, recall 0.687107, f_score 0.525727\n",
      "=== total 113800 match 37979\n",
      ">>> step 113900\n",
      "step 113800-113899, precision 0.426027, recall 0.687412, f_score 0.526039\n",
      "=== total 113900 match 38007\n",
      ">>> step 114000\n",
      "step 113900-113999, precision 0.426153, recall 0.687651, f_score 0.526205\n",
      "=== total 114000 match 38038\n",
      ">>> step 114100\n",
      "step 114000-114099, precision 0.426418, recall 0.687924, f_score 0.526486\n",
      "=== total 114100 match 38073\n",
      ">>> step 114200\n",
      "step 114100-114199, precision 0.427103, recall 0.688584, f_score 0.527202\n",
      "=== total 114200 match 38129\n",
      ">>> step 114300\n",
      "step 114200-114299, precision 0.427332, recall 0.688757, f_score 0.527427\n",
      "=== total 114300 match 38160\n",
      ">>> step 114400\n",
      "step 114300-114399, precision 0.427532, recall 0.689098, f_score 0.527680\n",
      "=== total 114400 match 38203\n",
      ">>> step 114500\n",
      "step 114400-114499, precision 0.428247, recall 0.689752, f_score 0.528416\n",
      "=== total 114500 match 38256\n",
      ">>> step 114600\n",
      "step 114500-114599, precision 0.428370, recall 0.689997, f_score 0.528582\n",
      "=== total 114600 match 38294\n",
      ">>> step 114700\n",
      "step 114600-114699, precision 0.428407, recall 0.690022, f_score 0.528617\n",
      "=== total 114700 match 38321\n",
      ">>> step 114800\n",
      "step 114700-114799, precision 0.429078, recall 0.690666, f_score 0.529316\n",
      "=== total 114800 match 38387\n",
      ">>> step 114900\n",
      "step 114800-114899, precision 0.429285, recall 0.691003, f_score 0.529573\n",
      "=== total 114900 match 38429\n",
      ">>> step 115000\n",
      "step 114900-114999, precision 0.429185, recall 0.691003, f_score 0.529497\n",
      "=== total 115000 match 38438\n",
      ">>> step 115100\n",
      "step 115000-115099, precision 0.429092, recall 0.691029, f_score 0.529433\n",
      "=== total 115100 match 38451\n",
      ">>> step 115200\n",
      "step 115100-115199, precision 0.429336, recall 0.691339, f_score 0.529711\n",
      "=== total 115200 match 38485\n",
      ">>> step 115300\n",
      "step 115200-115299, precision 0.429984, recall 0.691710, f_score 0.530312\n",
      "=== total 115300 match 38541\n",
      ">>> step 115400\n",
      "step 115300-115399, precision 0.430127, recall 0.691125, f_score 0.530250\n",
      "=== total 115400 match 38563\n",
      ">>> step 115500\n",
      "step 115400-115499, precision 0.430562, recall 0.691102, f_score 0.530573\n",
      "=== total 115500 match 38603\n",
      ">>> step 115600\n",
      "step 115500-115599, precision 0.430834, recall 0.690747, f_score 0.530675\n",
      "=== total 115600 match 38639\n",
      ">>> step 115700\n",
      "step 115600-115699, precision 0.431620, recall 0.691528, f_score 0.531501\n",
      "=== total 115700 match 38710\n",
      ">>> step 115800\n",
      "step 115700-115799, precision 0.432183, recall 0.691857, f_score 0.532025\n",
      "=== total 115800 match 38766\n",
      ">>> step 115900\n",
      "step 115800-115899, precision 0.432064, recall 0.691612, f_score 0.531863\n",
      "=== total 115900 match 38779\n",
      ">>> step 116000\n",
      "step 115900-115999, precision 0.431919, recall 0.691612, f_score 0.531753\n",
      "=== total 116000 match 38792\n",
      ">>> step 116100\n",
      "step 116000-116099, precision 0.432132, recall 0.691943, f_score 0.532012\n",
      "=== total 116100 match 38833\n",
      ">>> step 116200\n",
      "step 116100-116199, precision 0.432478, recall 0.692349, f_score 0.532394\n",
      "=== total 116200 match 38876\n",
      ">>> step 116300\n",
      "step 116200-116299, precision 0.432400, recall 0.692349, f_score 0.532335\n",
      "=== total 116300 match 38883\n",
      ">>> step 116400\n",
      "step 116300-116399, precision 0.432905, recall 0.692570, f_score 0.532783\n",
      "=== total 116400 match 38930\n",
      ">>> step 116500\n",
      "step 116400-116499, precision 0.433148, recall 0.692734, f_score 0.533015\n",
      "=== total 116500 match 38959\n",
      ">>> step 116600\n",
      "step 116500-116599, precision 0.433326, recall 0.692939, f_score 0.533211\n",
      "=== total 116600 match 38996\n",
      ">>> step 116700\n",
      "step 116600-116699, precision 0.433915, recall 0.693526, f_score 0.533831\n",
      "=== total 116700 match 39056\n",
      ">>> step 116800\n",
      "step 116700-116799, precision 0.434051, recall 0.693484, f_score 0.533921\n",
      "=== total 116800 match 39083\n",
      ">>> step 116900\n",
      "step 116800-116899, precision 0.434271, recall 0.693809, f_score 0.534184\n",
      "=== total 116900 match 39123\n",
      ">>> step 117000\n",
      "step 116900-116999, precision 0.434752, recall 0.694217, f_score 0.534668\n",
      "=== total 117000 match 39181\n",
      ">>> step 117100\n",
      "step 117000-117099, precision 0.434804, recall 0.694329, f_score 0.534741\n",
      "=== total 117100 match 39197\n",
      ">>> step 117200\n",
      "step 117100-117199, precision 0.434715, recall 0.694329, f_score 0.534674\n",
      "=== total 117200 match 39205\n",
      ">>> step 117300\n",
      "step 117200-117299, precision 0.434972, recall 0.694571, f_score 0.534940\n",
      "=== total 117300 match 39237\n",
      ">>> step 117400\n",
      "step 117300-117399, precision 0.435476, recall 0.694866, f_score 0.535409\n",
      "=== total 117400 match 39288\n",
      ">>> step 117500\n",
      "step 117400-117499, precision 0.435453, recall 0.693580, f_score 0.535009\n",
      "=== total 117500 match 39297\n",
      ">>> step 117600\n",
      "step 117500-117599, precision 0.435792, recall 0.693350, f_score 0.535196\n",
      "=== total 117600 match 39333\n",
      ">>> step 117700\n",
      "step 117600-117699, precision 0.435955, recall 0.692596, f_score 0.535094\n",
      "=== total 117700 match 39355\n",
      ">>> step 117800\n",
      "step 117700-117799, precision 0.436456, recall 0.692893, f_score 0.535561\n",
      "=== total 117800 match 39406\n",
      ">>> step 117900\n",
      "step 117800-117899, precision 0.436774, recall 0.692623, f_score 0.535719\n",
      "=== total 117900 match 39446\n",
      ">>> step 118000\n",
      "step 117900-117999, precision 0.436697, recall 0.691983, f_score 0.535470\n",
      "=== total 118000 match 39453\n",
      ">>> step 118100\n",
      "step 118000-118099, precision 0.436476, recall 0.691983, f_score 0.535303\n",
      "=== total 118100 match 39473\n",
      ">>> step 118200\n",
      "step 118100-118199, precision 0.436736, recall 0.692305, f_score 0.535595\n",
      "=== total 118200 match 39509\n",
      ">>> step 118300\n",
      "step 118200-118299, precision 0.436597, recall 0.691894, f_score 0.535368\n",
      "=== total 118300 match 39533\n",
      ">>> step 118400\n",
      "step 118300-118399, precision 0.436287, recall 0.691904, f_score 0.535137\n",
      "=== total 118400 match 39568\n",
      ">>> step 118500\n",
      "step 118400-118499, precision 0.436481, recall 0.691726, f_score 0.535230\n",
      "=== total 118500 match 39610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 118600\n",
      "step 118500-118599, precision 0.436278, recall 0.691087, f_score 0.534886\n",
      "=== total 118600 match 39633\n",
      ">>> step 118700\n",
      "step 118600-118699, precision 0.436377, recall 0.691328, f_score 0.535033\n",
      "=== total 118700 match 39679\n",
      ">>> step 118800\n",
      "step 118700-118799, precision 0.436997, recall 0.691784, f_score 0.535635\n",
      "=== total 118800 match 39728\n",
      ">>> step 118900\n",
      "step 118800-118899, precision 0.436788, recall 0.691425, f_score 0.535371\n",
      "=== total 118900 match 39747\n",
      ">>> step 119000\n",
      "step 118900-118999, precision 0.436633, recall 0.691108, f_score 0.535159\n",
      "=== total 119000 match 39784\n",
      ">>> step 119100\n",
      "step 119000-119099, precision 0.436891, recall 0.690854, f_score 0.535277\n",
      "=== total 119100 match 39836\n",
      ">>> step 119200\n",
      "step 119100-119199, precision 0.436694, recall 0.690827, f_score 0.535121\n",
      "=== total 119200 match 39854\n",
      ">>> step 119300\n",
      "step 119200-119299, precision 0.436488, recall 0.690876, f_score 0.534981\n",
      "=== total 119300 match 39882\n",
      ">>> step 119400\n",
      "step 119300-119399, precision 0.436601, recall 0.691146, f_score 0.535147\n",
      "=== total 119400 match 39922\n",
      ">>> step 119500\n",
      "step 119400-119499, precision 0.437200, recall 0.691772, f_score 0.535784\n",
      "=== total 119500 match 40000\n",
      ">>> step 119600\n",
      "step 119500-119599, precision 0.437570, recall 0.692207, f_score 0.536193\n",
      "=== total 119600 match 40053\n",
      ">>> step 119700\n",
      "step 119600-119699, precision 0.437966, recall 0.692814, f_score 0.536672\n",
      "=== total 119700 match 40131\n",
      ">>> step 119800\n",
      "step 119700-119799, precision 0.438504, recall 0.693430, f_score 0.537261\n",
      "=== total 119800 match 40198\n",
      ">>> step 119900\n",
      "step 119800-119899, precision 0.439077, recall 0.693838, f_score 0.537813\n",
      "=== total 119900 match 40264\n",
      ">>> step 120000\n",
      "step 119900-119999, precision 0.439539, recall 0.694299, f_score 0.538298\n",
      "=== total 120000 match 40340\n",
      ">>> step 120100\n",
      "step 120000-120099, precision 0.439404, recall 0.694343, f_score 0.538211\n",
      "=== total 120100 match 40366\n",
      ">>> step 120200\n",
      "step 120100-120199, precision 0.439198, recall 0.694343, f_score 0.538056\n",
      "=== total 120200 match 40385\n",
      ">>> step 120300\n",
      "step 120200-120299, precision 0.439222, recall 0.694570, f_score 0.538142\n",
      "=== total 120300 match 40426\n",
      ">>> step 120400\n",
      "step 120300-120399, precision 0.438701, recall 0.694570, f_score 0.537751\n",
      "=== total 120400 match 40474\n",
      ">>> step 120500\n",
      "step 120400-120499, precision 0.438438, recall 0.694726, f_score 0.537599\n",
      "=== total 120500 match 40528\n",
      ">>> step 120600\n",
      "step 120500-120599, precision 0.438398, recall 0.694952, f_score 0.537637\n",
      "=== total 120600 match 40575\n",
      ">>> step 120700\n",
      "step 120600-120699, precision 0.437891, recall 0.694952, f_score 0.537256\n",
      "=== total 120700 match 40622\n",
      ">>> step 120800\n",
      "step 120700-120799, precision 0.437737, recall 0.695107, f_score 0.537186\n",
      "=== total 120800 match 40666\n",
      ">>> step 120900\n",
      "step 120800-120899, precision 0.437629, recall 0.695107, f_score 0.537105\n",
      "=== total 120900 match 40676\n",
      ">>> step 121000\n",
      "step 120900-120999, precision 0.437411, recall 0.695262, f_score 0.536987\n",
      "=== total 121000 match 40726\n",
      ">>> step 121100\n",
      "step 121000-121099, precision 0.437164, recall 0.695262, f_score 0.536801\n",
      "=== total 121100 match 40749\n",
      ">>> step 121200\n",
      "step 121100-121199, precision 0.437138, recall 0.694987, f_score 0.536699\n",
      "=== total 121200 match 40756\n",
      ">>> step 121300\n",
      "step 121200-121299, precision 0.436774, recall 0.694987, f_score 0.536425\n",
      "=== total 121300 match 40790\n",
      ">>> step 121400\n",
      "step 121300-121399, precision 0.436431, recall 0.694987, f_score 0.536166\n",
      "=== total 121400 match 40822\n",
      ">>> step 121500\n",
      "step 121400-121499, precision 0.436354, recall 0.695142, f_score 0.536154\n",
      "=== total 121500 match 40859\n",
      ">>> step 121600\n",
      "step 121500-121599, precision 0.436253, recall 0.695368, f_score 0.536145\n",
      "=== total 121600 match 40912\n",
      ">>> step 121700\n",
      "step 121600-121699, precision 0.436115, recall 0.695288, f_score 0.536017\n",
      "=== total 121700 match 40941\n",
      ">>> step 121800\n",
      "step 121700-121799, precision 0.435975, recall 0.695442, f_score 0.535957\n",
      "=== total 121800 match 40984\n",
      ">>> step 121900\n",
      "step 121800-121899, precision 0.436032, recall 0.695738, f_score 0.536088\n",
      "=== total 121900 match 41036\n",
      ">>> step 122000\n",
      "step 121900-121999, precision 0.435575, recall 0.695738, f_score 0.535743\n",
      "=== total 122000 match 41079\n",
      ">>> step 122100\n",
      "step 122000-122099, precision 0.435236, recall 0.695738, f_score 0.535486\n",
      "=== total 122100 match 41111\n",
      ">>> step 122200\n",
      "step 122100-122199, precision 0.434972, recall 0.695738, f_score 0.535286\n",
      "=== total 122200 match 41136\n",
      ">>> step 122300\n",
      "step 122200-122299, precision 0.434760, recall 0.695738, f_score 0.535126\n",
      "=== total 122300 match 41156\n",
      ">>> step 122400\n",
      "step 122300-122399, precision 0.434549, recall 0.695738, f_score 0.534966\n",
      "=== total 122400 match 41176\n",
      ">>> step 122500\n",
      "step 122400-122499, precision 0.434349, recall 0.695738, f_score 0.534814\n",
      "=== total 122500 match 41195\n",
      ">>> step 122600\n",
      "step 122500-122599, precision 0.434145, recall 0.695853, f_score 0.534693\n",
      "=== total 122600 match 41242\n",
      ">>> step 122700\n",
      "step 122600-122699, precision 0.433857, recall 0.695742, f_score 0.534442\n",
      "=== total 122700 match 41274\n",
      ">>> step 122800\n",
      "step 122700-122799, precision 0.433838, recall 0.695907, f_score 0.534477\n",
      "=== total 122800 match 41308\n",
      ">>> step 122900\n",
      "step 122800-122899, precision 0.433687, recall 0.695966, f_score 0.534379\n",
      "=== total 122900 match 41334\n",
      ">>> step 123000\n",
      "step 122900-122999, precision 0.433452, recall 0.695801, f_score 0.534152\n",
      "=== total 123000 match 41361\n",
      ">>> step 123100\n",
      "step 123000-123099, precision 0.433399, recall 0.695450, f_score 0.534008\n",
      "=== total 123100 match 41366\n",
      ">>> step 123200\n",
      "step 123100-123199, precision 0.433232, recall 0.695603, f_score 0.533927\n",
      "=== total 123200 match 41412\n",
      ">>> step 123300\n",
      "step 123200-123299, precision 0.433225, recall 0.695768, f_score 0.533970\n",
      "=== total 123300 match 41445\n",
      ">>> step 123400\n",
      "step 123300-123399, precision 0.433005, recall 0.695768, f_score 0.533803\n",
      "=== total 123400 match 41466\n",
      ">>> step 123500\n",
      "step 123400-123499, precision 0.432901, recall 0.695768, f_score 0.533724\n",
      "=== total 123500 match 41476\n",
      ">>> step 123600\n",
      "step 123500-123599, precision 0.433028, recall 0.695735, f_score 0.533810\n",
      "=== total 123600 match 41510\n",
      ">>> step 123700\n",
      "step 123600-123699, precision 0.432805, recall 0.695570, f_score 0.533593\n",
      "=== total 123700 match 41536\n",
      ">>> step 123800\n",
      "step 123700-123799, precision 0.432737, recall 0.695503, f_score 0.533521\n",
      "=== total 123800 match 41561\n",
      ">>> step 123900\n",
      "step 123800-123899, precision 0.432793, recall 0.695023, f_score 0.533422\n",
      "=== total 123900 match 41588\n",
      ">>> step 124000\n",
      "step 123900-123999, precision 0.432889, recall 0.695258, f_score 0.533564\n",
      "=== total 124000 match 41625\n",
      ">>> step 124100\n",
      "step 124000-124099, precision 0.432733, recall 0.695258, f_score 0.533446\n",
      "=== total 124100 match 41640\n",
      ">>> step 124200\n",
      "step 124100-124199, precision 0.432442, recall 0.695258, f_score 0.533225\n",
      "=== total 124200 match 41668\n",
      ">>> step 124300\n",
      "step 124200-124299, precision 0.432214, recall 0.695258, f_score 0.533051\n",
      "=== total 124300 match 41690\n",
      ">>> step 124400\n",
      "step 124300-124399, precision 0.431976, recall 0.695258, f_score 0.532870\n",
      "=== total 124400 match 41713\n",
      ">>> step 124500\n",
      "step 124400-124499, precision 0.431727, recall 0.695258, f_score 0.532681\n",
      "=== total 124500 match 41737\n",
      ">>> step 124600\n",
      "step 124500-124599, precision 0.431843, recall 0.695563, f_score 0.532859\n",
      "=== total 124600 match 41786\n",
      ">>> step 124700\n",
      "step 124600-124699, precision 0.431379, recall 0.695563, f_score 0.532505\n",
      "=== total 124700 match 41831\n",
      ">>> step 124800\n",
      "step 124700-124799, precision 0.431770, recall 0.696079, f_score 0.532954\n",
      "=== total 124800 match 41895\n",
      ">>> step 124900\n",
      "step 124800-124899, precision 0.431854, recall 0.696383, f_score 0.533108\n",
      "=== total 124900 match 41947\n",
      ">>> step 125000\n",
      "step 124900-124999, precision 0.431890, recall 0.696225, f_score 0.533088\n",
      "=== total 125000 match 41976\n",
      ">>> step 125100\n",
      "step 125000-125099, precision 0.431766, recall 0.696225, f_score 0.532994\n",
      "=== total 125100 match 41988\n",
      ">>> step 125200\n",
      "step 125100-125199, precision 0.431728, recall 0.695569, f_score 0.532773\n",
      "=== total 125200 match 41994\n",
      ">>> step 125300\n",
      "step 125200-125299, precision 0.431644, recall 0.695682, f_score 0.532742\n",
      "=== total 125300 match 42030\n",
      ">>> step 125400\n",
      "step 125300-125399, precision 0.431387, recall 0.695682, f_score 0.532547\n",
      "=== total 125400 match 42055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 125500\n",
      "step 125400-125499, precision 0.431544, recall 0.695985, f_score 0.532755\n",
      "=== total 125500 match 42100\n",
      ">>> step 125600\n",
      "step 125500-125599, precision 0.431499, recall 0.695484, f_score 0.532573\n",
      "=== total 125600 match 42116\n",
      ">>> step 125700\n",
      "step 125600-125699, precision 0.431634, recall 0.695787, f_score 0.532765\n",
      "=== total 125700 match 42163\n",
      ">>> step 125800\n",
      "step 125700-125799, precision 0.431266, recall 0.695787, f_score 0.532485\n",
      "=== total 125800 match 42199\n",
      ">>> step 125900\n",
      "step 125800-125899, precision 0.431011, recall 0.695787, f_score 0.532290\n",
      "=== total 125900 match 42224\n",
      ">>> step 126000\n",
      "step 125900-125999, precision 0.431155, recall 0.695548, f_score 0.532330\n",
      "=== total 126000 match 42247\n",
      ">>> step 126100\n",
      "step 126000-126099, precision 0.431572, recall 0.696128, f_score 0.532818\n",
      "=== total 126100 match 42322\n",
      ">>> step 126200\n",
      "step 126100-126199, precision 0.431501, recall 0.696128, f_score 0.532764\n",
      "=== total 126200 match 42329\n",
      ">>> step 126300\n",
      "step 126200-126299, precision 0.431979, recall 0.696477, f_score 0.533230\n",
      "=== total 126300 match 42384\n",
      ">>> step 126400\n",
      "step 126300-126399, precision 0.432383, recall 0.697054, f_score 0.533707\n",
      "=== total 126400 match 42460\n",
      ">>> step 126500\n",
      "step 126400-126499, precision 0.432301, recall 0.696480, f_score 0.533477\n",
      "=== total 126500 match 42475\n",
      ">>> step 126600\n",
      "step 126500-126599, precision 0.432786, recall 0.697055, f_score 0.534014\n",
      "=== total 126600 match 42543\n",
      ">>> step 126700\n",
      "step 126600-126699, precision 0.432501, recall 0.697055, f_score 0.533797\n",
      "=== total 126700 match 42571\n",
      ">>> step 126800\n",
      "step 126700-126799, precision 0.432217, recall 0.697055, f_score 0.533581\n",
      "=== total 126800 match 42599\n",
      ">>> step 126900\n",
      "step 126800-126899, precision 0.431994, recall 0.697055, f_score 0.533411\n",
      "=== total 126900 match 42621\n",
      ">>> step 127000\n",
      "step 126900-126999, precision 0.431761, recall 0.697055, f_score 0.533233\n",
      "=== total 127000 match 42644\n",
      ">>> step 127100\n",
      "step 127000-127099, precision 0.431617, recall 0.697008, f_score 0.533110\n",
      "=== total 127100 match 42686\n",
      ">>> step 127200\n",
      "step 127100-127199, precision 0.431464, recall 0.697005, f_score 0.532992\n",
      "=== total 127200 match 42722\n",
      ">>> step 127300\n",
      "step 127200-127299, precision 0.431744, recall 0.696973, f_score 0.533197\n",
      "=== total 127300 match 42773\n",
      ">>> step 127400\n",
      "step 127300-127399, precision 0.431767, recall 0.697270, f_score 0.533301\n",
      "=== total 127400 match 42831\n",
      ">>> step 127500\n",
      "step 127400-127499, precision 0.431364, recall 0.697270, f_score 0.532993\n",
      "=== total 127500 match 42871\n",
      ">>> step 127600\n",
      "step 127500-127599, precision 0.431437, recall 0.697567, f_score 0.533136\n",
      "=== total 127600 match 42924\n",
      ">>> step 127700\n",
      "step 127600-127699, precision 0.431168, recall 0.697646, f_score 0.532954\n",
      "=== total 127700 match 42967\n",
      ">>> step 127800\n",
      "step 127700-127799, precision 0.430989, recall 0.697637, f_score 0.532814\n",
      "=== total 127800 match 43015\n",
      ">>> step 127900\n",
      "step 127800-127899, precision 0.430940, recall 0.697669, f_score 0.532786\n",
      "=== total 127900 match 43064\n",
      ">>> step 128000\n",
      "step 127900-127999, precision 0.431023, recall 0.697964, f_score 0.532936\n",
      "=== total 128000 match 43116\n",
      ">>> step 128100\n",
      "step 128000-128099, precision 0.430997, recall 0.698259, f_score 0.533001\n",
      "=== total 128100 match 43179\n",
      ">>> step 128200\n",
      "step 128100-128199, precision 0.430697, recall 0.698259, f_score 0.532772\n",
      "=== total 128200 match 43209\n",
      ">>> step 128300\n",
      "step 128200-128299, precision 0.430339, recall 0.698259, f_score 0.532498\n",
      "=== total 128300 match 43245\n",
      ">>> step 128400\n",
      "step 128300-128399, precision 0.430492, recall 0.698553, f_score 0.532701\n",
      "=== total 128400 match 43290\n",
      ">>> step 128500\n",
      "step 128400-128499, precision 0.430951, recall 0.699005, f_score 0.533183\n",
      "=== total 128500 match 43353\n",
      ">>> step 128600\n",
      "step 128500-128599, precision 0.430742, recall 0.699005, f_score 0.533023\n",
      "=== total 128600 match 43374\n",
      ">>> step 128700\n",
      "step 128600-128699, precision 0.431003, recall 0.699365, f_score 0.533328\n",
      "=== total 128700 match 43422\n",
      ">>> step 128800\n",
      "step 128700-128799, precision 0.431469, recall 0.699925, f_score 0.533847\n",
      "=== total 128800 match 43491\n",
      ">>> step 128900\n",
      "step 128800-128899, precision 0.431531, recall 0.700216, f_score 0.533979\n",
      "=== total 128900 match 43545\n",
      ">>> step 129000\n",
      "step 128900-128999, precision 0.431979, recall 0.700588, f_score 0.534430\n",
      "=== total 129000 match 43604\n",
      ">>> step 129100\n",
      "step 129000-129099, precision 0.431751, recall 0.700588, f_score 0.534256\n",
      "=== total 129100 match 43627\n",
      ">>> step 129200\n",
      "step 129100-129199, precision 0.431494, recall 0.700588, f_score 0.534059\n",
      "=== total 129200 match 43653\n",
      ">>> step 129300\n",
      "step 129200-129299, precision 0.431217, recall 0.700588, f_score 0.533847\n",
      "=== total 129300 match 43681\n",
      ">>> step 129400\n",
      "step 129300-129399, precision 0.430951, recall 0.700588, f_score 0.533643\n",
      "=== total 129400 match 43708\n",
      ">>> step 129500\n",
      "step 129400-129499, precision 0.430861, recall 0.700695, f_score 0.533605\n",
      "=== total 129500 match 43745\n",
      ">>> step 129600\n",
      "step 129500-129599, precision 0.430843, recall 0.700840, f_score 0.533633\n",
      "=== total 129600 match 43777\n",
      ">>> step 129700\n",
      "step 129600-129699, precision 0.430636, recall 0.700840, f_score 0.533475\n",
      "=== total 129700 match 43798\n",
      ">>> step 129800\n",
      "step 129700-129799, precision 0.431080, recall 0.701328, f_score 0.533957\n",
      "=== total 129800 match 43855\n",
      ">>> step 129900\n",
      "step 129800-129899, precision 0.431253, recall 0.701542, f_score 0.534152\n",
      "=== total 129900 match 43893\n",
      ">>> step 130000\n",
      "step 129900-129999, precision 0.431058, recall 0.701685, f_score 0.534044\n",
      "=== total 130000 match 43943\n",
      ">>> step 130100\n",
      "step 130000-130099, precision 0.431251, recall 0.701925, f_score 0.534261\n",
      "=== total 130100 match 43979\n",
      ">>> step 130200\n",
      "step 130100-130199, precision 0.431058, recall 0.701936, f_score 0.534116\n",
      "=== total 130200 match 44001\n",
      ">>> step 130300\n",
      "step 130200-130299, precision 0.431054, recall 0.701783, f_score 0.534069\n",
      "=== total 130300 match 44013\n",
      ">>> step 130400\n",
      "step 130300-130399, precision 0.430893, recall 0.701904, f_score 0.533980\n",
      "=== total 130400 match 44055\n",
      ">>> step 130500\n",
      "step 130400-130499, precision 0.431088, recall 0.702187, f_score 0.534212\n",
      "=== total 130500 match 44100\n",
      ">>> step 130600\n",
      "step 130500-130599, precision 0.431290, recall 0.702354, f_score 0.534415\n",
      "=== total 130600 match 44142\n",
      ">>> step 130700\n",
      "step 130600-130699, precision 0.431172, recall 0.702420, f_score 0.534343\n",
      "=== total 130700 match 44168\n",
      ">>> step 130800\n",
      "step 130700-130799, precision 0.430967, recall 0.702420, f_score 0.534186\n",
      "=== total 130800 match 44189\n",
      ">>> step 130900\n",
      "step 130800-130899, precision 0.430984, recall 0.702650, f_score 0.534266\n",
      "=== total 130900 match 44236\n",
      ">>> step 131000\n",
      "step 130900-130999, precision 0.431175, recall 0.702978, f_score 0.534507\n",
      "=== total 131000 match 44286\n",
      ">>> step 131100\n",
      "step 131000-131099, precision 0.431301, recall 0.703251, f_score 0.534683\n",
      "=== total 131100 match 44331\n",
      ">>> step 131200\n",
      "step 131100-131199, precision 0.431642, recall 0.703666, f_score 0.535065\n",
      "=== total 131200 match 44384\n",
      ">>> step 131300\n",
      "step 131200-131299, precision 0.431864, recall 0.704003, f_score 0.535333\n",
      "=== total 131300 match 44433\n",
      ">>> step 131400\n",
      "step 131300-131399, precision 0.432095, recall 0.704339, f_score 0.535607\n",
      "=== total 131400 match 44481\n",
      ">>> step 131500\n",
      "step 131400-131499, precision 0.432370, recall 0.704696, f_score 0.535922\n",
      "=== total 131500 match 44529\n",
      ">>> step 131600\n",
      "step 131500-131599, precision 0.432686, recall 0.705095, f_score 0.536280\n",
      "=== total 131600 match 44582\n",
      ">>> step 131700\n",
      "step 131600-131699, precision 0.432414, recall 0.705095, f_score 0.536072\n",
      "=== total 131700 match 44610\n",
      ">>> step 131800\n",
      "step 131700-131799, precision 0.432172, recall 0.705095, f_score 0.535885\n",
      "=== total 131800 match 44635\n",
      ">>> step 131900\n",
      "step 131800-131899, precision 0.431988, recall 0.705095, f_score 0.535744\n",
      "=== total 131900 match 44654\n",
      ">>> step 132000\n",
      "step 131900-131999, precision 0.431833, recall 0.705095, f_score 0.535625\n",
      "=== total 132000 match 44670\n",
      ">>> step 132100\n",
      "step 132000-132099, precision 0.431979, recall 0.705440, f_score 0.535837\n",
      "=== total 132100 match 44729\n",
      ">>> step 132200\n",
      "step 132100-132199, precision 0.431759, recall 0.705515, f_score 0.535689\n",
      "=== total 132200 match 44768\n",
      ">>> step 132300\n",
      "step 132200-132299, precision 0.432121, recall 0.705977, f_score 0.536101\n",
      "=== total 132300 match 44830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 132400\n",
      "step 132300-132399, precision 0.432161, recall 0.706255, f_score 0.536212\n",
      "=== total 132400 match 44886\n",
      ">>> step 132500\n",
      "step 132400-132499, precision 0.432106, recall 0.706501, f_score 0.536240\n",
      "=== total 132500 match 44945\n",
      ">>> step 132600\n",
      "step 132500-132599, precision 0.431856, recall 0.706533, f_score 0.536056\n",
      "=== total 132600 match 44978\n",
      ">>> step 132700\n",
      "step 132600-132699, precision 0.431884, recall 0.706665, f_score 0.536116\n",
      "=== total 132700 match 45026\n",
      ">>> step 132800\n",
      "step 132700-132799, precision 0.431568, recall 0.706665, f_score 0.535872\n",
      "=== total 132800 match 45059\n",
      ">>> step 132900\n",
      "step 132800-132899, precision 0.431617, recall 0.706974, f_score 0.535999\n",
      "=== total 132900 match 45121\n",
      ">>> step 133000\n",
      "step 132900-132999, precision 0.431682, recall 0.707303, f_score 0.536144\n",
      "=== total 133000 match 45186\n",
      ">>> step 133100\n",
      "step 133000-133099, precision 0.431660, recall 0.707494, f_score 0.536182\n",
      "=== total 133100 match 45230\n",
      ">>> step 133200\n",
      "step 133100-133199, precision 0.431517, recall 0.707494, f_score 0.536072\n",
      "=== total 133200 match 45245\n",
      ">>> step 133300\n",
      "step 133200-133299, precision 0.431361, recall 0.707547, f_score 0.535966\n",
      "=== total 133300 match 45273\n",
      ">>> step 133400\n",
      "step 133300-133399, precision 0.431591, recall 0.707885, f_score 0.536241\n",
      "=== total 133400 match 45323\n",
      ">>> step 133500\n",
      "step 133400-133499, precision 0.431984, recall 0.708297, f_score 0.536663\n",
      "=== total 133500 match 45372\n",
      ">>> step 133600\n",
      "step 133500-133599, precision 0.431968, recall 0.708508, f_score 0.536711\n",
      "=== total 133600 match 45420\n",
      ">>> step 133700\n",
      "step 133600-133699, precision 0.432276, recall 0.708886, f_score 0.537057\n",
      "=== total 133700 match 45471\n",
      ">>> step 133800\n",
      "step 133700-133799, precision 0.432352, recall 0.709190, f_score 0.537203\n",
      "=== total 133800 match 45530\n",
      ">>> step 133900\n",
      "step 133800-133899, precision 0.432479, recall 0.709536, f_score 0.537400\n",
      "=== total 133900 match 45593\n",
      ">>> step 134000\n",
      "step 133900-133999, precision 0.432843, recall 0.709953, f_score 0.537801\n",
      "=== total 134000 match 45647\n",
      ">>> step 134100\n",
      "step 134000-134099, precision 0.432625, recall 0.709953, f_score 0.537633\n",
      "=== total 134100 match 45670\n",
      ">>> step 134200\n",
      "step 134100-134199, precision 0.432464, recall 0.709953, f_score 0.537508\n",
      "=== total 134200 match 45687\n",
      ">>> step 134300\n",
      "step 134200-134299, precision 0.432455, recall 0.709953, f_score 0.537501\n",
      "=== total 134300 match 45688\n",
      ">>> step 134400\n",
      "step 134300-134399, precision 0.432389, recall 0.709189, f_score 0.537231\n",
      "=== total 134400 match 45695\n",
      ">>> step 134500\n",
      "step 134400-134499, precision 0.432237, recall 0.708655, f_score 0.536961\n",
      "=== total 134500 match 45711\n",
      ">>> step 134600\n",
      "step 134500-134599, precision 0.432237, recall 0.708324, f_score 0.536866\n",
      "=== total 134600 match 45711\n",
      ">>> step 134700\n",
      "step 134600-134699, precision 0.432228, recall 0.707893, f_score 0.536734\n",
      "=== total 134700 match 45712\n",
      ">>> step 134800\n",
      "step 134700-134799, precision 0.432152, recall 0.707158, f_score 0.536465\n",
      "=== total 134800 match 45720\n",
      ">>> step 134900\n",
      "step 134800-134899, precision 0.432114, recall 0.706678, f_score 0.536297\n",
      "=== total 134900 match 45724\n",
      ">>> step 135000\n",
      "step 134900-134999, precision 0.432114, recall 0.705845, f_score 0.536057\n",
      "=== total 135000 match 45724\n",
      ">>> step 135100\n",
      "step 135000-135099, precision 0.432105, recall 0.705391, f_score 0.535919\n",
      "=== total 135100 match 45725\n",
      ">>> step 135200\n",
      "step 135100-135199, precision 0.432105, recall 0.704310, f_score 0.535607\n",
      "=== total 135200 match 45725\n",
      ">>> step 135300\n",
      "step 135200-135299, precision 0.432058, recall 0.704134, f_score 0.535520\n",
      "=== total 135300 match 45730\n",
      ">>> step 135400\n",
      "step 135300-135399, precision 0.432058, recall 0.704134, f_score 0.535520\n",
      "=== total 135400 match 45730\n",
      ">>> step 135500\n",
      "step 135400-135499, precision 0.432262, recall 0.704230, f_score 0.535704\n",
      "=== total 135500 match 45757\n",
      ">>> step 135600\n",
      "step 135500-135599, precision 0.432572, recall 0.703867, f_score 0.535837\n",
      "=== total 135600 match 45782\n",
      ">>> step 135700\n",
      "step 135600-135699, precision 0.432595, recall 0.703501, f_score 0.535749\n",
      "=== total 135700 match 45798\n",
      ">>> step 135800\n",
      "step 135700-135799, precision 0.432707, recall 0.703172, f_score 0.535739\n",
      "=== total 135800 match 45807\n",
      ">>> step 135900\n",
      "step 135800-135899, precision 0.432690, recall 0.702043, f_score 0.535398\n",
      "=== total 135900 match 45818\n",
      ">>> step 136000\n",
      "step 135900-135999, precision 0.432730, recall 0.701571, f_score 0.535291\n",
      "=== total 136000 match 45830\n",
      ">>> step 136100\n",
      "step 136000-136099, precision 0.432733, recall 0.701507, f_score 0.535275\n",
      "=== total 136100 match 45832\n",
      ">>> step 136200\n",
      "step 136100-136199, precision 0.433116, recall 0.701264, f_score 0.535497\n",
      "=== total 136200 match 45863\n",
      ">>> step 136300\n",
      "step 136200-136299, precision 0.433375, recall 0.701188, f_score 0.535673\n",
      "=== total 136300 match 45884\n",
      ">>> step 136400\n",
      "step 136300-136399, precision 0.433844, recall 0.700971, f_score 0.535968\n",
      "=== total 136400 match 45922\n",
      ">>> step 136500\n",
      "step 136400-136499, precision 0.434056, recall 0.700872, f_score 0.536100\n",
      "=== total 136500 match 45948\n",
      ">>> step 136600\n",
      "step 136500-136599, precision 0.434292, recall 0.700326, f_score 0.536120\n",
      "=== total 136600 match 45976\n",
      ">>> step 136700\n",
      "step 136600-136699, precision 0.434403, recall 0.700126, f_score 0.536146\n",
      "=== total 136700 match 45985\n",
      ">>> step 136800\n",
      "step 136700-136799, precision 0.434393, recall 0.700126, f_score 0.536139\n",
      "=== total 136800 match 45986\n",
      ">>> step 136900\n",
      "step 136800-136899, precision 0.434393, recall 0.699489, f_score 0.535952\n",
      "=== total 136900 match 45986\n",
      ">>> step 137000\n",
      "step 136900-136999, precision 0.434393, recall 0.698413, f_score 0.535636\n",
      "=== total 137000 match 45986\n",
      ">>> step 137100\n",
      "step 137000-137099, precision 0.434393, recall 0.697291, f_score 0.535306\n",
      "=== total 137100 match 45986\n",
      ">>> step 137200\n",
      "step 137100-137199, precision 0.434393, recall 0.696659, f_score 0.535119\n",
      "=== total 137200 match 45986\n",
      ">>> step 137300\n",
      "step 137200-137299, precision 0.434393, recall 0.696198, f_score 0.534983\n",
      "=== total 137300 match 45986\n",
      ">>> step 137400\n",
      "step 137300-137399, precision 0.434384, recall 0.695567, f_score 0.534790\n",
      "=== total 137400 match 45987\n",
      ">>> step 137500\n",
      "step 137400-137499, precision 0.434384, recall 0.694552, f_score 0.534489\n",
      "=== total 137500 match 45987\n",
      ">>> step 137600\n",
      "step 137500-137599, precision 0.434384, recall 0.694310, f_score 0.534418\n",
      "=== total 137600 match 45987\n",
      ">>> step 137700\n",
      "step 137600-137699, precision 0.434384, recall 0.693539, f_score 0.534189\n",
      "=== total 137700 match 45987\n",
      ">>> step 137800\n",
      "step 137700-137799, precision 0.434384, recall 0.692337, f_score 0.533832\n",
      "=== total 137800 match 45987\n",
      ">>> step 137900\n",
      "step 137800-137899, precision 0.434384, recall 0.691139, f_score 0.533476\n",
      "=== total 137900 match 45987\n",
      ">>> step 138000\n",
      "step 137900-137999, precision 0.434384, recall 0.689946, f_score 0.533120\n",
      "=== total 138000 match 45987\n",
      ">>> step 138100\n",
      "step 138000-138099, precision 0.434384, recall 0.688709, f_score 0.532750\n",
      "=== total 138100 match 45987\n",
      ">>> step 138200\n",
      "step 138100-138199, precision 0.434365, recall 0.688139, f_score 0.532566\n",
      "=== total 138200 match 45989\n",
      ">>> step 138300\n",
      "step 138200-138299, precision 0.434346, recall 0.688139, f_score 0.532551\n",
      "=== total 138300 match 45991\n",
      ">>> step 138400\n",
      "step 138300-138399, precision 0.434346, recall 0.687784, f_score 0.532445\n",
      "=== total 138400 match 45991\n",
      ">>> step 138500\n",
      "step 138400-138499, precision 0.434358, recall 0.687558, f_score 0.532386\n",
      "=== total 138500 match 45992\n",
      ">>> step 138600\n",
      "step 138500-138599, precision 0.434370, recall 0.686364, f_score 0.532037\n",
      "=== total 138600 match 45993\n",
      ">>> step 138700\n",
      "step 138600-138699, precision 0.434459, recall 0.685603, f_score 0.531875\n",
      "=== total 138700 match 46002\n",
      ">>> step 138800\n",
      "step 138700-138799, precision 0.434779, recall 0.685883, f_score 0.532199\n",
      "=== total 138800 match 46028\n",
      ">>> step 138900\n",
      "step 138800-138899, precision 0.434779, recall 0.685577, f_score 0.532107\n",
      "=== total 138900 match 46028\n",
      ">>> step 139000\n",
      "step 138900-138999, precision 0.434779, recall 0.684920, f_score 0.531909\n",
      "=== total 139000 match 46028\n",
      ">>> step 139100\n",
      "step 139000-139099, precision 0.434954, recall 0.684263, f_score 0.531841\n",
      "=== total 139100 match 46044\n",
      ">>> step 139200\n",
      "step 139100-139199, precision 0.434954, recall 0.684263, f_score 0.531841\n",
      "=== total 139200 match 46044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 139300\n",
      "step 139200-139299, precision 0.435248, recall 0.684171, f_score 0.532033\n",
      "=== total 139300 match 46068\n",
      ">>> step 139400\n",
      "step 139300-139399, precision 0.435260, recall 0.683203, f_score 0.531749\n",
      "=== total 139400 match 46069\n",
      ">>> step 139500\n",
      "step 139400-139499, precision 0.435395, recall 0.682415, f_score 0.531611\n",
      "=== total 139500 match 46080\n",
      ">>> step 139600\n",
      "step 139500-139599, precision 0.435630, recall 0.681635, f_score 0.531549\n",
      "=== total 139600 match 46101\n",
      ">>> step 139700\n",
      "step 139600-139699, precision 0.435707, recall 0.680440, f_score 0.531243\n",
      "=== total 139700 match 46109\n",
      ">>> step 139800\n",
      "step 139700-139799, precision 0.435707, recall 0.680417, f_score 0.531236\n",
      "=== total 139800 match 46109\n",
      ">>> step 139900\n",
      "step 139800-139899, precision 0.435707, recall 0.680417, f_score 0.531236\n",
      "=== total 139900 match 46109\n",
      ">>> step 140000\n",
      "step 139900-139999, precision 0.435707, recall 0.680417, f_score 0.531236\n",
      "=== total 140000 match 46109\n",
      ">>> step 140100\n",
      "step 140000-140099, precision 0.435707, recall 0.680417, f_score 0.531236\n",
      "=== total 140100 match 46109\n",
      ">>> step 140200\n",
      "step 140100-140199, precision 0.435707, recall 0.680095, f_score 0.531137\n",
      "=== total 140200 match 46109\n",
      ">>> step 140300\n",
      "step 140200-140299, precision 0.435707, recall 0.679359, f_score 0.530913\n",
      "=== total 140300 match 46109\n",
      ">>> step 140400\n",
      "step 140300-140399, precision 0.435707, recall 0.679037, f_score 0.530814\n",
      "=== total 140400 match 46109\n",
      ">>> step 140500\n",
      "step 140400-140499, precision 0.435707, recall 0.678739, f_score 0.530723\n",
      "=== total 140500 match 46109\n",
      ">>> step 140600\n",
      "step 140500-140599, precision 0.435707, recall 0.678098, f_score 0.530527\n",
      "=== total 140600 match 46109\n",
      ">>> step 140700\n",
      "step 140600-140699, precision 0.435707, recall 0.678098, f_score 0.530527\n",
      "=== total 140700 match 46109\n",
      ">>> step 140800\n",
      "step 140700-140799, precision 0.435669, recall 0.677366, f_score 0.530275\n",
      "=== total 140800 match 46113\n",
      ">>> step 140900\n",
      "step 140800-140899, precision 0.435631, recall 0.676978, f_score 0.530128\n",
      "=== total 140900 match 46117\n",
      ">>> step 141000\n",
      "step 140900-140999, precision 0.435593, recall 0.675908, f_score 0.529772\n",
      "=== total 141000 match 46121\n",
      ">>> step 141100\n",
      "step 141000-141099, precision 0.435573, recall 0.675713, f_score 0.529697\n",
      "=== total 141100 match 46130\n",
      ">>> step 141200\n",
      "step 141100-141199, precision 0.435564, recall 0.675123, f_score 0.529509\n",
      "=== total 141200 match 46131\n",
      ">>> step 141300\n",
      "step 141200-141299, precision 0.435526, recall 0.674533, f_score 0.529299\n",
      "=== total 141300 match 46135\n",
      ">>> step 141400\n",
      "step 141300-141399, precision 0.435451, recall 0.673674, f_score 0.528979\n",
      "=== total 141400 match 46143\n",
      ">>> step 141500\n",
      "step 141400-141499, precision 0.435422, recall 0.673087, f_score 0.528777\n",
      "=== total 141500 match 46146\n",
      ">>> step 141600\n",
      "step 141500-141599, precision 0.435413, recall 0.672727, f_score 0.528659\n",
      "=== total 141600 match 46147\n",
      ">>> step 141700\n",
      "step 141600-141699, precision 0.435403, recall 0.671378, f_score 0.528235\n",
      "=== total 141700 match 46148\n",
      ">>> step 141800\n",
      "step 141700-141799, precision 0.435403, recall 0.670638, f_score 0.528006\n",
      "=== total 141800 match 46148\n",
      ">>> step 141900\n",
      "step 141800-141899, precision 0.435403, recall 0.669276, f_score 0.527583\n",
      "=== total 141900 match 46148\n",
      ">>> step 142000\n",
      "step 141900-141999, precision 0.435366, recall 0.668697, f_score 0.527375\n",
      "=== total 142000 match 46152\n",
      ">>> step 142100\n",
      "step 142000-142099, precision 0.435366, recall 0.668697, f_score 0.527375\n",
      "=== total 142100 match 46152\n",
      ">>> step 142200\n",
      "step 142100-142199, precision 0.435253, recall 0.668697, f_score 0.527292\n",
      "=== total 142200 match 46164\n",
      ">>> step 142300\n",
      "step 142200-142299, precision 0.435270, recall 0.668840, f_score 0.527350\n",
      "=== total 142300 match 46192\n",
      ">>> step 142400\n",
      "step 142300-142399, precision 0.435110, recall 0.668840, f_score 0.527232\n",
      "=== total 142400 match 46209\n",
      ">>> step 142500\n",
      "step 142400-142499, precision 0.434988, recall 0.668551, f_score 0.527053\n",
      "=== total 142500 match 46222\n",
      ">>> step 142600\n",
      "step 142500-142599, precision 0.434818, recall 0.668395, f_score 0.526880\n",
      "=== total 142600 match 46240\n",
      ">>> step 142700\n",
      "step 142600-142699, precision 0.434720, recall 0.668295, f_score 0.526776\n",
      "=== total 142700 match 46262\n",
      ">>> step 142800\n",
      "step 142700-142799, precision 0.434782, recall 0.668405, f_score 0.526856\n",
      "=== total 142800 match 46283\n",
      ">>> step 142900\n",
      "step 142800-142899, precision 0.434707, recall 0.668405, f_score 0.526801\n",
      "=== total 142900 match 46291\n",
      ">>> step 143000\n",
      "step 142900-142999, precision 0.434613, recall 0.668405, f_score 0.526732\n",
      "=== total 143000 match 46301\n",
      ">>> step 143100\n",
      "step 143000-143099, precision 0.434459, recall 0.668515, f_score 0.526653\n",
      "=== total 143100 match 46345\n",
      ">>> step 143200\n",
      "step 143100-143199, precision 0.434318, recall 0.668658, f_score 0.526594\n",
      "=== total 143200 match 46390\n",
      ">>> step 143300\n",
      "step 143200-143299, precision 0.434205, recall 0.668658, f_score 0.526511\n",
      "=== total 143300 match 46402\n",
      ">>> step 143400\n",
      "step 143300-143399, precision 0.434140, recall 0.668658, f_score 0.526463\n",
      "=== total 143400 match 46409\n",
      ">>> step 143500\n",
      "step 143400-143499, precision 0.434274, recall 0.668889, f_score 0.526633\n",
      "=== total 143500 match 46443\n",
      ">>> step 143600\n",
      "step 143500-143599, precision 0.434176, recall 0.668655, f_score 0.526489\n",
      "=== total 143600 match 46465\n",
      ">>> step 143700\n",
      "step 143600-143699, precision 0.434204, recall 0.668798, f_score 0.526553\n",
      "=== total 143700 match 46492\n",
      ">>> step 143800\n",
      "step 143700-143799, precision 0.434243, recall 0.668774, f_score 0.526575\n",
      "=== total 143800 match 46520\n",
      ">>> step 143900\n",
      "step 143800-143899, precision 0.434065, recall 0.668807, f_score 0.526454\n",
      "=== total 143900 match 46546\n",
      ">>> step 144000\n",
      "step 143900-143999, precision 0.433935, recall 0.668807, f_score 0.526358\n",
      "=== total 144000 match 46560\n",
      ">>> step 144100\n",
      "step 144000-144099, precision 0.433776, recall 0.668807, f_score 0.526242\n",
      "=== total 144100 match 46577\n",
      ">>> step 144200\n",
      "step 144100-144199, precision 0.433609, recall 0.668807, f_score 0.526118\n",
      "=== total 144200 match 46595\n",
      ">>> step 144300\n",
      "step 144200-144299, precision 0.433488, recall 0.668807, f_score 0.526029\n",
      "=== total 144300 match 46608\n",
      ">>> step 144400\n",
      "step 144300-144399, precision 0.433356, recall 0.668873, f_score 0.525953\n",
      "=== total 144400 match 46636\n",
      ">>> step 144500\n",
      "step 144400-144499, precision 0.433221, recall 0.668961, f_score 0.525880\n",
      "=== total 144500 match 46669\n",
      ">>> step 144600\n",
      "step 144500-144599, precision 0.433435, recall 0.669245, f_score 0.526126\n",
      "=== total 144600 match 46706\n",
      ">>> step 144700\n",
      "step 144600-144699, precision 0.433428, recall 0.669243, f_score 0.526120\n",
      "=== total 144700 match 46739\n",
      ">>> step 144800\n",
      "step 144700-144799, precision 0.433146, recall 0.669298, f_score 0.525929\n",
      "=== total 144800 match 46781\n",
      ">>> step 144900\n",
      "step 144800-144899, precision 0.433074, recall 0.669143, f_score 0.525828\n",
      "=== total 144900 match 46798\n",
      ">>> step 145000\n",
      "step 144900-144999, precision 0.432898, recall 0.669143, f_score 0.525699\n",
      "=== total 145000 match 46817\n",
      ">>> step 145100\n",
      "step 145000-145099, precision 0.432714, recall 0.669186, f_score 0.525576\n",
      "=== total 145100 match 46860\n",
      ">>> step 145200\n",
      "step 145100-145199, precision 0.432465, recall 0.669186, f_score 0.525393\n",
      "=== total 145200 match 46887\n",
      ">>> step 145300\n",
      "step 145200-145299, precision 0.432340, recall 0.669339, f_score 0.525347\n",
      "=== total 145300 match 46933\n",
      ">>> step 145400\n",
      "step 145300-145399, precision 0.432245, recall 0.669161, f_score 0.525222\n",
      "=== total 145400 match 46971\n",
      ">>> step 145500\n",
      "step 145400-145499, precision 0.432061, recall 0.669161, f_score 0.525087\n",
      "=== total 145500 match 46991\n",
      ">>> step 145600\n",
      "step 145500-145599, precision 0.431991, recall 0.669171, f_score 0.525038\n",
      "=== total 145600 match 47001\n",
      ">>> step 145700\n",
      "step 145600-145699, precision 0.431960, recall 0.669368, f_score 0.525076\n",
      "=== total 145700 match 47046\n",
      ">>> step 145800\n",
      "step 145700-145799, precision 0.431804, recall 0.669368, f_score 0.524960\n",
      "=== total 145800 match 47063\n",
      ">>> step 145900\n",
      "step 145800-145899, precision 0.431529, recall 0.669368, f_score 0.524757\n",
      "=== total 145900 match 47093\n",
      ">>> step 146000\n",
      "step 145900-145999, precision 0.431944, recall 0.669780, f_score 0.525191\n",
      "=== total 146000 match 47145\n",
      ">>> step 146100\n",
      "step 146000-146099, precision 0.431688, recall 0.669780, f_score 0.525001\n",
      "=== total 146100 match 47173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 146200\n",
      "step 146100-146199, precision 0.431514, recall 0.669780, f_score 0.524872\n",
      "=== total 146200 match 47192\n",
      ">>> step 146300\n",
      "step 146200-146299, precision 0.431395, recall 0.669780, f_score 0.524784\n",
      "=== total 146300 match 47205\n",
      ">>> step 146400\n",
      "step 146300-146399, precision 0.431340, recall 0.669780, f_score 0.524744\n",
      "=== total 146400 match 47211\n",
      ">>> step 146500\n",
      "step 146400-146499, precision 0.431267, recall 0.669780, f_score 0.524690\n",
      "=== total 146500 match 47219\n",
      ">>> step 146600\n",
      "step 146500-146599, precision 0.431139, recall 0.669428, f_score 0.524487\n",
      "=== total 146600 match 47233\n",
      ">>> step 146700\n",
      "step 146600-146699, precision 0.431002, recall 0.669208, f_score 0.524318\n",
      "=== total 146700 match 47248\n",
      ">>> step 146800\n",
      "step 146700-146799, precision 0.430939, recall 0.668659, f_score 0.524102\n",
      "=== total 146800 match 47255\n",
      ">>> step 146900\n",
      "step 146800-146899, precision 0.430932, recall 0.668285, f_score 0.523983\n",
      "=== total 146900 match 47265\n",
      ">>> step 147000\n",
      "step 146900-146999, precision 0.430937, recall 0.667967, f_score 0.523889\n",
      "=== total 147000 match 47276\n",
      ">>> step 147100\n",
      "step 147000-147099, precision 0.430892, recall 0.667814, f_score 0.523808\n",
      "=== total 147100 match 47281\n",
      ">>> step 147200\n",
      "step 147100-147199, precision 0.430801, recall 0.667683, f_score 0.523701\n",
      "=== total 147200 match 47291\n",
      ">>> step 147300\n",
      "step 147200-147299, precision 0.430893, recall 0.667824, f_score 0.523813\n",
      "=== total 147300 match 47311\n",
      ">>> step 147400\n",
      "step 147300-147399, precision 0.430720, recall 0.667824, f_score 0.523685\n",
      "=== total 147400 match 47330\n",
      ">>> step 147500\n",
      "step 147400-147499, precision 0.430744, recall 0.667518, f_score 0.523608\n",
      "=== total 147500 match 47346\n",
      ">>> step 147600\n",
      "step 147500-147599, precision 0.430987, recall 0.667735, f_score 0.523854\n",
      "=== total 147600 match 47375\n",
      ">>> step 147700\n",
      "step 147600-147699, precision 0.430923, recall 0.667735, f_score 0.523807\n",
      "=== total 147700 match 47382\n",
      ">>> step 147800\n",
      "step 147700-147799, precision 0.430832, recall 0.667168, f_score 0.523565\n",
      "=== total 147800 match 47392\n",
      ">>> step 147900\n",
      "step 147800-147899, precision 0.430741, recall 0.666601, f_score 0.523324\n",
      "=== total 147900 match 47402\n",
      ">>> step 148000\n",
      "step 147900-147999, precision 0.430660, recall 0.666601, f_score 0.523263\n",
      "=== total 148000 match 47411\n",
      ">>> step 148100\n",
      "step 148000-148099, precision 0.430569, recall 0.666036, f_score 0.523022\n",
      "=== total 148100 match 47421\n",
      ">>> step 148200\n",
      "step 148100-148199, precision 0.430487, recall 0.665472, f_score 0.522788\n",
      "=== total 148200 match 47430\n",
      ">>> step 148300\n",
      "step 148200-148299, precision 0.430433, recall 0.664692, f_score 0.522507\n",
      "=== total 148300 match 47436\n",
      ">>> step 148400\n",
      "step 148300-148399, precision 0.430315, recall 0.664519, f_score 0.522366\n",
      "=== total 148400 match 47449\n",
      ">>> step 148500\n",
      "step 148400-148499, precision 0.430151, recall 0.664519, f_score 0.522246\n",
      "=== total 148500 match 47467\n",
      ">>> step 148600\n",
      "step 148500-148599, precision 0.430088, recall 0.664519, f_score 0.522199\n",
      "=== total 148600 match 47474\n",
      ">>> step 148700\n",
      "step 148600-148699, precision 0.429907, recall 0.664519, f_score 0.522066\n",
      "=== total 148700 match 47494\n",
      ">>> step 148800\n",
      "step 148700-148799, precision 0.429717, recall 0.664519, f_score 0.521926\n",
      "=== total 148800 match 47515\n",
      ">>> step 148900\n",
      "step 148800-148899, precision 0.429575, recall 0.664519, f_score 0.521821\n",
      "=== total 148900 match 47540\n",
      ">>> step 149000\n",
      "step 148900-148999, precision 0.429779, recall 0.664705, f_score 0.522029\n",
      "=== total 149000 match 47571\n",
      ">>> step 149100\n",
      "step 149000-149099, precision 0.429721, recall 0.664825, f_score 0.522023\n",
      "=== total 149100 match 47603\n",
      ">>> step 149200\n",
      "step 149100-149199, precision 0.430097, recall 0.665249, f_score 0.522432\n",
      "=== total 149200 match 47652\n",
      ">>> step 149300\n",
      "step 149200-149299, precision 0.430009, recall 0.665110, f_score 0.522324\n",
      "=== total 149300 match 47692\n",
      ">>> step 149400\n",
      "step 149300-149399, precision 0.429820, recall 0.665110, f_score 0.522184\n",
      "=== total 149400 match 47713\n",
      ">>> step 149500\n",
      "step 149400-149499, precision 0.429750, recall 0.665251, f_score 0.522176\n",
      "=== total 149500 match 47751\n",
      ">>> step 149600\n",
      "step 149500-149599, precision 0.429489, recall 0.665079, f_score 0.521930\n",
      "=== total 149600 match 47780\n",
      ">>> step 149700\n",
      "step 149600-149699, precision 0.429292, recall 0.664788, f_score 0.521695\n",
      "=== total 149700 match 47809\n",
      ">>> step 149800\n",
      "step 149700-149799, precision 0.429130, recall 0.664508, f_score 0.521489\n",
      "=== total 149800 match 47827\n",
      ">>> step 149900\n",
      "step 149800-149899, precision 0.428995, recall 0.664057, f_score 0.521251\n",
      "=== total 149900 match 47842\n",
      ">>> step 150000\n",
      "step 149900-149999, precision 0.428771, recall 0.663767, f_score 0.520996\n",
      "=== total 150000 match 47874\n",
      ">>> step 150100\n",
      "step 150000-150099, precision 0.428667, recall 0.663908, f_score 0.520963\n",
      "=== total 150100 match 47916\n",
      ">>> step 150200\n",
      "step 150100-150199, precision 0.428697, recall 0.664115, f_score 0.521048\n",
      "=== total 150200 match 47957\n",
      ">>> step 150300\n",
      "step 150200-150299, precision 0.428801, recall 0.664332, f_score 0.521192\n",
      "=== total 150300 match 47992\n",
      ">>> step 150400\n",
      "step 150300-150399, precision 0.428759, recall 0.664472, f_score 0.521204\n",
      "=== total 150400 match 48027\n",
      ">>> step 150500\n",
      "step 150400-150499, precision 0.428785, recall 0.664700, f_score 0.521294\n",
      "=== total 150500 match 48073\n",
      ">>> step 150600\n",
      "step 150500-150599, precision 0.428714, recall 0.664894, f_score 0.521301\n",
      "=== total 150600 match 48123\n",
      ">>> step 150700\n",
      "step 150600-150699, precision 0.429144, recall 0.665336, f_score 0.521754\n",
      "=== total 150700 match 48175\n",
      ">>> step 150800\n",
      "step 150700-150799, precision 0.428957, recall 0.665336, f_score 0.521616\n",
      "=== total 150800 match 48196\n",
      ">>> step 150900\n",
      "step 150800-150899, precision 0.428725, recall 0.665336, f_score 0.521445\n",
      "=== total 150900 match 48222\n",
      ">>> step 151000\n",
      "step 150900-150999, precision 0.428468, recall 0.665336, f_score 0.521255\n",
      "=== total 151000 match 48251\n",
      ">>> step 151100\n",
      "step 151000-151099, precision 0.428317, recall 0.665336, f_score 0.521143\n",
      "=== total 151100 match 48268\n",
      ">>> step 151200\n",
      "step 151100-151199, precision 0.428291, recall 0.665487, f_score 0.521170\n",
      "=== total 151200 match 48313\n",
      ">>> step 151300\n",
      "step 151200-151299, precision 0.428122, recall 0.665487, f_score 0.521045\n",
      "=== total 151300 match 48332\n",
      ">>> step 151400\n",
      "step 151300-151399, precision 0.428368, recall 0.665543, f_score 0.521244\n",
      "=== total 151400 match 48386\n",
      ">>> step 151500\n",
      "step 151400-151499, precision 0.428356, recall 0.665341, f_score 0.521173\n",
      "=== total 151500 match 48413\n",
      ">>> step 151600\n",
      "step 151500-151599, precision 0.428223, recall 0.665341, f_score 0.521075\n",
      "=== total 151600 match 48428\n",
      ">>> step 151700\n",
      "step 151600-151699, precision 0.428138, recall 0.665480, f_score 0.521055\n",
      "=== total 151700 match 48468\n",
      ">>> step 151800\n",
      "step 151700-151799, precision 0.427979, recall 0.665480, f_score 0.520937\n",
      "=== total 151800 match 48486\n",
      ">>> step 151900\n",
      "step 151800-151899, precision 0.427867, recall 0.664958, f_score 0.520694\n",
      "=== total 151900 match 48501\n",
      ">>> step 152000\n",
      "step 151900-151999, precision 0.427709, recall 0.664958, f_score 0.520576\n",
      "=== total 152000 match 48519\n",
      ">>> step 152100\n",
      "step 152000-152099, precision 0.427618, recall 0.665012, f_score 0.520526\n",
      "=== total 152100 match 48555\n",
      ">>> step 152200\n",
      "step 152100-152199, precision 0.427631, recall 0.664971, f_score 0.520523\n",
      "=== total 152200 match 48591\n",
      ">>> step 152300\n",
      "step 152200-152299, precision 0.427481, recall 0.664971, f_score 0.520412\n",
      "=== total 152300 match 48608\n",
      ">>> step 152400\n",
      "step 152300-152399, precision 0.427443, recall 0.664482, f_score 0.520234\n",
      "=== total 152400 match 48617\n",
      ">>> step 152500\n",
      "step 152400-152499, precision 0.427573, recall 0.664473, f_score 0.520327\n",
      "=== total 152500 match 48642\n",
      ">>> step 152600\n",
      "step 152500-152599, precision 0.427441, recall 0.664473, f_score 0.520230\n",
      "=== total 152600 match 48657\n",
      ">>> step 152700\n",
      "step 152600-152699, precision 0.427248, recall 0.664473, f_score 0.520087\n",
      "=== total 152700 match 48679\n",
      ">>> step 152800\n",
      "step 152700-152799, precision 0.427560, recall 0.664625, f_score 0.520364\n",
      "=== total 152800 match 48723\n",
      ">>> step 152900\n",
      "step 152800-152899, precision 0.427402, recall 0.664625, f_score 0.520247\n",
      "=== total 152900 match 48741\n",
      ">>> step 153000\n",
      "step 152900-152999, precision 0.427200, recall 0.664625, f_score 0.520098\n",
      "=== total 153000 match 48764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 153100\n",
      "step 153000-153099, precision 0.427034, recall 0.664625, f_score 0.519975\n",
      "=== total 153100 match 48783\n",
      ">>> step 153200\n",
      "step 153100-153199, precision 0.426876, recall 0.664625, f_score 0.519858\n",
      "=== total 153200 match 48801\n",
      ">>> step 153300\n",
      "step 153200-153299, precision 0.426737, recall 0.664625, f_score 0.519754\n",
      "=== total 153300 match 48817\n",
      ">>> step 153400\n",
      "step 153300-153399, precision 0.426684, recall 0.663947, f_score 0.519508\n",
      "=== total 153400 match 48823\n",
      ">>> step 153500\n",
      "step 153400-153499, precision 0.426553, recall 0.663947, f_score 0.519411\n",
      "=== total 153500 match 48838\n",
      ">>> step 153600\n",
      "step 153500-153599, precision 0.426518, recall 0.662986, f_score 0.519091\n",
      "=== total 153600 match 48849\n",
      ">>> step 153700\n",
      "step 153600-153699, precision 0.426613, recall 0.662883, f_score 0.519129\n",
      "=== total 153700 match 48871\n",
      ">>> step 153800\n",
      "step 153700-153799, precision 0.426482, recall 0.662883, f_score 0.519032\n",
      "=== total 153800 match 48886\n",
      ">>> step 153900\n",
      "step 153800-153899, precision 0.426369, recall 0.662609, f_score 0.518864\n",
      "=== total 153900 match 48899\n",
      ">>> step 154000\n",
      "step 153900-153999, precision 0.426220, recall 0.662609, f_score 0.518754\n",
      "=== total 154000 match 48916\n",
      ">>> step 154100\n",
      "step 154000-154099, precision 0.426294, recall 0.662284, f_score 0.518709\n",
      "=== total 154100 match 48924\n",
      ">>> step 154200\n",
      "step 154100-154199, precision 0.426224, recall 0.662284, f_score 0.518658\n",
      "=== total 154200 match 48932\n",
      ">>> step 154300\n",
      "step 154200-154299, precision 0.426295, recall 0.662119, f_score 0.518659\n",
      "=== total 154300 match 48952\n",
      ">>> step 154400\n",
      "step 154300-154399, precision 0.426399, recall 0.662112, f_score 0.518734\n",
      "=== total 154400 match 48980\n",
      ">>> step 154500\n",
      "step 154400-154499, precision 0.426268, recall 0.662112, f_score 0.518637\n",
      "=== total 154500 match 48995\n",
      ">>> step 154600\n",
      "step 154500-154599, precision 0.426228, recall 0.661788, f_score 0.518508\n",
      "=== total 154600 match 49016\n",
      ">>> step 154700\n",
      "step 154600-154699, precision 0.426127, recall 0.661370, f_score 0.518305\n",
      "=== total 154700 match 49037\n",
      ">>> step 154800\n",
      "step 154700-154799, precision 0.426023, recall 0.661370, f_score 0.518228\n",
      "=== total 154800 match 49049\n",
      ">>> step 154900\n",
      "step 154800-154899, precision 0.425927, recall 0.660827, f_score 0.517991\n",
      "=== total 154900 match 49060\n",
      ">>> step 155000\n",
      "step 154900-154999, precision 0.425896, recall 0.660505, f_score 0.517869\n",
      "=== total 155000 match 49080\n",
      ">>> step 155100\n",
      "step 155000-155099, precision 0.425953, recall 0.660093, f_score 0.517784\n",
      "=== total 155100 match 49111\n",
      ">>> step 155200\n",
      "step 155100-155199, precision 0.425832, recall 0.660093, f_score 0.517695\n",
      "=== total 155200 match 49125\n",
      ">>> step 155300\n",
      "step 155200-155299, precision 0.425676, recall 0.660093, f_score 0.517579\n",
      "=== total 155300 match 49143\n",
      ">>> step 155400\n",
      "step 155300-155399, precision 0.425538, recall 0.660093, f_score 0.517477\n",
      "=== total 155400 match 49159\n",
      ">>> step 155500\n",
      "step 155400-155499, precision 0.425364, recall 0.660093, f_score 0.517349\n",
      "=== total 155500 match 49179\n",
      ">>> step 155600\n",
      "step 155500-155599, precision 0.425269, recall 0.660093, f_score 0.517278\n",
      "=== total 155600 match 49190\n",
      ">>> step 155700\n",
      "step 155600-155699, precision 0.425429, recall 0.660350, f_score 0.517475\n",
      "=== total 155700 match 49228\n",
      ">>> step 155800\n",
      "step 155700-155799, precision 0.425418, recall 0.660436, f_score 0.517494\n",
      "=== total 155800 match 49248\n",
      ">>> step 155900\n",
      "step 155800-155899, precision 0.425639, recall 0.660756, f_score 0.517755\n",
      "=== total 155900 match 49293\n",
      ">>> step 156000\n",
      "step 155900-155999, precision 0.425733, recall 0.660970, f_score 0.517891\n",
      "=== total 156000 match 49329\n",
      ">>> step 156100\n",
      "step 156000-156099, precision 0.425967, recall 0.661247, f_score 0.518149\n",
      "=== total 156100 match 49363\n",
      ">>> step 156200\n",
      "step 156100-156199, precision 0.425836, recall 0.661332, f_score 0.518078\n",
      "=== total 156200 match 49397\n",
      ">>> step 156300\n",
      "step 156200-156299, precision 0.425695, recall 0.661386, f_score 0.517991\n",
      "=== total 156300 match 49425\n",
      ">>> step 156400\n",
      "step 156300-156399, precision 0.425845, recall 0.661526, f_score 0.518144\n",
      "=== total 156400 match 49457\n",
      ">>> step 156500\n",
      "step 156400-156499, precision 0.425770, recall 0.661537, f_score 0.518092\n",
      "=== total 156500 match 49468\n",
      ">>> step 156600\n",
      "step 156500-156599, precision 0.425951, recall 0.661813, f_score 0.518311\n",
      "=== total 156600 match 49508\n",
      ">>> step 156700\n",
      "step 156600-156699, precision 0.426115, recall 0.662088, f_score 0.518517\n",
      "=== total 156700 match 49550\n",
      ">>> step 156800\n",
      "step 156700-156799, precision 0.425935, recall 0.662088, f_score 0.518383\n",
      "=== total 156800 match 49571\n",
      ">>> step 156900\n",
      "step 156800-156899, precision 0.425918, recall 0.662364, f_score 0.518455\n",
      "=== total 156900 match 49634\n",
      ">>> step 157000\n",
      "step 156900-156999, precision 0.425929, recall 0.662617, f_score 0.518541\n",
      "=== total 157000 match 49689\n",
      ">>> step 157100\n",
      "step 157000-157099, precision 0.425764, recall 0.662639, f_score 0.518425\n",
      "=== total 157100 match 49713\n",
      ">>> step 157200\n",
      "step 157100-157199, precision 0.425756, recall 0.662913, f_score 0.518503\n",
      "=== total 157200 match 49775\n",
      ">>> step 157300\n",
      "step 157200-157299, precision 0.425782, recall 0.663187, f_score 0.518606\n",
      "=== total 157300 match 49833\n",
      ">>> step 157400\n",
      "step 157300-157399, precision 0.425748, recall 0.663397, f_score 0.518645\n",
      "=== total 157400 match 49884\n",
      ">>> step 157500\n",
      "step 157400-157499, precision 0.425887, recall 0.663649, f_score 0.518826\n",
      "=== total 157500 match 49924\n",
      ">>> step 157600\n",
      "step 157500-157599, precision 0.425734, recall 0.663649, f_score 0.518712\n",
      "=== total 157600 match 49942\n",
      ">>> step 157700\n",
      "step 157600-157699, precision 0.425589, recall 0.663649, f_score 0.518604\n",
      "=== total 157700 match 49959\n",
      ">>> step 157800\n",
      "step 157700-157799, precision 0.425572, recall 0.663649, f_score 0.518592\n",
      "=== total 157800 match 49961\n",
      ">>> step 157900\n",
      "step 157800-157899, precision 0.425461, recall 0.663380, f_score 0.518427\n",
      "=== total 157900 match 49974\n",
      ">>> step 158000\n",
      "step 157900-157999, precision 0.425434, recall 0.663019, f_score 0.518296\n",
      "=== total 158000 match 49989\n",
      ">>> step 158100\n",
      "step 158000-158099, precision 0.425396, recall 0.663156, f_score 0.518310\n",
      "=== total 158100 match 50024\n",
      ">>> step 158200\n",
      "step 158100-158199, precision 0.425394, recall 0.663208, f_score 0.518325\n",
      "=== total 158200 match 50036\n",
      ">>> step 158300\n",
      "step 158200-158299, precision 0.425402, recall 0.663324, f_score 0.518366\n",
      "=== total 158300 match 50075\n",
      ">>> step 158400\n",
      "step 158300-158399, precision 0.425228, recall 0.663440, f_score 0.518273\n",
      "=== total 158400 match 50126\n",
      ">>> step 158500\n",
      "step 158400-158499, precision 0.425218, recall 0.663286, f_score 0.518218\n",
      "=== total 158500 match 50139\n",
      ">>> step 158600\n",
      "step 158500-158599, precision 0.425222, recall 0.663164, f_score 0.518184\n",
      "=== total 158600 match 50162\n",
      ">>> step 158700\n",
      "step 158600-158699, precision 0.425144, recall 0.662989, f_score 0.518072\n",
      "=== total 158700 match 50183\n",
      ">>> step 158800\n",
      "step 158700-158799, precision 0.425111, recall 0.662785, f_score 0.517985\n",
      "=== total 158800 match 50201\n",
      ">>> step 158900\n",
      "step 158800-158899, precision 0.425111, recall 0.662785, f_score 0.517985\n",
      "=== total 158900 match 50201\n",
      ">>> step 159000\n",
      "step 158900-158999, precision 0.425086, recall 0.662785, f_score 0.517967\n",
      "=== total 159000 match 50204\n",
      ">>> step 159100\n",
      "step 159000-159099, precision 0.425018, recall 0.662250, f_score 0.517753\n",
      "=== total 159100 match 50212\n",
      ">>> step 159200\n",
      "step 159100-159199, precision 0.425093, recall 0.661984, f_score 0.517727\n",
      "=== total 159200 match 50222\n",
      ">>> step 159300\n",
      "step 159200-159299, precision 0.425071, recall 0.661791, f_score 0.517652\n",
      "=== total 159300 match 50241\n",
      ">>> step 159400\n",
      "step 159300-159399, precision 0.425012, recall 0.660849, f_score 0.517320\n",
      "=== total 159400 match 50248\n",
      ">>> step 159500\n",
      "step 159400-159499, precision 0.424970, recall 0.659828, f_score 0.516976\n",
      "=== total 159500 match 50253\n",
      ">>> step 159600\n",
      "step 159500-159599, precision 0.424905, recall 0.659329, f_score 0.516775\n",
      "=== total 159600 match 50263\n",
      ">>> step 159700\n",
      "step 159600-159699, precision 0.424863, recall 0.658740, f_score 0.516562\n",
      "=== total 159700 match 50268\n",
      ">>> step 159800\n",
      "step 159700-159799, precision 0.424820, recall 0.657908, f_score 0.516275\n",
      "=== total 159800 match 50273\n",
      ">>> step 159900\n",
      "step 159800-159899, precision 0.424807, recall 0.657270, f_score 0.516068\n",
      "=== total 159900 match 50277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 160000\n",
      "step 159900-159999, precision 0.424773, recall 0.656240, f_score 0.515726\n",
      "=== total 160000 match 50281\n",
      ">>> step 160100\n",
      "step 160000-160099, precision 0.424764, recall 0.656240, f_score 0.515719\n",
      "=== total 160100 match 50282\n",
      ">>> step 160200\n",
      "step 160100-160199, precision 0.425040, recall 0.656557, f_score 0.516020\n",
      "=== total 160200 match 50320\n",
      ">>> step 160300\n",
      "step 160200-160299, precision 0.425198, recall 0.656789, f_score 0.516208\n",
      "=== total 160300 match 50353\n",
      ">>> step 160400\n",
      "step 160300-160399, precision 0.425702, recall 0.657377, f_score 0.516761\n",
      "=== total 160400 match 50425\n",
      ">>> step 160500\n",
      "step 160400-160499, precision 0.425634, recall 0.657377, f_score 0.516711\n",
      "=== total 160500 match 50433\n",
      ">>> step 160600\n",
      "step 160500-160599, precision 0.425888, recall 0.657650, f_score 0.516983\n",
      "=== total 160600 match 50464\n",
      ">>> step 160700\n",
      "step 160600-160699, precision 0.426057, recall 0.657922, f_score 0.517191\n",
      "=== total 160700 match 50505\n",
      ">>> step 160800\n",
      "step 160700-160799, precision 0.426310, recall 0.658194, f_score 0.517462\n",
      "=== total 160800 match 50536\n",
      ">>> step 160900\n",
      "step 160800-160899, precision 0.426550, recall 0.658409, f_score 0.517705\n",
      "=== total 160900 match 50592\n",
      ">>> step 161000\n",
      "step 160900-160999, precision 0.426621, recall 0.658383, f_score 0.517749\n",
      "=== total 161000 match 50614\n",
      ">>> step 161100\n",
      "step 161000-161099, precision 0.426789, recall 0.658485, f_score 0.517905\n",
      "=== total 161100 match 50662\n",
      ">>> step 161200\n",
      "step 161100-161199, precision 0.426966, recall 0.658817, f_score 0.518137\n",
      "=== total 161200 match 50716\n",
      ">>> step 161300\n",
      "step 161200-161299, precision 0.426957, recall 0.658817, f_score 0.518131\n",
      "=== total 161300 match 50717\n",
      ">>> step 161400\n",
      "step 161300-161399, precision 0.426941, recall 0.658817, f_score 0.518119\n",
      "=== total 161400 match 50719\n",
      ">>> step 161500\n",
      "step 161400-161499, precision 0.426932, recall 0.658296, f_score 0.517952\n",
      "=== total 161500 match 50720\n",
      ">>> step 161600\n",
      "step 161500-161599, precision 0.426932, recall 0.657677, f_score 0.517760\n",
      "=== total 161600 match 50720\n",
      ">>> step 161700\n",
      "step 161600-161699, precision 0.426932, recall 0.656898, f_score 0.517518\n",
      "=== total 161700 match 50720\n",
      ">>> step 161800\n",
      "step 161700-161799, precision 0.426924, recall 0.656381, f_score 0.517351\n",
      "=== total 161800 match 50721\n",
      ">>> step 161900\n",
      "step 161800-161899, precision 0.426924, recall 0.656122, f_score 0.517271\n",
      "=== total 161900 match 50721\n",
      ">>> step 162000\n",
      "step 161900-161999, precision 0.426915, recall 0.655606, f_score 0.517104\n",
      "=== total 162000 match 50722\n",
      ">>> step 162100\n",
      "step 162000-162099, precision 0.426915, recall 0.655090, f_score 0.516944\n",
      "=== total 162100 match 50722\n",
      ">>> step 162200\n",
      "step 162100-162199, precision 0.426915, recall 0.654575, f_score 0.516783\n",
      "=== total 162200 match 50722\n",
      ">>> step 162300\n",
      "step 162200-162299, precision 0.426915, recall 0.653943, f_score 0.516586\n",
      "=== total 162300 match 50722\n",
      ">>> step 162400\n",
      "step 162300-162399, precision 0.426915, recall 0.653311, f_score 0.516389\n",
      "=== total 162400 match 50722\n",
      ">>> step 162500\n",
      "step 162400-162499, precision 0.426915, recall 0.652170, f_score 0.516032\n",
      "=== total 162500 match 50722\n",
      ">>> step 162600\n",
      "step 162500-162599, precision 0.426890, recall 0.651934, f_score 0.515940\n",
      "=== total 162600 match 50725\n",
      ">>> step 162700\n",
      "step 162600-162699, precision 0.426882, recall 0.651934, f_score 0.515934\n",
      "=== total 162700 match 50726\n",
      ">>> step 162800\n",
      "step 162700-162799, precision 0.426823, recall 0.651934, f_score 0.515891\n",
      "=== total 162800 match 50733\n",
      ">>> step 162900\n",
      "step 162800-162899, precision 0.426814, recall 0.651424, f_score 0.515725\n",
      "=== total 162900 match 50734\n",
      ">>> step 163000\n",
      "step 162900-162999, precision 0.426814, recall 0.650563, f_score 0.515455\n",
      "=== total 163000 match 50734\n",
      ">>> step 163100\n",
      "step 163000-163099, precision 0.426798, recall 0.650056, f_score 0.515283\n",
      "=== total 163100 match 50736\n",
      ">>> step 163200\n",
      "step 163100-163199, precision 0.426789, recall 0.649549, f_score 0.515118\n",
      "=== total 163200 match 50737\n",
      ">>> step 163300\n",
      "step 163200-163299, precision 0.426772, recall 0.649295, f_score 0.515026\n",
      "=== total 163300 match 50739\n",
      ">>> step 163400\n",
      "step 163300-163399, precision 0.426772, recall 0.648537, f_score 0.514787\n",
      "=== total 163400 match 50739\n",
      ">>> step 163500\n",
      "step 163400-163499, precision 0.426764, recall 0.648207, f_score 0.514677\n",
      "=== total 163500 match 50740\n",
      ">>> step 163600\n",
      "step 163500-163599, precision 0.426764, recall 0.647161, f_score 0.514347\n",
      "=== total 163600 match 50740\n",
      ">>> step 163700\n",
      "step 163600-163699, precision 0.426739, recall 0.646794, f_score 0.514212\n",
      "=== total 163700 match 50743\n",
      ">>> step 163800\n",
      "step 163700-163799, precision 0.426739, recall 0.646157, f_score 0.514011\n",
      "=== total 163800 match 50743\n",
      ">>> step 163900\n",
      "step 163800-163899, precision 0.426739, recall 0.645194, f_score 0.513706\n",
      "=== total 163900 match 50743\n",
      ">>> step 164000\n",
      "step 163900-163999, precision 0.426713, recall 0.645194, f_score 0.513688\n",
      "=== total 164000 match 50746\n",
      ">>> step 164100\n",
      "step 164000-164099, precision 0.426621, recall 0.645194, f_score 0.513621\n",
      "=== total 164100 match 50757\n",
      ">>> step 164200\n",
      "step 164100-164199, precision 0.426579, recall 0.644695, f_score 0.513432\n",
      "=== total 164200 match 50762\n",
      ">>> step 164300\n",
      "step 164200-164299, precision 0.426573, recall 0.644264, f_score 0.513292\n",
      "=== total 164300 match 50765\n",
      ">>> step 164400\n",
      "step 164300-164399, precision 0.426573, recall 0.643383, f_score 0.513012\n",
      "=== total 164400 match 50765\n",
      ">>> step 164500\n",
      "step 164400-164499, precision 0.426523, recall 0.642887, f_score 0.512817\n",
      "=== total 164500 match 50771\n",
      ">>> step 164600\n",
      "step 164500-164599, precision 0.426481, recall 0.642639, f_score 0.512708\n",
      "=== total 164600 match 50776\n",
      ">>> step 164700\n",
      "step 164600-164699, precision 0.426422, recall 0.642162, f_score 0.512514\n",
      "=== total 164700 match 50783\n",
      ">>> step 164800\n",
      "step 164700-164799, precision 0.426397, recall 0.641649, f_score 0.512332\n",
      "=== total 164800 match 50786\n",
      ">>> step 164900\n",
      "step 164800-164899, precision 0.426389, recall 0.641155, f_score 0.512169\n",
      "=== total 164900 match 50787\n",
      ">>> step 165000\n",
      "step 164900-164999, precision 0.426347, recall 0.640548, f_score 0.511945\n",
      "=== total 165000 match 50792\n",
      ">>> step 165100\n",
      "step 165000-165099, precision 0.426347, recall 0.640056, f_score 0.511787\n",
      "=== total 165100 match 50792\n",
      ">>> step 165200\n",
      "step 165100-165199, precision 0.426350, recall 0.639065, f_score 0.511472\n",
      "=== total 165200 match 50794\n",
      ">>> step 165300\n",
      "step 165200-165299, precision 0.426302, recall 0.638680, f_score 0.511315\n",
      "=== total 165300 match 50802\n",
      ">>> step 165400\n",
      "step 165300-165399, precision 0.426243, recall 0.638680, f_score 0.511273\n",
      "=== total 165400 match 50809\n",
      ">>> step 165500\n",
      "step 165400-165499, precision 0.426118, recall 0.638680, f_score 0.511182\n",
      "=== total 165500 match 50824\n",
      ">>> step 165600\n",
      "step 165500-165599, precision 0.426215, recall 0.638927, f_score 0.511332\n",
      "=== total 165600 match 50871\n",
      ">>> step 165700\n",
      "step 165600-165699, precision 0.426518, recall 0.639307, f_score 0.511671\n",
      "=== total 165700 match 50931\n",
      ">>> step 165800\n",
      "step 165700-165799, precision 0.426707, recall 0.639465, f_score 0.511858\n",
      "=== total 165800 match 50960\n",
      ">>> step 165900\n",
      "step 165800-165899, precision 0.426868, recall 0.639505, f_score 0.511986\n",
      "=== total 165900 match 50983\n",
      ">>> step 166000\n",
      "step 165900-165999, precision 0.426823, recall 0.639320, f_score 0.511895\n",
      "=== total 166000 match 50993\n",
      ">>> step 166100\n",
      "step 166000-166099, precision 0.426917, recall 0.639304, f_score 0.511957\n",
      "=== total 166100 match 51024\n",
      ">>> step 166200\n",
      "step 166100-166199, precision 0.426987, recall 0.639254, f_score 0.511992\n",
      "=== total 166200 match 51046\n",
      ">>> step 166300\n",
      "step 166200-166299, precision 0.427171, recall 0.639292, f_score 0.512136\n",
      "=== total 166300 match 51099\n",
      ">>> step 166400\n",
      "step 166300-166399, precision 0.427200, recall 0.639368, f_score 0.512181\n",
      "=== total 166400 match 51133\n",
      ">>> step 166500\n",
      "step 166400-166499, precision 0.427374, recall 0.639625, f_score 0.512389\n",
      "=== total 166500 match 51194\n",
      ">>> step 166600\n",
      "step 166500-166599, precision 0.427788, recall 0.640034, f_score 0.512818\n",
      "=== total 166600 match 51252\n",
      ">>> step 166700\n",
      "step 166600-166699, precision 0.427771, recall 0.640034, f_score 0.512806\n",
      "=== total 166700 match 51254\n",
      ">>> step 166800\n",
      "step 166700-166799, precision 0.427771, recall 0.640034, f_score 0.512806\n",
      "=== total 166800 match 51254\n",
      "TOTAL 166800, precision 0.427771, recall 0.640034, f_score 0.512806\n",
      "TOTAL true = 34256\n",
      "TOTAL error rate = 0.249760\n"
     ]
    }
   ],
   "source": [
    "# TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "cubes_set = pre_process_training(\"PX303-Fg006-R-C02-R02-D08032015-T114223-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 1, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, is_enriched, ROOT_FOLDER + \"models/front_tear_model2.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RE-TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "train_imgs, train_lbls, is_enriched = \\\n",
    "    load_train_from_disk(ROOT_FOLDER + \"train_concats3/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250, 1)    \n",
    "# validate2_for_cross_validation(train_imgs, train_lbls, is_enriched, ROOT_FOLDER + \"model_binary/tear_model3.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x[1] == 1 for x in train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.78\n",
    "step 99, training accuracy 0.84\n",
    "step 149, training accuracy 0.9\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.84\n",
    "step 349, training accuracy 0.92\n",
    "step 399, training accuracy 0.86\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.86\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.84\n",
    "step 99, training accuracy 0.94\n",
    "step 149, training accuracy 0.86\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.96\n",
    "step 399, training accuracy 0.96\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.96\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "WARNING:tensorflow:From /home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
    "Instructions for updating:\n",
    "Use `tf.global_variables_initializer` instead.\n",
    "step 49, training accuracy 0.82\n",
    "step 99, training accuracy 0.92\n",
    "step 149, training accuracy 0.72\n",
    "step 199, training accuracy 0.8\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.88\n",
    "step 349, training accuracy 0.94\n",
    "step 399, training accuracy 0.84\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.84\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.76\n",
    "step 99, training accuracy 0.82\n",
    "step 149, training accuracy 0.96\n",
    "step 199, training accuracy 0.86\n",
    "step 249, training accuracy 0.76\n",
    "step 299, training accuracy 0.82\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.88\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.88\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.88\n",
    "step 149, training accuracy 0.88\n",
    "step 199, training accuracy 0.88\n",
    "step 249, training accuracy 0.92\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.94\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.94\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.96\n",
    "step 149, training accuracy 0.84\n",
    "step 199, training accuracy 0.84\n",
    "step 249, training accuracy 0.84\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.98\n",
    "step 399, training accuracy 0.9\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.9\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    ">>> step 100\n",
    "step 0-99, precision 0.695652, recall 0.640000, f_score 0.666667\n",
    ">>> step 200\n",
    "step 100-199, precision 0.512500, recall 0.732143, f_score 0.602941\n",
    ">>> step 300\n",
    "step 200-299, precision 0.350427, recall 0.732143, f_score 0.473988\n",
    ">>> step 400\n",
    "step 300-399, precision 0.408805, recall 0.677083, f_score 0.509804\n",
    ">>> step 500\n",
    "step 400-499, precision 0.366834, recall 0.651786, f_score 0.469453\n",
    ">>> step 600\n",
    "step 500-599, precision 0.349282, recall 0.651786, f_score 0.454829\n",
    ">>> step 700\n",
    "step 600-699, precision 0.376471, recall 0.662069, f_score 0.480000\n",
    ">>> step 800\n",
    "step 700-799, precision 0.383142, recall 0.595238, f_score 0.466200\n",
    ">>> step 900\n",
    "step 800-899, precision 0.394649, recall 0.617801, f_score 0.481633\n",
    ">>> step 1000\n",
    "step 900-999, precision 0.407821, recall 0.651786, f_score 0.501718\n",
    ">>> step 1100\n",
    "step 1000-1099, precision 0.404432, recall 0.651786, f_score 0.499145\n",
    ">>> step 1200\n",
    "step 1100-1199, precision 0.423559, recall 0.657588, f_score 0.515244\n",
    ">>> step 1300\n",
    "step 1200-1299, precision 0.408983, recall 0.662835, f_score 0.505848\n",
    ">>> step 1400\n",
    "step 1300-1399, precision 0.405640, recall 0.667857, f_score 0.504723\n",
    ">>> step 1500\n",
    "step 1400-1499, precision 0.415020, recall 0.670927, f_score 0.512821\n",
    ">>> step 1600\n",
    "step 1500-1599, precision 0.402852, recall 0.672619, f_score 0.503902\n",
    ">>> step 1700\n",
    "step 1600-1699, precision 0.401681, recall 0.647696, f_score 0.495851\n",
    ">>> step 1800\n",
    "step 1700-1799, precision 0.387987, recall 0.647696, f_score 0.485279\n",
    ">>> step 1900\n",
    "step 1800-1899, precision 0.385484, recall 0.647696, f_score 0.483316\n",
    ">>> step 2000\n",
    "step 1900-1999, precision 0.388802, recall 0.647668, f_score 0.485909\n",
    ">>> step 2100\n",
    "step 2000-2099, precision 0.379161, recall 0.650124, f_score 0.478976\n",
    ">>> step 2200\n",
    "step 2100-2199, precision 0.374286, recall 0.650124, f_score 0.475068\n",
    ">>> step 2300\n",
    "step 2200-2299, precision 0.381868, recall 0.651054, f_score 0.481385\n",
    ">>> step 2400\n",
    "step 2300-2399, precision 0.392622, recall 0.659292, f_score 0.492155\n",
    ">>> step 2500\n",
    "step 2400-2499, precision 0.393604, recall 0.665281, f_score 0.494590\n",
    ">>> step 2600\n",
    "step 2500-2599, precision 0.392601, recall 0.664646, f_score 0.493623\n",
    ">>> step 2700\n",
    "step 2600-2699, precision 0.389810, recall 0.664646, f_score 0.491412\n",
    ">>> step 2800\n",
    "step 2700-2799, precision 0.384884, recall 0.655446, f_score 0.484982\n",
    ">>> step 2900\n",
    "step 2800-2899, precision 0.390572, recall 0.664122, f_score 0.491873\n",
    ">>> step 3000\n",
    "step 2900-2999, precision 0.401064, recall 0.679279, f_score 0.504348\n",
    ">>> step 3100\n",
    "step 3000-3099, precision 0.418534, recall 0.695431, f_score 0.522568\n",
    ">>> step 3200\n",
    "step 3100-3199, precision 0.430129, recall 0.704545, f_score 0.534154\n",
    ">>> step 3300\n",
    "step 3200-3299, precision 0.446036, recall 0.716258, f_score 0.549735\n",
    ">>> step 3400\n",
    "step 3300-3399, precision 0.448563, recall 0.722388, f_score 0.553459\n",
    ">>> step 3500\n",
    "step 3400-3499, precision 0.455204, recall 0.719599, f_score 0.557650\n",
    ">>> step 3600\n",
    "step 3500-3599, precision 0.456560, recall 0.718271, f_score 0.558266\n",
    ">>> step 3700\n",
    "step 3600-3699, precision 0.457841, recall 0.724000, f_score 0.560950\n",
    ">>> step 3800\n",
    "step 3700-3799, precision 0.455285, recall 0.720721, f_score 0.558047\n",
    ">>> step 3900\n",
    "step 3800-3899, precision 0.459969, recall 0.724351, f_score 0.562650\n",
    ">>> step 4000\n",
    "step 3900-3999, precision 0.445289, recall 0.724351, f_score 0.551529\n",
    ">>> step 4100\n",
    "step 4000-4099, precision 0.442598, recall 0.724351, f_score 0.549461\n",
    ">>> step 4200\n",
    "step 4100-4199, precision 0.445596, recall 0.725301, f_score 0.552040\n",
    ">>> step 4300\n",
    "step 4200-4299, precision 0.429793, recall 0.724760, f_score 0.539597\n",
    ">>> step 4400\n",
    "step 4300-4399, precision 0.426450, recall 0.724760, f_score 0.536955\n",
    ">>> step 4500\n",
    "step 4400-4499, precision 0.427491, recall 0.727485, f_score 0.538528\n",
    ">>> step 4600\n",
    "step 4500-4599, precision 0.427310, recall 0.723820, f_score 0.537377\n",
    ">>> step 4700\n",
    "step 4600-4699, precision 0.422999, recall 0.723820, f_score 0.533956\n",
    ">>> step 4800\n",
    "step 4700-4799, precision 0.419893, recall 0.723820, f_score 0.531474\n",
    ">>> step 4900\n",
    "step 4800-4899, precision 0.423740, recall 0.723669, f_score 0.534504\n",
    ">>> step 5000\n",
    "step 4900-4999, precision 0.428387, recall 0.725683, f_score 0.538742\n",
    ">>> step 5100\n",
    "step 5000-5099, precision 0.417348, recall 0.725683, f_score 0.529928\n",
    ">>> step 5200\n",
    "step 5100-5199, precision 0.414482, recall 0.725683, f_score 0.527612\n",
    ">>> step 5300\n",
    "step 5200-5299, precision 0.412935, recall 0.725683, f_score 0.526358\n",
    ">>> step 5400\n",
    "step 5300-5399, precision 0.418093, recall 0.720759, f_score 0.529207\n",
    ">>> step 5500\n",
    "step 5400-5499, precision 0.409934, recall 0.720294, f_score 0.522502\n",
    ">>> step 5600\n",
    "step 5500-5599, precision 0.409280, recall 0.719665, f_score 0.521805\n",
    ">>> step 5700\n",
    "step 5600-5699, precision 0.411176, recall 0.716189, f_score 0.522422\n",
    ">>> step 5800\n",
    "step 5700-5799, precision 0.413056, recall 0.707921, f_score 0.521707\n",
    ">>> step 5900\n",
    "step 5800-5899, precision 0.408427, recall 0.701061, f_score 0.516152\n",
    ">>> step 6000\n",
    "step 5900-5999, precision 0.404113, recall 0.701061, f_score 0.512694\n",
    ">>> step 6100\n",
    "step 6000-6099, precision 0.399565, recall 0.700382, f_score 0.508839\n",
    ">>> step 6200\n",
    "step 6100-6199, precision 0.398913, recall 0.700382, f_score 0.508310\n",
    ">>> step 6300\n",
    "step 6200-6299, precision 0.402681, recall 0.704503, f_score 0.512453\n",
    ">>> step 6400\n",
    "step 6300-6399, precision 0.404178, recall 0.709441, f_score 0.514970\n",
    ">>> step 6500\n",
    "step 6400-6499, precision 0.402955, recall 0.713255, f_score 0.514974\n",
    ">>> step 6600\n",
    "step 6500-6599, precision 0.402532, recall 0.713645, f_score 0.514730\n",
    ">>> step 6700\n",
    "step 6600-6699, precision 0.413011, recall 0.712585, f_score 0.522933\n",
    ">>> step 6800\n",
    "step 6700-6799, precision 0.416506, recall 0.713813, f_score 0.526059\n",
    ">>> step 6900\n",
    "step 6800-6899, precision 0.421002, recall 0.712439, f_score 0.529253\n",
    ">>> step 7000\n",
    "step 6900-6999, precision 0.428773, recall 0.709176, f_score 0.534427\n",
    ">>> step 7100\n",
    "step 7000-7099, precision 0.430497, recall 0.704718, f_score 0.534488\n",
    ">>> step 7200\n",
    "step 7100-7199, precision 0.437929, recall 0.695273, f_score 0.537381\n",
    ">>> step 7300\n",
    "step 7200-7299, precision 0.440254, recall 0.693133, f_score 0.538483\n",
    ">>> step 7400\n",
    "step 7300-7399, precision 0.441348, recall 0.690577, f_score 0.538525\n",
    ">>> step 7500\n",
    "step 7400-7499, precision 0.439821, recall 0.688375, f_score 0.536719\n",
    ">>> step 7600\n",
    "step 7500-7599, precision 0.440693, recall 0.683196, f_score 0.535782\n",
    ">>> step 7700\n",
    "step 7600-7699, precision 0.440618, recall 0.676152, f_score 0.533547\n",
    ">>> step 7800\n",
    "step 7700-7799, precision 0.435618, recall 0.676152, f_score 0.529865\n",
    ">>> step 7900\n",
    "step 7800-7899, precision 0.431701, recall 0.674044, f_score 0.526316\n",
    ">>> step 8000\n",
    "step 7900-7999, precision 0.429482, recall 0.668651, f_score 0.523021\n",
    ">>> step 8100\n",
    "step 8000-8099, precision 0.428027, recall 0.667768, f_score 0.521672\n",
    ">>> step 8200\n",
    "step 8100-8199, precision 0.429717, recall 0.664057, f_score 0.521784\n",
    ">>> step 8300\n",
    "step 8200-8299, precision 0.428451, recall 0.664057, f_score 0.520849\n",
    ">>> step 8400\n",
    "step 8300-8399, precision 0.429589, recall 0.660438, f_score 0.520569\n",
    ">>> step 8500\n",
    "step 8400-8499, precision 0.428870, recall 0.660438, f_score 0.520041\n",
    ">>> step 8600\n",
    "step 8500-8599, precision 0.427618, recall 0.660438, f_score 0.519119\n",
    ">>> step 8700\n",
    "step 8600-8699, precision 0.424528, recall 0.655478, f_score 0.515310\n",
    ">>> step 8800\n",
    "step 8700-8799, precision 0.421074, recall 0.655478, f_score 0.512757\n",
    ">>> step 8900\n",
    "step 8800-8899, precision 0.420032, recall 0.650814, f_score 0.510555\n",
    ">>> step 9000\n",
    "step 8900-8999, precision 0.415667, recall 0.650814, f_score 0.507317\n",
    ">>> step 9100\n",
    "step 9000-9099, precision 0.414201, recall 0.646154, f_score 0.504808\n",
    ">>> step 9200\n",
    "step 9100-9199, precision 0.414510, recall 0.642944, f_score 0.504053\n",
    ">>> step 9300\n",
    "step 9200-9299, precision 0.413793, recall 0.639138, f_score 0.502352\n",
    ">>> step 9400\n",
    "step 9300-9399, precision 0.408101, recall 0.639138, f_score 0.498134\n",
    ">>> step 9500\n",
    "step 9400-9499, precision 0.407789, recall 0.639138, f_score 0.497902\n",
    ">>> step 9600\n",
    "step 9500-9599, precision 0.408248, recall 0.639218, f_score 0.498268\n",
    ">>> step 9700\n",
    "step 9600-9699, precision 0.406097, recall 0.639218, f_score 0.496663\n",
    ">>> step 9800\n",
    "step 9700-9799, precision 0.408482, recall 0.639487, f_score 0.498524\n",
    ">>> step 9900\n",
    "step 9800-9899, precision 0.407298, recall 0.639098, f_score 0.497524\n",
    ">>> step 10000\n",
    "step 9900-9999, precision 0.406250, recall 0.639098, f_score 0.496741\n",
    ">>> step 10100\n",
    "step 10000-10099, precision 0.407407, recall 0.636312, f_score 0.496758\n",
    ">>> step 10200\n",
    "step 10100-10199, precision 0.408892, recall 0.636415, f_score 0.497892\n",
    ">>> step 10300\n",
    "step 10200-10299, precision 0.407852, recall 0.636415, f_score 0.497120\n",
    ">>> step 10400\n",
    "step 10300-10399, precision 0.408796, recall 0.637079, f_score 0.498024\n",
    ">>> step 10500\n",
    "step 10400-10499, precision 0.406452, recall 0.637079, f_score 0.496280\n",
    ">>> step 10600\n",
    "step 10500-10599, precision 0.405540, recall 0.637989, f_score 0.495875\n",
    ">>> step 10700\n",
    "step 10600-10699, precision 0.403583, recall 0.636918, f_score 0.494087\n",
    ">>> step 10800\n",
    "step 10700-10799, precision 0.401748, recall 0.636918, f_score 0.492710\n",
    ">>> step 10900\n",
    "step 10800-10899, precision 0.402490, recall 0.636066, f_score 0.493011\n",
    ">>> step 11000\n",
    "step 10900-10999, precision 0.400619, recall 0.633496, f_score 0.490836\n",
    ">>> step 11100\n",
    "step 11000-11099, precision 0.396798, recall 0.633496, f_score 0.487958\n",
    ">>> step 11200\n",
    "step 11100-11199, precision 0.396944, recall 0.629510, f_score 0.486880\n",
    ">>> step 11300\n",
    "step 11200-11299, precision 0.397928, recall 0.628496, f_score 0.487316\n",
    ">>> step 11400\n",
    "step 11300-11399, precision 0.398474, recall 0.625521, f_score 0.486826\n",
    ">>> step 11500\n",
    "step 11400-11499, precision 0.397233, recall 0.620690, f_score 0.484435\n",
    ">>> step 11600\n",
    "step 11500-11599, precision 0.396189, recall 0.620690, f_score 0.483658\n",
    ">>> step 11700\n",
    "step 11600-11699, precision 0.397906, recall 0.620725, f_score 0.484945\n",
    ">>> step 11800\n",
    "step 11700-11799, precision 0.397856, recall 0.618999, f_score 0.484381\n",
    ">>> step 11900\n",
    "step 11800-11899, precision 0.396184, recall 0.618999, f_score 0.483139\n",
    ">>> step 12000\n",
    "step 11900-11999, precision 0.394779, recall 0.618999, f_score 0.482094\n",
    ">>> step 12100\n",
    "step 12000-12099, precision 0.397894, recall 0.619473, f_score 0.484554\n",
    ">>> step 12200\n",
    "step 12100-12199, precision 0.402704, recall 0.621242, f_score 0.488652\n",
    ">>> step 12300\n",
    "step 12200-12299, precision 0.408271, recall 0.618464, f_score 0.491852\n",
    ">>> step 12400\n",
    "step 12300-12399, precision 0.411149, recall 0.615207, f_score 0.492893\n",
    ">>> step 12500\n",
    "step 12400-12499, precision 0.414962, recall 0.612991, f_score 0.494902\n",
    ">>> step 12600\n",
    "step 12500-12599, precision 0.416262, recall 0.606275, f_score 0.493614\n",
    ">>> step 12700\n",
    "step 12600-12699, precision 0.419520, recall 0.603456, f_score 0.494951\n",
    ">>> step 12800\n",
    "step 12700-12799, precision 0.418013, recall 0.603456, f_score 0.493901\n",
    ">>> step 12900\n",
    "step 12800-12899, precision 0.417014, recall 0.600943, f_score 0.492362\n",
    ">>> step 13000\n",
    "step 12900-12999, precision 0.412596, recall 0.600943, f_score 0.489269\n",
    ">>> step 13100\n",
    "step 13000-13099, precision 0.410480, recall 0.599745, f_score 0.487383\n",
    ">>> step 13200\n",
    "step 13100-13199, precision 0.408577, recall 0.599745, f_score 0.486039\n",
    ">>> step 13300\n",
    "step 13200-13299, precision 0.407397, recall 0.599745, f_score 0.485203\n",
    ">>> step 13400\n",
    "step 13300-13399, precision 0.406178, recall 0.599409, f_score 0.484228\n",
    ">>> step 13500\n",
    "step 13400-13499, precision 0.401925, recall 0.599409, f_score 0.481193\n",
    ">>> step 13600\n",
    "step 13500-13599, precision 0.401584, recall 0.599409, f_score 0.480948\n",
    ">>> step 13700\n",
    "step 13600-13699, precision 0.400391, recall 0.600335, f_score 0.480389\n",
    ">>> step 13800\n",
    "step 13700-13799, precision 0.399224, recall 0.598753, f_score 0.479042\n",
    ">>> step 13900\n",
    "step 13800-13899, precision 0.395553, recall 0.598919, f_score 0.476442\n",
    ">>> step 14000\n",
    "step 13900-13999, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14100\n",
    "step 14000-14099, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14200\n",
    "step 14100-14199, precision 0.392741, recall 0.598432, f_score 0.474244\n",
    ">>> step 14300\n",
    "step 14200-14299, precision 0.393562, recall 0.597536, f_score 0.474560\n",
    ">>> step 14400\n",
    "step 14300-14399, precision 0.390675, recall 0.594374, f_score 0.471463\n",
    ">>> step 14500\n",
    "step 14400-14499, precision 0.389944, recall 0.592924, f_score 0.470474\n",
    ">>> step 14600\n",
    "step 14500-14599, precision 0.386943, recall 0.592924, f_score 0.468283\n",
    ">>> step 14700\n",
    "step 14600-14699, precision 0.386532, recall 0.592924, f_score 0.467983\n",
    ">>> step 14800\n",
    "step 14700-14799, precision 0.387326, recall 0.595152, f_score 0.469258\n",
    ">>> step 14900\n",
    "step 14800-14899, precision 0.386903, recall 0.595343, f_score 0.469007\n",
    ">>> step 15000\n",
    "step 14900-14999, precision 0.388658, recall 0.595694, f_score 0.470403\n",
    ">>> step 15100\n",
    "step 15000-15099, precision 0.387264, recall 0.595304, f_score 0.469260\n",
    ">>> step 15200\n",
    "step 15100-15199, precision 0.386923, recall 0.596679, f_score 0.469435\n",
    ">>> step 15300\n",
    "step 15200-15299, precision 0.387583, recall 0.597950, f_score 0.470315\n",
    ">>> step 15400\n",
    "step 15300-15399, precision 0.384228, recall 0.599214, f_score 0.468222\n",
    ">>> step 15500\n",
    "step 15400-15499, precision 0.383070, recall 0.599214, f_score 0.467361\n",
    ">>> step 15600\n",
    "step 15500-15599, precision 0.384500, recall 0.599143, f_score 0.468403\n",
    ">>> step 15700\n",
    "step 15600-15699, precision 0.386607, recall 0.601239, f_score 0.470606\n",
    ">>> step 15800\n",
    "step 15700-15799, precision 0.382607, recall 0.601239, f_score 0.467630\n",
    ">>> step 15900\n",
    "step 15800-15899, precision 0.382626, recall 0.603309, f_score 0.468269\n",
    ">>> step 16000\n",
    "step 15900-15999, precision 0.383248, recall 0.603296, f_score 0.468731\n",
    ">>> step 16100\n",
    "step 16000-16099, precision 0.382317, recall 0.603296, f_score 0.468034\n",
    ">>> step 16200\n",
    "step 16100-16199, precision 0.381946, recall 0.603296, f_score 0.467756\n",
    ">>> step 16300\n",
    "step 16200-16299, precision 0.379460, recall 0.603296, f_score 0.465887\n",
    ">>> step 16400\n",
    "step 16300-16399, precision 0.382472, recall 0.604669, f_score 0.468563\n",
    ">>> step 16500\n",
    "step 16400-16499, precision 0.385124, recall 0.605644, f_score 0.470843\n",
    ">>> step 16600\n",
    "step 16500-16599, precision 0.386226, recall 0.609174, f_score 0.472732\n",
    ">>> step 16700\n",
    "step 16600-16699, precision 0.385509, recall 0.609174, f_score 0.472195\n",
    ">>> step 16800\n",
    "step 16700-16799, precision 0.387558, recall 0.609641, f_score 0.473870\n",
    ">>> step 16900\n",
    "step 16800-16899, precision 0.389587, recall 0.611689, f_score 0.476004\n",
    ">>> step 17000\n",
    "step 16900-16999, precision 0.392534, recall 0.612858, f_score 0.478555\n",
    ">>> step 17100\n",
    "step 17000-17099, precision 0.393763, recall 0.611711, f_score 0.479115\n",
    ">>> step 17200\n",
    "step 17100-17199, precision 0.392442, recall 0.611711, f_score 0.478136\n",
    ">>> step 17300\n",
    "step 17200-17299, precision 0.394895, recall 0.614296, f_score 0.480746\n",
    ">>> step 17400\n",
    "step 17300-17399, precision 0.397101, recall 0.616644, f_score 0.483100\n",
    ">>> step 17500\n",
    "step 17400-17499, precision 0.401132, recall 0.617085, f_score 0.486208\n",
    ">>> step 17600\n",
    "step 17500-17599, precision 0.399220, recall 0.617085, f_score 0.484801\n",
    ">>> step 17700\n",
    "step 17600-17699, precision 0.400259, recall 0.617539, f_score 0.485707\n",
    ">>> step 17800\n",
    "step 17700-17799, precision 0.402402, recall 0.618734, f_score 0.487653\n",
    ">>> step 17900\n",
    "step 17800-17899, precision 0.403036, recall 0.617830, f_score 0.487836\n",
    ">>> step 18000\n",
    "step 17900-17999, precision 0.404838, recall 0.618076, f_score 0.489231\n",
    ">>> step 18100\n",
    "step 18000-18099, precision 0.407634, recall 0.618758, f_score 0.491482\n",
    ">>> step 18200\n",
    "step 18100-18199, precision 0.406473, recall 0.618682, f_score 0.490614\n",
    ">>> step 18300\n",
    "step 18200-18299, precision 0.407004, recall 0.619139, f_score 0.491144\n",
    ">>> step 18400\n",
    "step 18300-18399, precision 0.406982, recall 0.619472, f_score 0.491232\n",
    ">>> step 18500\n",
    "step 18400-18499, precision 0.405737, recall 0.619093, f_score 0.490206\n",
    ">>> step 18600\n",
    "step 18500-18599, precision 0.404076, recall 0.620025, f_score 0.489282\n",
    ">>> step 18700\n",
    "step 18600-18699, precision 0.401760, recall 0.620025, f_score 0.487581\n",
    ">>> step 18800\n",
    "step 18700-18799, precision 0.399308, recall 0.620025, f_score 0.485771\n",
    ">>> step 18900\n",
    "step 18800-18899, precision 0.398062, recall 0.620321, f_score 0.484938\n",
    ">>> step 19000\n",
    "step 18900-18999, precision 0.396462, recall 0.620321, f_score 0.483748\n",
    ">>> step 19100\n",
    "step 19000-19099, precision 0.396994, recall 0.620614, f_score 0.484234\n",
    ">>> step 19200\n",
    "step 19100-19199, precision 0.395224, recall 0.620431, f_score 0.482859\n",
    ">>> step 19300\n",
    "step 19200-19299, precision 0.393585, recall 0.620281, f_score 0.481589\n",
    ">>> step 19400\n",
    "step 19300-19399, precision 0.393508, recall 0.620281, f_score 0.481531\n",
    ">>> step 19500\n",
    "step 19400-19499, precision 0.393243, recall 0.621160, f_score 0.481597\n",
    ">>> step 19600\n",
    "step 19500-19599, precision 0.392549, recall 0.621160, f_score 0.481077\n",
    ">>> step 19700\n",
    "step 19600-19699, precision 0.392088, recall 0.621160, f_score 0.480730\n",
    ">>> step 19800\n",
    "step 19700-19799, precision 0.391704, recall 0.621160, f_score 0.480442\n",
    ">>> step 19900\n",
    "step 19800-19899, precision 0.391016, recall 0.621160, f_score 0.479923\n",
    ">>> step 20000\n",
    "step 19900-19999, precision 0.390358, recall 0.620519, f_score 0.479236\n",
    ">>> step 20100\n",
    "step 20000-20099, precision 0.390504, recall 0.620764, f_score 0.479419\n",
    ">>> step 20200\n",
    "step 20100-20199, precision 0.386887, recall 0.621114, f_score 0.476787\n",
    ">>> step 20300\n",
    "step 20200-20299, precision 0.384308, recall 0.621114, f_score 0.474824\n",
    ">>> step 20400\n",
    "step 20300-20399, precision 0.384161, recall 0.621114, f_score 0.474712\n",
    ">>> step 20500\n",
    "step 20400-20499, precision 0.383577, recall 0.621114, f_score 0.474266\n",
    ">>> step 20600\n",
    "step 20500-20599, precision 0.383660, recall 0.622861, f_score 0.474837\n",
    ">>> step 20700\n",
    "step 20600-20699, precision 0.383569, recall 0.624886, f_score 0.475355\n",
    ">>> step 20800\n",
    "step 20700-20799, precision 0.382785, recall 0.624886, f_score 0.474752\n",
    ">>> step 20900\n",
    "step 20800-20899, precision 0.384644, recall 0.627704, f_score 0.476995\n",
    ">>> step 21000\n",
    "step 20900-20999, precision 0.383092, recall 0.627704, f_score 0.475800\n",
    ">>> step 21100\n",
    "step 21000-21099, precision 0.383464, recall 0.626977, f_score 0.475878\n",
    ">>> step 21200\n",
    "step 21100-21199, precision 0.383045, recall 0.626977, f_score 0.475555\n",
    ">>> step 21300\n",
    "step 21200-21299, precision 0.383430, recall 0.626852, f_score 0.475816\n",
    ">>> step 21400\n",
    "step 21300-21399, precision 0.384685, recall 0.628119, f_score 0.477146\n",
    ">>> step 21500\n",
    "step 21400-21499, precision 0.383581, recall 0.628119, f_score 0.476296\n",
    ">>> step 21600\n",
    "step 21500-21599, precision 0.384506, recall 0.629446, f_score 0.477391\n",
    ">>> step 21700\n",
    "step 21600-21699, precision 0.382462, recall 0.629446, f_score 0.475813\n",
    ">>> step 21800\n",
    "step 21700-21799, precision 0.382716, recall 0.628439, f_score 0.475721\n",
    ">>> step 21900\n",
    "step 21800-21899, precision 0.380835, recall 0.628439, f_score 0.474265\n",
    ">>> step 22000\n",
    "step 21900-21999, precision 0.380235, recall 0.628439, f_score 0.473799\n",
    ">>> step 22100\n",
    "step 22000-22099, precision 0.380420, recall 0.629084, f_score 0.474126\n",
    ">>> step 22200\n",
    "step 22100-22199, precision 0.378838, recall 0.629758, f_score 0.473086\n",
    ">>> step 22300\n",
    "step 22200-22299, precision 0.378196, recall 0.628481, f_score 0.472225\n",
    ">>> step 22400\n",
    "step 22300-22399, precision 0.377388, recall 0.627647, f_score 0.471359\n",
    ">>> step 22500\n",
    "step 22400-22499, precision 0.377378, recall 0.626815, f_score 0.471117\n",
    ">>> step 22600\n",
    "step 22500-22599, precision 0.377937, recall 0.628183, f_score 0.471939\n",
    ">>> step 22700\n",
    "step 22600-22699, precision 0.376335, recall 0.628183, f_score 0.470688\n",
    ">>> step 22800\n",
    "step 22700-22799, precision 0.376016, recall 0.628183, f_score 0.470439\n",
    ">>> step 22900\n",
    "step 22800-22899, precision 0.375968, recall 0.629473, f_score 0.470762\n",
    ">>> step 23000\n",
    "step 22900-22999, precision 0.375210, recall 0.629473, f_score 0.470167\n",
    ">>> step 23100\n",
    "step 23000-23099, precision 0.374708, recall 0.630191, f_score 0.469973\n",
    ">>> step 23200\n",
    "step 23100-23199, precision 0.376144, recall 0.631564, f_score 0.471484\n",
    ">>> step 23300\n",
    "step 23200-23299, precision 0.376630, recall 0.632834, f_score 0.472219\n",
    ">>> step 23400\n",
    "step 23300-23399, precision 0.375021, recall 0.632834, f_score 0.470952\n",
    ">>> step 23500\n",
    "step 23400-23499, precision 0.374286, recall 0.633527, f_score 0.470564\n",
    ">>> step 23600\n",
    "step 23500-23599, precision 0.374068, recall 0.634763, f_score 0.470732\n",
    ">>> step 23700\n",
    "step 23600-23699, precision 0.373814, recall 0.635494, f_score 0.470731\n",
    ">>> step 23800\n",
    "step 23700-23799, precision 0.372496, recall 0.635494, f_score 0.469685\n",
    ">>> step 23900\n",
    "step 23800-23899, precision 0.372906, recall 0.635918, f_score 0.470127\n",
    ">>> step 24000\n",
    "step 23900-23999, precision 0.372976, recall 0.634865, f_score 0.469894\n",
    ">>> step 24100\n",
    "step 24000-24099, precision 0.373713, recall 0.634750, f_score 0.470448\n",
    ">>> step 24200\n",
    "step 24100-24199, precision 0.373123, recall 0.634750, f_score 0.469979\n",
    ">>> step 24300\n",
    "step 24200-24299, precision 0.374528, recall 0.635369, f_score 0.471263\n",
    ">>> step 24400\n",
    "step 24300-24399, precision 0.374765, recall 0.635397, f_score 0.471458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test %.2f\"%0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(len(cubes_set)):\n",
    "    if (cubes_set[i][\"file\"]) == \"PX303-Fg001-V-C01-R01_TEAR_8X5_PIECE_2X4\":\n",
    "        plt.imshow(cubes_set[i][\"cube\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(len(cubes_set)):\n",
    "    if (cubes_set[i][\"file\"]) == \"PX303-Fg001-V-C01-R01_TEAR_8X5_PIECE_4X4\":\n",
    "        plt.imshow(cubes_set[i][\"cube\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# When trained on a ratio of 1:2 non-match:match - and tested on the same the results are excellend\n",
    "*** MATCHED=19065\n",
    "*** NOT MATCHED=8183\n",
    "*** DISCARDED=1979027\n",
    "TOTAL 27300, precision 0.815258, recall 0.979806, f_score 0.889990\n",
    "TOTAL true = 19065\n",
    "TOTAL error rate = 0.169158\n",
    "\n",
    "# When trained on a ratio of 1:2 non-match:match - and tested on 6:1 (much more non-match) the results are poor\n",
    "# coverage is still excellent but precision is bad - means a lot of false-positive\n",
    "# this usually implies that we'll get a lot of trash in the validation set\n",
    "# hence next attempt will be to train on 6:1 and see it the test is better\n",
    "*** MATCHED=19065\n",
    "*** NOT MATCHED=125210\n",
    "*** DISCARDED=1862000\n",
    "precision 0.237849, recall 0.973684, f_score 0.382309\n",
    "\n",
    "# When trained on a ratio of 9:1 non-match:match - and tested on 6:1 the results are poor\n",
    "# coverage is around 70% and precision is 35 - means a lot of false-positive\n",
    "# this usually implies that we'll get a lot of trash in the validation set\n",
    "# hence next attempt will be to train on 6:1 and see it the test is better\n",
    "*** MATCHED=19065\n",
    "*** NOT MATCHED=125210\n",
    "*** DISCARDED=1862000\n",
    "precision 0.355357, recall 0.737720, f_score 0.479663\n",
    "\n",
    "\n",
    "# Test from 0ct-07\n",
    "=== total 144000 match 62339\n",
    "TOTAL 144000, precision 0.286081, recall 0.952518, f_score 0.440009\n",
    "TOTAL true = 18723\n",
    "TOTAL error rate = 0.315236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#debug\n",
    "\n",
    "short_name = \"PX303-Fg006-V-C02-R02\"\n",
    "# image = read_and_crop(img_name, x_start, x_end, y_start, y_end)\n",
    "\n",
    "cubes_set = []\n",
    "col_cut = 3\n",
    "row_cut = 2\n",
    "pieces = []\n",
    "piece = {}\n",
    "image = img.imread(ROOT_FOLDER + \"test_fragments/PX303Fg006_3X2_1X1.jpg\")\n",
    "image = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "piece[\"cut\"] = image[:,:,0]\n",
    "piece[\"col\"] = 1\n",
    "piece[\"row\"] = 1\n",
    "piece[\"col_px\"] = 0\n",
    "piece[\"row_px\"] = 0\n",
    "pieces.append(piece)\n",
    "piece = {}\n",
    "image = img.imread(ROOT_FOLDER + \"test_fragments/PX303Fg006_3X2_2X1.jpg\")\n",
    "image = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "piece[\"cut\"] = image[:,:,0]\n",
    "piece[\"col\"] = 2\n",
    "piece[\"row\"] = 1\n",
    "piece[\"col_px\"] = 2300\n",
    "piece[\"row_px\"] = 0\n",
    "pieces.append(piece)\n",
    "\n",
    "for piece in pieces:\n",
    "    # print(\"PRE_PROCESS:::\"+\"PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"]))\n",
    "    fragment_name = short_name + \"_TEAR_\"+str(col_cut)+\"X\"+str(row_cut)+\"_PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "    fragment_file_name = short_name + \"_\"+str(col_cut)+\"X\"+str(row_cut)+\"_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "    cubes = VAL_slice_TEAR_to_static_slices(fragment_name, piece)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    for cube in cubes:\n",
    "        cube[\"tear\"] = str(col_cut)+\"X\"+str(row_cut)\n",
    "        cube[\"piece_col\"] = piece[\"col\"]\n",
    "        cube[\"piece_row\"] = piece[\"row\"]\n",
    "    # print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "    cubes_set.extend(cubes)\n",
    "    \n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 1, 7)\n",
    "import pdb; pdb.set_trace()    \n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, is_enriched, ROOT_FOLDER + \"model_binary/tear_model3.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T10:32:29.152185Z",
     "start_time": "2019-04-03T09:38:59.768856Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#TRAIN 3\n",
    "cubes_set = pre_process_training(\"PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400, max_cols=8, max_rows=4)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 2, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)    \n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T13:33:46.532102Z",
     "start_time": "2019-04-03T12:32:32.371972Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#TRAIN 2\n",
    "cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", \\\n",
    "                                 100, -1, 400, -1, max_cols=8, max_rows=4)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 2, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)\n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model2.ckpt\", ROOT_FOLDER + \"models/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T13:49:29.331063Z",
     "start_time": "2019-04-03T13:34:34.742578Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# validate 1\n",
    "cubes_set = pre_process_training(\"PX303-Fg001-V-C01-R01-D05032015-T112520-ML638__006.jpg\", max_cols=8, max_rows=4)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "  NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 1, 5)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, is_enriched, ROOT_FOLDER + \"models/tear_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############2_3_val_1\n",
    "=== total 91300 match 12738 /// 1,5 => 1,5\n",
    "TOTAL 91300, precision 0.475899, recall 0.349899, f_score 0.403286\n",
    "TOTAL true = 17325\n",
    "TOTAL error rate = 0.196484\n",
    "=== total 91900 match 12457 /// 1,7 => 1,5\n",
    "TOTAL 91900, precision 0.511760, recall 0.364765, f_score 0.425937\n",
    "TOTAL true = 17477\n",
    "TOTAL error rate = 0.186986\n",
    "=== total 91000 match 20063 /// 2,7 => 1,5\n",
    "TOTAL 91000, precision 0.516672, recall 0.600266, f_score 0.555341\n",
    "TOTAL true = 17269\n",
    "TOTAL error rate = 0.182418\n",
    "=== total 91100 match 19002 /// 3,7 => 1,5\n",
    "TOTAL 91100, precision 0.501474, recall 0.552022, f_score 0.525535\n",
    "TOTAL true = 17262\n",
    "TOTAL error rate = 0.188869\n",
    "=== total 90800 match 15016 /// 4,7 => 1,5\n",
    "TOTAL 90800, precision 0.472030, recall 0.416280, f_score 0.442406\n",
    "TOTAL true = 17027\n",
    "TOTAL error rate = 0.196773\n",
    "=== total 91400 match 23705 /// 2,7 => 1,5\n",
    "TOTAL 91400, precision 0.453449, recall 0.622842, f_score 0.524815\n",
    "TOTAL true = 17258\n",
    "TOTAL error rate = 0.212965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############1_2_val_3\n",
    "=== total 536900 match 120543 ## previously - not sure which params\n",
    "TOTAL 536900, precision 0.348216, recall 0.684915, f_score 0.461700\n",
    "TOTAL true = 61285\n",
    "TOTAL error rate = 0.182302\n",
    "=== total 538600 match 208051 /// 2,7 => 1,5\n",
    "TOTAL 538600, precision 0.279508, recall 0.944471, f_score 0.431359\n",
    "TOTAL true = 61571\n",
    "TOTAL error rate = 0.284660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############1_3_val_2\n",
    "=== total 329000 match 111791 /// 2,7 => 1,5\n",
    "TOTAL 329000, precision 0.344285, recall 0.757131, f_score 0.473334\n",
    "TOTAL true = 50834\n",
    "TOTAL error rate = 0.260331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:47:01.403854Z",
     "start_time": "2019-04-03T19:52:46.394114Z"
    }
   },
   "outputs": [],
   "source": [
    "#TRAIN 3\n",
    "cubes_set = pre_process_training(\"PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400, max_cols=8, max_rows=4)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 2, 7)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)    \n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model2.ckpt\", ROOT_FOLDER + \"models/tear_model1.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T05:24:20.228262Z",
     "start_time": "2019-04-04T04:18:30.422157Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate 2\n",
    "cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", max_cols=8, max_rows=4)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta, is_enriched = \\\n",
    "  NEW_build_train_set_for_binary_labeling(cubes_set, cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 1, 5)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, is_enriched, ROOT_FOLDER + \"models/tear_model2.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T05:53:22.125080Z",
     "start_time": "2019-04-04T05:34:40.304989Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_tf_deep(250)    \n",
    "train(train_imgs, train_lbls, ROOT_FOLDER + \"models/tear_model3.ckpt\", ROOT_FOLDER + \"models/tear_model2.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:50:30.106916Z",
     "start_time": "2019-10-02T11:50:30.100682Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-3-898be71fcaa8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-898be71fcaa8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir='/media/1KGB_ILAN/papyrus/models'\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir='/media/1KGB_ILAN/papyrus/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
