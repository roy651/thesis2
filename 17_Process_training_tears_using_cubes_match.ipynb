{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# RUN Main import block and TODO list\n",
    "\n",
    "# TODO: see how uri calculated the ridges\n",
    "\n",
    "# TODO: Perform Histogram equalization - start with it\n",
    "# TODO: \n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph \n",
    "# This will be the peakness measure for the PSD ==> The desired ridge index\n",
    "# TODO:\n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph - it's the peakness measure for the PSD\n",
    "# must select a peak above a min threshold in order to ignore noisy frequency\n",
    "# must ignore peaks above a certain threshold in order to detect meaningful frequency\n",
    "# run the PSD in moving windows every 200 px (deduced from the below PSD pointing to a freq of 1/0.02=50-> times 4= 200px)\n",
    "# and medianf the result of the windows\n",
    "# TODO:\n",
    "# Another alternative: (with Yariv)\n",
    "# Run PSD column by column - get the phase, freq, peakness and reconstruct an artificial ridge slice\n",
    "# from this - reconstruct a \"clean\" artificial ridge image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "#import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import mahotas as mh\n",
    "from mahotas import polygon\n",
    "# import pymorph as pm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "import skimage.transform as transform\n",
    "import skimage.morphology as mp\n",
    "import skimage.io as sio\n",
    "import scipy.misc as sm\n",
    "from skimage.filters import threshold_otsu, threshold_adaptive\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "from skimage import exposure\n",
    "from skimage import data, img_as_float\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from bisect import bisect_left\n",
    "import math\n",
    "import warnings\n",
    "import csv\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     56,
     70,
     76,
     82,
     96,
     105,
     120,
     138,
     155,
     389
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Utility functions\n",
    "\n",
    "# One time init\n",
    "# with open('results.csv', 'w') as csvfile:\n",
    "#     csvout = csv.writer(csvfile)\n",
    "#     csvout.writerow([\"File\", \"Model\", \"Gap\", \"Slice_size\", \"Count\", \"Precision\", \"Recall\", \"F-score\", \"True Count\", \"Error Rate\"])\n",
    "\n",
    "#BASIC CROP FRAME\n",
    "X_START = 1000\n",
    "X_END = 6000\n",
    "Y_START = 800\n",
    "Y_END = 4300\n",
    "BG_2_OBJ_RATIO = 0.91\n",
    "CUBE_SIZE = 250\n",
    "EDGE_GAP = 50\n",
    "ROOT_FOLDER = \"/home/il239838/files/\"\n",
    "# ROOT_FOLDER = \"/Users/il239838/Downloads/private/Thesis/Papyrus/PX303/files/\"\n",
    "LEARNING_RATE = 0.001\n",
    "BATCHES = 800\n",
    "BATCH_SIZE = 50\n",
    "BREAK_VAL = 1000\n",
    "\n",
    "# Simple crop by x/y ranges\n",
    "def crop(image, ymin, ymax, xmin, xmax):\n",
    "    return image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "# returns a logical matrix of values beyond a threshld\n",
    "def thresholded(image, val): \n",
    "    return np.logical_and(*[image[...] > val  for t in enumerate([0, 0])])\n",
    "\n",
    "def find_min_max_without_orphand_pixels(nonzero_dimension, crop_filter=20):\n",
    "    sorted = np.sort(nonzero_dimension)\n",
    "    prev=-1\n",
    "    min_val = sorted[0]\n",
    "    for i, x in enumerate(sorted[:100]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            min_val = x\n",
    "        prev = x\n",
    "    prev=-1\n",
    "    max_val = sorted[-1]\n",
    "    for i, x in enumerate(sorted[-100:]):\n",
    "        if prev >= 0 and x - prev > crop_filter:\n",
    "            max_val = prev\n",
    "            break\n",
    "        prev = x\n",
    "    \n",
    "    return min_val, max_val\n",
    "\n",
    "def calc_min_max_coordinates(image, crop_val=50):\n",
    "    temp = thresholded(image, crop_val)\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "def calc_min_max_coordinates_dynamic(image, cutoff=1):\n",
    "    temp = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    flat = np.sort(np.matrix.getA1(temp))\n",
    "    sum_all = np.sum(flat)\n",
    "    index = np.argmin(flat.cumsum() < (sum_all * cutoff))\n",
    "\n",
    "    temp = thresholded(temp, flat[index])\n",
    "    temp = temp * 1\n",
    "    temp = np.nonzero(temp)\n",
    "    ymin, ymax = find_min_max_without_orphand_pixels(temp[0])\n",
    "    xmin,xmax = find_min_max_without_orphand_pixels(temp[1])\n",
    "    return ymin, ymax, xmin, xmax\n",
    "\n",
    "# initial static crop and a seondary dynamic crop based on signal2noise ratio\n",
    "def crop_full_scan(image, x_start, x_end, y_start, y_end):\n",
    "    temp = crop(image, y_start, y_end, x_start, x_end)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(temp, cutoff=BG_2_OBJ_RATIO)\n",
    "    temp = crop(image, y_start+ymin, y_start+ymax, x_start+xmin, x_start+xmax)\n",
    "    return temp\n",
    "\n",
    "def crop_thresholded(image):\n",
    "    temp = crop(image, 0, image.shape[0]-1, 0, image.shape[1]-1)\n",
    "    ymin, ymax, xmin, xmax = calc_min_max_coordinates(temp)\n",
    "    temp = crop(image, ymin, ymax, xmin, xmax)\n",
    "    return temp\n",
    "\n",
    "def read_and_crop(image_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END):\n",
    "    if \"il239838\" in os.getcwd():\n",
    "        image = img.imread(ROOT_FOLDER + image_name)\n",
    "    else:\n",
    "        f = urllib.request.urlopen(\"https://dl.dropboxusercontent.com/s/31b96942qdcn73k/\" + image_name)\n",
    "        image = img.imread(f, format='jpeg')\n",
    "\n",
    "    # Smart-crop the image to get rid of all the noise and redundant area\n",
    "    # return crop_full_scan(image)\n",
    "    cropped = crop_full_scan(image, x_start, x_end, y_start, y_end)\n",
    "    return exposure.equalize_adapthist(cropped, clip_limit=0.03)\n",
    "\n",
    "\n",
    "# TODO: fix performance!!! http://scikit-image.org/docs/dev/user_guide/tutorial_parallelization.html\n",
    "def combine_3_images_to_RGB(red, green, blue):\n",
    "    new_image = np.empty((blue.shape[0],blue.shape[1],3))\n",
    "    for x in range(0, blue.shape[0]):\n",
    "        for y in range(0, blue.shape[1]):\n",
    "            new_image[x,y,0] = red[x,y]\n",
    "            new_image[x,y,1] = green[x,y]\n",
    "            new_image[x,y,2] = blue[x,y]\n",
    "    return new_image\n",
    "\n",
    "def slice_image_left_edge(original, width=200, rotate=0):\n",
    "    rot = ndimage.rotate(original, rotate)\n",
    "    # Slice the left slice of the so-called \"blue\" image\n",
    "    left_edge_orig = crop(rot, 1, 1400, 1, width)\n",
    "    left_edge_orig = crop_thresholded(left_edge_orig)\n",
    "\n",
    "    # Copy to a new array so we don't thrash the origin\n",
    "    left_edge = np.empty_like (left_edge_orig)\n",
    "    np.copyto(left_edge, left_edge_orig)\n",
    "\n",
    "    # Zero down low level \"noise\" values\n",
    "    low_values_indices = left_edge < 30  # Where values are low\n",
    "    left_edge[low_values_indices] = 0  # All low values set to 0\n",
    "    return left_edge\n",
    "\n",
    "def get_best_angle_rotation(original, crop=True, width=200):\n",
    "    min_var = 99999999999\n",
    "    best_angle = -10\n",
    "    for x in range(-5,5):\n",
    "        if crop:            \n",
    "            rot_edge = slice_image_left_edge(original, width, x)\n",
    "        else:\n",
    "            rot_edge = ndimage.rotate(original, x)\n",
    "        left_var = np.var(rot_edge, axis=1)\n",
    "        # left_var = np.apply_along_axis(lambda v: np.var(v[np.nonzero(v)]), 1, rot_edge)\n",
    "        var_sum = np.sum(left_var)\n",
    "        if (var_sum < min_var):\n",
    "            min_var = var_sum\n",
    "            best_angle = x\n",
    "    print (\"best_angle=\"+str(best_angle))\n",
    "    return best_angle\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "def calc_neighbors(slice_map, col, row):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if ((col-1, row) in slice_map and slice_map[(col-1, row)] != None):\n",
    "        slice_map[(col, row)][\"left\"] = slice_map[(col-1, row)]\n",
    "        slice_map[(col-1, row)][\"right\"] = slice_map[(col, row)]\n",
    "    if ((col+1, row) in slice_map and slice_map[(col+1, row)] != None):\n",
    "        slice_map[(col, row)][\"right\"] = slice_map[(col+1, row)]\n",
    "        slice_map[(col+1, row)][\"left\"] = slice_map[(col, row)]\n",
    "    if ((col, row-1) in slice_map and slice_map[(col, row-1)] != None):\n",
    "        slice_map[(col, row)][\"top\"] = slice_map[(col, row-1)]\n",
    "        slice_map[(col, row-1)][\"bottom\"] = slice_map[(col, row)]\n",
    "    if ((col, row+1) in slice_map and slice_map[(col, row+1)] != None):\n",
    "        slice_map[(col, row)][\"bottom\"] = slice_map[(col, row+1)]\n",
    "        slice_map[(col, row+1)][\"top\"] = slice_map[(col, row)]\n",
    "    \n",
    "\n",
    "\n",
    "def VAL_create_cube(name, raw, x, y):\n",
    "    cube = {}\n",
    "    cube[\"cube\"] = raw\n",
    "    cube[\"file\"] = name\n",
    "    #     if name.find('P') == 0:\n",
    "    #         cube[\"index\"] = int(name[name.find('P')+1:name.find('P')+4]) * 1000 + int(name[name.find('Fg')+2:name.find('Fg')+5])\n",
    "    #     else:\n",
    "    # print(\"Found a ZERO index cube with the name:\"+name)\n",
    "    cube[\"index\"] = 0\n",
    "    cube[\"top_row\"] = x\n",
    "    cube[\"left_col\"] = y\n",
    "    cube[\"right_col\"] = y + CUBE_SIZE\n",
    "    return cube\n",
    "    \n",
    "\n",
    "ZERO_CUBE = VAL_create_cube(\"ZERO\", np.zeros((CUBE_SIZE, CUBE_SIZE), dtype=np.int), -1, -2)\n",
    "\n",
    "# slice an image to cubes with 250X250 pixel size\n",
    "def VAL_slice_TEAR_to_static_slices(name, cropped_original):\n",
    "    structure = {}\n",
    "    # cropped_original = cropped_original / 256 # divide by 256 to \"normalize\" between 0 and 1\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    x, y = cropped_original[\"cut\"].shape\n",
    "    # print (x,y)\n",
    "    n = 0\n",
    "    # see n offset to see offset in pixels on the x axis == rows. every n equals CUBE_SIZE\n",
    "    while ((n + 1) * CUBE_SIZE < x):\n",
    "        # mark the piece as narrow so the first would be counted as lastt too\n",
    "        narrow = True if ((CUBE_SIZE + (4 * EDGE_GAP)) > y) else False\n",
    "        # cut a cube of 250X250 at the FIRST column\n",
    "        start_row_px = int(np.round(n * CUBE_SIZE, -1))\n",
    "        end_row_px = int(np.round((n + 1) * CUBE_SIZE, -1))\n",
    "        cube = (crop(cropped_original[\"cut\"], start_row_px, end_row_px, EDGE_GAP, CUBE_SIZE + EDGE_GAP))\n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, start_row_px, EDGE_GAP)\n",
    "            new_cube[\"col\"] = 0 # marks that the cube is on the first col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = narrow # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + CUBE_SIZE + EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + start_row_px\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + end_row_px\n",
    "            structure[(0, n)] = new_cube\n",
    "\n",
    "        # cut a cube of 250X250 at the LAST column\n",
    "        cube = (crop(cropped_original[\"cut\"], start_row_px, end_row_px, y - CUBE_SIZE - EDGE_GAP, y - EDGE_GAP))\n",
    "        # keep only cubes for which half of the pixels have some \"color\"\n",
    "        # aligned with the normalization 0.2 correlates to 50\n",
    "        if np.median(cube) > 0.2:\n",
    "            # keep the cube\n",
    "            new_cube = VAL_create_cube(name, cube, start_row_px, y - CUBE_SIZE - EDGE_GAP)\n",
    "            new_cube[\"col\"] = 1 # marks that the cube is on the last col of the piece\n",
    "            new_cube[\"row\"] = n\n",
    "            new_cube[\"last\"] = not narrow # like col - marks that the cube is on the last col of the piece\n",
    "            new_cube[\"orig\"] = cropped_original\n",
    "            new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + y - CUBE_SIZE - EDGE_GAP\n",
    "            new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + y - EDGE_GAP\n",
    "            new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + start_row_px\n",
    "            new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + end_row_px\n",
    "            structure[(1, n)] = new_cube\n",
    "\n",
    "    #         m = 0\n",
    "    #         # every 250 pixels on the y axis == cols\n",
    "    #         while ((m + 1) * CUBE_SIZE < y):            \n",
    "    #             if ((m == 0) or ((m + 2) * CUBE_SIZE >= y)): # Only keep the left and right edges of the piece for matching!!\n",
    "    #                 # cut a cube of 250X250\n",
    "    #                 cube = crop(cropped_original[\"cut\"], n * CUBE_SIZE, (n + 1) * CUBE_SIZE, m * CUBE_SIZE, (m + 1) * CUBE_SIZE)\n",
    "    #                 # keep only cubes for which half of the pixels have some \"color\"\n",
    "    #                 # print(np.median(cube))\n",
    "    #                 if np.median(cube) > 0.2: # aligned with the normalization 0.2 correlates to 50\n",
    "    #                     # keep the cube\n",
    "    #                     new_cube = VAL_create_cube(name, cube, n * CUBE_SIZE, m * CUBE_SIZE)\n",
    "    #                     new_cube[\"col\"] = m\n",
    "    #                     new_cube[\"row\"] = n\n",
    "    #                     new_cube[\"orig\"] = cropped_original\n",
    "    #                     new_cube[\"col_px_left\"] = cropped_original[\"col_px\"] + m * CUBE_SIZE\n",
    "    #                     new_cube[\"col_px_right\"] = cropped_original[\"col_px\"] + (m + 1) * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_top\"] = cropped_original[\"row_px\"] + n * CUBE_SIZE\n",
    "    #                     new_cube[\"row_px_bottom\"] = cropped_original[\"row_px\"] + (n + 1) * CUBE_SIZE\n",
    "    #                     if ((m + 2) * CUBE_SIZE >= y):\n",
    "    #                         new_cube[\"last\"] = True\n",
    "    #                     else:\n",
    "    #                         new_cube[\"last\"] = False\n",
    "    #                     structure[(m, n)] = new_cube\n",
    "    #             m += 1\n",
    "        n += 0.2 # currently set to jump in 50 px offset\n",
    "            \n",
    "    # this loop has to be performed only after we've established all the None cubes\n",
    "    for cube in structure.values():\n",
    "        # set the reference to neighbor cubes\n",
    "        if cube != None:\n",
    "            calc_neighbors(structure, cube[\"col\"], cube[\"row\"])\n",
    "\n",
    "    # return the data structure with all the cubes and the counters of the rows and columns\n",
    "    return structure.values()\n",
    "\n",
    "def pad_above(original, above, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount), above[\"cube\"][-amount:], axis=0)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE,CUBE_SIZE+amount), axis=0)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"] - amount, original[\"left_col\"])\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"]\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"]\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"] - amount\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"] - amount\n",
    "    return cube\n",
    "\n",
    "def pad_below(original, below, amount):\n",
    "    res = np.insert(original[\"cube\"], np.full(amount, CUBE_SIZE), below[\"cube\"][:amount], axis=0)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=0)\n",
    "    cube =  VAL_create_cube(original[\"file\"], res, original[\"top_row\"] + amount, original[\"left_col\"])\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"]\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"]\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"] + amount\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"] + amount\n",
    "    return cube\n",
    "    \n",
    "def pad_left(original, left, amount):\n",
    "    res = np.insert(original[\"cube\"], np.zeros(amount, dtype=int), left[\"cube\"][:,-amount:], axis=1)\n",
    "    res = np.delete(res, np.arange(CUBE_SIZE, CUBE_SIZE+amount), axis=1)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] - amount)\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"] - amount\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"] - amount\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"]\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"]\n",
    "    return cube\n",
    "\n",
    "def pad_right(original, right, amount):\n",
    "    res = np.insert(original[\"cube\"], [CUBE_SIZE], right[\"cube\"][:,:amount], axis=1)\n",
    "    res = np.delete(res, np.arange(0, amount), axis=1)\n",
    "    cube = VAL_create_cube(original[\"file\"], res, original[\"top_row\"], original[\"left_col\"] + amount)\n",
    "    cube[\"col_px_left\"] = original[\"col_px_left\"] + amount\n",
    "    cube[\"col_px_right\"] = original[\"col_px_right\"] + amount\n",
    "    cube[\"row_px_top\"] = original[\"row_px_top\"]\n",
    "    cube[\"row_px_bottom\"] = original[\"row_px_bottom\"]\n",
    "    return cube\n",
    "    \n",
    "\n",
    "# \"Shave\" the right edge of the cube with <gap> pixels and pad with zeros on the left\n",
    "def shave_right(original, amount):\n",
    "    return pad_left(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# \"Shave\" the left edge of the cube with <gap> pixels and pad with zeros on the right    \n",
    "def shave_left(original, amount):\n",
    "    return pad_right(original, ZERO_CUBE, amount)\n",
    "    \n",
    "\n",
    "# concatenate cubes \n",
    "def concatenate_cubes(left, right, slice_size):\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right[\"cube\"][:,:slice_size]), axis=1)\n",
    "    x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "    return con, x_delta, y_delta\n",
    "\n",
    "# concatenate cubes \n",
    "def VAL_concatenate_cubes(left, right, slice_size):\n",
    "    right_img = right[\"cube\"]\n",
    "    # next block is not relevant for training ...\n",
    "    #     # if the left cube is matched to another left cube (or right cube to another right cube) then rotate the right\n",
    "    #     # cube by 180 so we try to match it upside down, covering the option that the cube was pictured rotated\n",
    "    #     if ((left[\"col\"] == 0 and right[\"col\"] == 0) or (left[\"col\"] != 0 and right[\"col\"] != 0)):\n",
    "    #         right_img = np.rot90(right[\"cube\"], 2);\n",
    "\n",
    "    con = np.concatenate((left[\"cube\"][:,-slice_size:], right_img[:,:slice_size]), axis=1)\n",
    "\n",
    "    # next block calculates distance based on the distance between left's right-top corner and right's left-top corner    \n",
    "    #     x_delta = right[\"top_row\"] - left[\"top_row\"]\n",
    "    #     y_delta = right[\"left_col\"] - left[\"right_col\"] \n",
    "\n",
    "    # next block calculates the distance between the centers of cubes, accounting for test set's possibility of reverse slices (left instead of right and vice versa)\n",
    "    x_delta = right[\"row_px_top\"] - left[\"row_px_top\"] # equivalent to distance between vertical centers\n",
    "    y_delta = (right[\"col_px_left\"] + (slice_size / 2)) - (left[\"col_px_right\"] - (slice_size / 2)) # measuring the distance between horizontal centers of the slices\n",
    "\n",
    "    return con, x_delta, y_delta, left[\"file\"], right[\"file\"]\n",
    "    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cube with zeros\n",
    "def concatenate_cubes_zero_pad_gaps(left_orig, right_orig, gap):\n",
    "    left = left_orig if gap == 0 else shave_right(left_orig, gap)\n",
    "    right = right_orig if gap == 0 else shave_left(right_orig, gap)\n",
    "    return concatenate_cubes(left, right)    \n",
    "\n",
    "# concatenate cubes while artificially creating a gap between them. Pad the other end of the cobe with the nearby\n",
    "# continuation of the cubes\n",
    "def concatenate_cubes_with_gap(left_orig, right_orig, gap, left_pad, right_pad, slice_size):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    left = left_orig if gap == 0 else pad_left(left_orig, left_pad, gap)\n",
    "    right = right_orig if gap == 0 else pad_right(right_orig, right_pad, gap)\n",
    "    return concatenate_cubes(left, right, slice_size)        \n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def VAL_build_train_set_for_euclidean_distance(cubes, slice_size, folder):\n",
    "    # clean folder before starting\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    train_left_obj = []\n",
    "    train_right_obj = []\n",
    "    # iterate over all cubes   \n",
    "    for curr in cubes:\n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in cubes:\n",
    "            if (adj[\"file\"] != curr[\"file\"]): # no need to test against self CURRENTLY checking from directions!!!\n",
    "                #import pdb; pdb.set_trace()\n",
    "                # append the adjacent image to the current image\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "                output = folder+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                np.save(output, conc)\n",
    "                train_imgs.append(output)\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                train_left_obj.append(curr)\n",
    "                train_right_obj.append(adj)\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_x_delta, train_y_delta, train_left_obj, train_right_obj\n",
    "\n",
    "\n",
    "# convert the data structure of cubes into a train set of 2 arrays of images and labels\n",
    "# each image is a concatanation of 2 images from the original cubes set, covering all combinations of images\n",
    "# effectively creating Nx(N-1) images\n",
    "def ORIG_build_train_set(cubes, gap):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    # iterate over the rows and cols, essentially going over the grid of sliced cubes\n",
    "    for row in range(0, rows):\n",
    "        for col in range(0, cols):\n",
    "            # if this cube exists (could have been removed previously due to lack of data)\n",
    "            if (cubes[(col, row)] != None):\n",
    "                # for each \"current\" image in the iteration\n",
    "                curr = cubes[(col, row)]\n",
    "                # iterate over all the cubes to find all the \"other\" (adjacent) cubes\n",
    "                for adj_row in range(0, rows):\n",
    "                    for adj_col in range(0, cols):\n",
    "                        if (adj_row != row or adj_col != col):\n",
    "                            if (cubes[(adj_col, adj_row)] != None):\n",
    "                                adj = cubes[(adj_col, adj_row)]\n",
    "                                # append the adjacent image to the current image\n",
    "                                # pass the filling cubes on the right and left to pad against the gap\n",
    "                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                    if (gap == 0):\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes(curr, adj, slice_size)\n",
    "                                    else:\n",
    "                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr, adj, gap, curr[\"left\"], adj[\"right\"], slice_size)\n",
    "                                    train_imgs.append(conc)\n",
    "                                    train_x_delta.append(x_delta)\n",
    "                                    train_y_delta.append(y_delta)\n",
    "                                    # if the adj image is on the same row and on the right of the curr image - it will be marked as match    \n",
    "                                    if (adj_row == row and adj_col == (col + 1)):\n",
    "                                        # mark the image as matched\n",
    "                                        train_lbls.append([0,1])\n",
    "                                        # need to enrich the set with a few more tru positive samples - so we offset \n",
    "                                        # the matched images up ad down a few times and create more matches\n",
    "                                        if (\"top\" in curr.keys() and \"top\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                                                adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"top\" in curr[\"left\"].keys() and \"top\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_above(curr[\"left\"], curr[\"left\"][\"top\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_above(adj[\"right\"], curr[\"right\"][\"top\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                                                adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys() and \"bottom\" in curr[\"left\"].keys() and \"bottom\"in curr[\"right\"].keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_below(curr[\"left\"], curr[\"left\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        adj1Right = pad_below(adj[\"right\"], curr[\"right\"][\"bottom\"], i) # FIXIT?\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"left\" in curr.keys()): # enough to check only the curr as the left of the adj is the curr\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_left(curr, curr[\"left\"],i)\n",
    "                                                adj1 = pad_left(adj, adj[\"left\"],i) # essentially the curr\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_left(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_left(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                        if (\"right\" in adj.keys()): # enough to check only the adj as the right of the curr is the adj\n",
    "                                            for i in range(5, 101, 5):\n",
    "                                                curr1 = pad_right(curr, curr[\"right\"],i) # essentially the adj\n",
    "                                                adj1 = pad_right(adj, adj[\"right\"],i)\n",
    "                                                if (gap == 0 or (\"left\" in curr.keys() and \"right\" in adj.keys())):\n",
    "                                                    if (gap == 0):\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes(curr1, adj1, slice_size)\n",
    "                                                    else:\n",
    "                                                        curr1Left = pad_right(curr[\"left\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        adj1Right = pad_right(adj[\"right\"], ZERO_CUBE, i) # FIXIT? + assuming the gap will not be more than 150\n",
    "                                                        conc, x_delta, y_delta = concatenate_cubes_with_gap(curr1, adj1, gap, curr1Left, adj1Right, slice_size)\n",
    "                                                    train_imgs.append(conc)\n",
    "                                                    train_x_delta.append(x_delta)\n",
    "                                                    train_y_delta.append(y_delta)\n",
    "                                                    # mark the image as matched\n",
    "                                                    train_lbls.append([0,1])\n",
    "                                    else:\n",
    "                                        # mark the image as not matched\n",
    "                                        train_lbls.append([1,0])\n",
    "                                \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN Utility functions 2\n",
    "SAVE_PNG=False\n",
    "def save_img(path, img):\n",
    "    np.save(path, img)  \n",
    "    if SAVE_PNG:\n",
    "        plt.imsave(path+\".png\", img, cmap=plt.cm.gray)\n",
    "                    \n",
    "def VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                              train_y_delta, curr, adj, tolerance_factor=0):\n",
    "    # need to enhance the set with a few more true positive samples\n",
    "    # allowing some up and down tolerance\n",
    "    if (\"top\" in curr.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "            adj1 = adj\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "    if (\"top\" in adj.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = curr\n",
    "            adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "    if (\"bottom\" in curr.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "            adj1 = adj\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "    if (\"bottom\"in adj.keys()):\n",
    "        for i in range(0, tolerance_factor * 10, 10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            curr1 = curr\n",
    "            adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "            # print(\">>> MATCH >>>\"+output)\n",
    "            save_img(output, conc)\n",
    "            # print(\">>> >>> >>> SAVED\")\n",
    "            train_imgs.append(output)\n",
    "            train_x_delta.append(x_delta)\n",
    "            train_y_delta.append(y_delta)\n",
    "            # mark the image as matched\n",
    "            train_lbls.append([0,1])\n",
    "\n",
    "                            \n",
    "# IMPORTANT: enrich_factor determines how many \"duplications\" of TRUE values will we have in the train set\n",
    "# This allows for a more balanced train set however, it reduces the strictness of the matches \n",
    "# i.e. (not sure why) when we have multiple nearby \"duplicates\" matches we get much more matches in the validation\n",
    "# PARAMS: enrich_factor=1 means no enrich/duplicate, 20 means duplicate by 20, every 10 pixels\n",
    "# PARAMS: tolerance_factor=0 means only match against exact horizon, each notch equals additional 10 pixels tolerance\n",
    "def NEW_build_train_set_for_binary_labeling(cubes, slice_size, folder, enrich_factor=1, tolerance_factor=0): \n",
    "    # enrich_factor is split by 2 because it is dual-sided and 1 means actually no enrichment - i.e. 0.5\n",
    "    enrich_factor = enrich_factor / 2 \n",
    "    # clean folder before starting\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    train_x_delta = []\n",
    "    train_y_delta = []\n",
    "    discard_c = 0\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        # iterate over the others (effectively n^2)\n",
    "        for adj in cubes:\n",
    "            # Initial filter: what CAN be matched against what?\n",
    "            # 1 - not of the same fragment (file==fragment)\n",
    "            # 2 - they ARE of the same tear - don't want to confuse the learning with false data coming from different tears\n",
    "            # 3 - no need to test against self and avoid checking from both directions\n",
    "            if  adj[\"file\"] != curr[\"file\"] and \\\n",
    "                adj[\"tear\"] == curr[\"tear\"] and \\\n",
    "                curr[\"piece_col\"] < adj[\"piece_col\"]: \n",
    "                # last condition above - actually ignores pieces of the same col but different rows\n",
    "                # the assumption is that they are either \"not-match\" and then will tilt the balance further to not-match\n",
    "                # or they are \"somewhat-matching\" but in a way that might confuse the algorithm\n",
    "                \n",
    "                # print(\">>> >>>\"+str(curr[\"cube\"].shape)+\" <<< <<<\"+str(adj[\"cube\"].shape))\n",
    "                # append the adjacent image to the current image\n",
    "                conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr, adj, slice_size)\n",
    "\n",
    "                train_x_delta.append(x_delta)\n",
    "                train_y_delta.append(y_delta)\n",
    "                                \n",
    "                # Condition for marking as match:\n",
    "                # 1 - the adj piece is on the same row as the curr\n",
    "                # 2 - the adj piece is just to the right of the curr\n",
    "                # 3 - the curr cube is on the right edge of the piece\n",
    "                # 4 - the adj cube is on the left edge of the piece\n",
    "                # 5 - the cubes are in the same horizon\n",
    "                if  curr[\"piece_row\"] == adj[\"piece_row\"] and \\\n",
    "                    curr[\"piece_col\"] + 1 == adj[\"piece_col\"] and \\\n",
    "                    (curr[\"col\"] != 0 or curr[\"last\"]) and \\\n",
    "                    (adj[\"col\"] == 0 or not adj[\"last\"]) and \\\n",
    "                    np.abs(x_delta) < 50: \n",
    "\n",
    "                    # print(x_delta, y_delta)\n",
    "                    \n",
    "                    # mark the image as matched\n",
    "                    output = folder+\"1=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\">>> MATCH >>>\"+output)\n",
    "                    save_img(output, conc)\n",
    "                    # print(\">>> >>> >>> SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([0,1])\n",
    "\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    # TOLERANCE\n",
    "                    VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                              train_y_delta, curr, adj, tolerance_factor)\n",
    "                    \n",
    "                    # ENRICH/DUPLICATE\n",
    "                    # need to enrich the set with a few more true positive samples - so we offset \n",
    "                    # the matched images up and down a few times and create more matches\n",
    "                    if (\"top\" in curr.keys() and \"top\" in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_above(curr, curr[\"top\"],i)\n",
    "                            adj1 = pad_above(adj, adj[\"top\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            save_img(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    "\n",
    "                            # TOLERANCE\n",
    "                            VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                                      train_y_delta, curr1, adj1, tolerance_factor)\n",
    "                            \n",
    "                    if (\"bottom\" in curr.keys() and \"bottom\"in adj.keys()):\n",
    "                        for i in range(0, 121, int(120/enrich_factor)):\n",
    "                            if i == 0:\n",
    "                                continue\n",
    "                            curr1 = pad_below(curr, curr[\"bottom\"],i)\n",
    "                            adj1 = pad_below(adj, adj[\"bottom\"],i)\n",
    "                            conc, x_delta, y_delta, x_file, y_file = VAL_concatenate_cubes(curr1, adj1, slice_size)\n",
    "                            output = folder+\"1=\"+x_file+\"_\"+str(curr1[\"top_row\"])+\"_\"+str(curr1[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj1[\"top_row\"])+\"_\"+str(adj1[\"left_col\"])\n",
    "                            # print(\">>> MATCH >>>\"+output)\n",
    "                            save_img(output, conc)\n",
    "                            # print(\">>> >>> >>> SAVED\")\n",
    "                            train_imgs.append(output)\n",
    "                            train_x_delta.append(x_delta)\n",
    "                            train_y_delta.append(y_delta)\n",
    "                            # mark the image as matched\n",
    "                            train_lbls.append([0,1])\n",
    "\n",
    "                            # TOLERANCE\n",
    "                            VAL_add_tolerance_matches(slice_size, folder, train_imgs, train_lbls, train_x_delta, \n",
    "                                                      train_y_delta, curr1, adj1, tolerance_factor)\n",
    " \n",
    "                # adding a condition for marking as not-matched - we mark only the \"key\" cubes which are every 250px\n",
    "                # and not overlap - hence we reduce the ratio in favour of not matched which is enormous\n",
    "                elif int(curr[\"row\"]) == curr[\"row\"] and int(adj[\"row\"]) == adj[\"row\"]: \n",
    "                    # mark the image as not matched\n",
    "                    output = folder+\"0=\"+x_file+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])+\"---\"+y_file+\"_\"+str(adj[\"top_row\"])+\"_\"+str(adj[\"left_col\"])\n",
    "                    # print(\"<<< nonmatch <<<\"+output)\n",
    "                    save_img(output, conc)\n",
    "                    # print(\"<<< <<< <<< SAVED\")\n",
    "                    train_imgs.append(output)\n",
    "                    train_lbls.append([1,0]) # not matched\n",
    "                \n",
    "                # discard not matched which are \n",
    "                else:\n",
    "                    discard_c += 1\n",
    "                    \n",
    "    print(\"*** MATCHED=\"+str(sum(x[1] == 1 for x in train_lbls)))\n",
    "    print(\"*** NOT MATCHED=\"+str(sum(x[0] == 1 for x in train_lbls)))\n",
    "    print(\"*** DISCARDED=\"+str(discard_c))\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return train_imgs, train_lbls, train_x_delta, train_y_delta\n",
    "\n",
    "\n",
    "def frame_to_n_by_m(orig, start_vector, end_vector, is_col):\n",
    "    max_val = np.amax(end_vector)\n",
    "    min_val = np.amin(start_vector)\n",
    "    width = max_val - min_val\n",
    "    if (is_col):\n",
    "        result = np.zeros((start_vector.size, width))\n",
    "    else:\n",
    "        result = np.zeros((width, start_vector.size))\n",
    "    \n",
    "    for i in range(0, start_vector.size):\n",
    "        if (is_col):\n",
    "            row_vec = orig[i, start_vector[i]:end_vector[i]]\n",
    "        else:\n",
    "            row_vec = orig[start_vector[i]:end_vector[i],i]\n",
    "        temp = np.lib.pad(row_vec, (start_vector[i]-min_val, max_val-end_vector[i]), 'constant', constant_values=(0.09, 0.09))\n",
    "        if (is_col):\n",
    "            if (result[i].size != width):\n",
    "                import pdb; pdb.set_trace()\n",
    "            result[i] = temp[0:width]\n",
    "        else:\n",
    "            result[:,i] = temp[0:width]\n",
    "    return min_val, result\n",
    "\n",
    "def rough_tear_line(orig, start_vector, cut_mean, is_col, chew_factor):\n",
    "    end_vector = np.empty(start_vector.size).astype(int)\n",
    "    if (is_col and np.absolute(cut_mean-orig.shape[1]) < 10):\n",
    "        end_vector.fill(orig.shape[1])\n",
    "    elif (not is_col and np.absolute(cut_mean-orig.shape[0]) < 10):\n",
    "        end_vector.fill(orig.shape[0])\n",
    "    else:\n",
    "        deviation_vector = np.random.normal(0, chew_factor, start_vector.size).astype(int)\n",
    "        end_vector[0] = cut_mean + deviation_vector[0]\n",
    "        for i in range(1, end_vector.size):\n",
    "            end_vector[i] = end_vector[i - 1] + deviation_vector[i]\n",
    "    \n",
    "    start_px, cut_piece = frame_to_n_by_m(orig, start_vector, end_vector, is_col)    \n",
    "    return start_px, cut_piece, end_vector\n",
    "\n",
    "def rough_tear_image(image, cols, rows):\n",
    "    pieces = []\n",
    "    col_width = int(image.shape[1] / cols)\n",
    "    row_height = int(image.shape[0] / rows)\n",
    "    # print(col_width, row_height)\n",
    "    next_col_start_vec = np.zeros((image.shape[0],), dtype=int)\n",
    "    for col_idx in range(0, cols):\n",
    "    #         import pdb; pdb.set_trace()\n",
    "        start_col_px, cut_column, next_col_start_vec =  rough_tear_line(image, next_col_start_vec, col_width * (col_idx + 1), True, 5)\n",
    "        next_row_start_vec = np.zeros((cut_column.shape[1],), dtype=int)\n",
    "        for row_idx in range(0, rows):\n",
    "            start_row_px, cut_piece, next_row_start_vec = rough_tear_line(cut_column, next_row_start_vec, row_height * (row_idx + 1), False, 1)\n",
    "\n",
    "            ymin, ymax, xmin, xmax = calc_min_max_coordinates_dynamic(cut_piece, cutoff=BG_2_OBJ_RATIO)\n",
    "            temp = crop(cut_piece, ymin, ymax, xmin, xmax)\n",
    "            \n",
    "            #import pdb; pdb.set_trace()\n",
    "            piece = {}\n",
    "            piece[\"orig\"] = cut_piece\n",
    "            piece[\"cut\"] = temp\n",
    "            piece[\"col\"] = col_idx\n",
    "            piece[\"row\"] = row_idx\n",
    "            piece[\"col_px\"] = start_col_px + xmin\n",
    "            piece[\"row_px\"] = start_row_px + ymin\n",
    "            pieces.append(piece)\n",
    "            \n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Define model util functions\n",
    "\n",
    "# initialize a shaped matrix of weights with random values\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# initialize a shaped matrix of bias with random values\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 2, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 1, 1],\n",
    "                        strides=[1, 2, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_1x1(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x5(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 5, 1],\n",
    "                        strides=[1, 5, 5, 1], padding='SAME')\n",
    "\n",
    "def max_pool_5x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 5, 2, 1],\n",
    "                        strides=[1, 5, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN Image utility functions (external source)\n",
    "def branchedPoints(skel):\n",
    "    branch1=np.array([[2, 1, 2], [1, 1, 1], [2, 2, 2]])\n",
    "    branch2=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch3=np.array([[1, 2, 1], [2, 1, 2], [1, 2, 2]])\n",
    "    branch4=np.array([[2, 1, 2], [1, 1, 2], [2, 1, 2]])\n",
    "    branch5=np.array([[1, 2, 2], [2, 1, 2], [1, 2, 1]])\n",
    "    branch6=np.array([[2, 2, 2], [1, 1, 1], [2, 1, 2]])\n",
    "    branch7=np.array([[2, 2, 1], [2, 1, 2], [1, 2, 1]])\n",
    "    branch8=np.array([[2, 1, 2], [2, 1, 1], [2, 1, 2]])\n",
    "    branch9=np.array([[1, 2, 1], [2, 1, 2], [2, 2, 1]])\n",
    "    br1=mh.morph.hitmiss(skel,branch1)\n",
    "    br2=mh.morph.hitmiss(skel,branch2)\n",
    "    br3=mh.morph.hitmiss(skel,branch3)\n",
    "    br4=mh.morph.hitmiss(skel,branch4)\n",
    "    br5=mh.morph.hitmiss(skel,branch5)\n",
    "    br6=mh.morph.hitmiss(skel,branch6)\n",
    "    br7=mh.morph.hitmiss(skel,branch7)\n",
    "    br8=mh.morph.hitmiss(skel,branch8)\n",
    "    br9=mh.morph.hitmiss(skel,branch9)\n",
    "    return br1+br2+br3+br4+br5+br6+br7+br8+br9\n",
    "\n",
    "def endPoints(skel):\n",
    "    endpoint1=np.array([[0, 0, 0],\n",
    "                        [0, 1, 0],\n",
    "                        [2, 1, 2]])\n",
    "    \n",
    "    endpoint2=np.array([[0, 0, 0],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 2, 1]])\n",
    "    \n",
    "    endpoint3=np.array([[0, 0, 2],\n",
    "                        [0, 1, 1],\n",
    "                        [0, 0, 2]])\n",
    "    \n",
    "    endpoint4=np.array([[0, 2, 1],\n",
    "                        [0, 1, 2],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint5=np.array([[2, 1, 2],\n",
    "                        [0, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint6=np.array([[1, 2, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [0, 0, 0]])\n",
    "    \n",
    "    endpoint7=np.array([[2, 0, 0],\n",
    "                        [1, 1, 0],\n",
    "                        [2, 0, 0]])\n",
    "    \n",
    "    endpoint8=np.array([[0, 0, 0],\n",
    "                        [2, 1, 0],\n",
    "                        [1, 2, 0]])\n",
    "    \n",
    "    ep1=mh.morph.hitmiss(skel,endpoint1)\n",
    "    ep2=mh.morph.hitmiss(skel,endpoint2)\n",
    "    ep3=mh.morph.hitmiss(skel,endpoint3)\n",
    "    ep4=mh.morph.hitmiss(skel,endpoint4)\n",
    "    ep5=mh.morph.hitmiss(skel,endpoint5)\n",
    "    ep6=mh.morph.hitmiss(skel,endpoint6)\n",
    "    ep7=mh.morph.hitmiss(skel,endpoint7)\n",
    "    ep8=mh.morph.hitmiss(skel,endpoint8)\n",
    "    ep = ep1+ep2+ep3+ep4+ep5+ep6+ep7+ep8\n",
    "    return ep\n",
    "\n",
    "def pruning(skeleton, size):\n",
    "    '''remove iteratively end points \"size\" \n",
    "       times from the skeleton\n",
    "    '''\n",
    "    for i in range(0, size):\n",
    "        endpoints = endPoints(skeleton)\n",
    "        endpoints = np.logical_not(endpoints)\n",
    "        skeleton = np.logical_and(skeleton,endpoints)\n",
    "    return skeleton\n",
    "\n",
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_adjustable('box-forced')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_deep - Define the model - 250, 125, 62, 25\n",
    "def model_tf_deep(input_width, forced_bias=0): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob, probability, probabilities #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv0 = bias_variable([32])\n",
    "    h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "    h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "#     W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "#     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    c_fc2 = tf.constant([0, forced_bias], dtype=tf.float32)\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 + c_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    probability = tf.nn.softmax(y_conv,1)\n",
    "    \n",
    "    probabilities=tf.reduce_sum(probability,1)\n",
    "    \n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_orig - Define the model - 250, 125, 62, 25\n",
    "def model_tf_orig(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # zero convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "#     b_conv0 = bias_variable([32])\n",
    "#     h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0, \"0020\")\n",
    "#     h_pool0 = max_pool_1x1(h_conv0) # size is maintained\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "#     W_conv1 = weight_variable([5, 5, 32, 32])\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "#     h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1, \"0021\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 32, 32])\n",
    "    b_conv3 = bias_variable([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*32]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*32, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN model_tf_wide - Define the model - 250, 125, 62, 25\n",
    "def model_tf_wide(input_width): \n",
    "    global accuracy, correct_prediction, train_step, x, y_, y_conv, keep_prob #, W_fc, b_fc, cost, y_conv_temp\n",
    "    \n",
    "    # foundation of the model - the input layer of the image 250 x input_width*2\n",
    "    x = tf.placeholder(tf.float32, [None, 250, input_width*2], \"001\")\n",
    "    x_image = tf.reshape(x, [-1,250,input_width*2,1], \"0011\") # 1 is the number of color channels\n",
    "\n",
    "    # the target digits of the model\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], \"002\") # 1\n",
    "\n",
    "    # first convolutional layer: one input image and 32 output filters of 5x5\n",
    "    W_conv1 = weight_variable([5, 5, 1, 64])\n",
    "    b_conv1 = bias_variable([64])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, \"0021\")\n",
    "    if (input_width == 250):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 125):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x250\n",
    "    elif (input_width == 62):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool1 = max_pool_2x1(h_conv1) # size is reduced to 125x50\n",
    "    else:\n",
    "        print(\"ERROR - unsupported slice width\")\n",
    "        return\n",
    "\n",
    "    # second convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, \"0022\")\n",
    "    if (input_width == 62):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x125\n",
    "    elif (input_width == 25):\n",
    "        h_pool2 = max_pool_1x1(h_conv2) # size is reduced to 125x50\n",
    "    else:\n",
    "        h_pool2 = max_pool_1x2(h_conv2) # size is reduced to 125x125\n",
    "\n",
    "\n",
    "    # third convolutional layer: 32 input (filtered) images and 32 output filters of 5x5\n",
    "    W_conv3 = weight_variable([5, 5, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3, \"0023\")\n",
    "    if (input_width == 25):\n",
    "        h_pool3 = max_pool_5x2(h_conv3) # size is reduced to 25x25\n",
    "    else:\n",
    "        h_pool3 = max_pool_5x5(h_conv3) # size is reduced to 25x25\n",
    "\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 25*25*64]) # shape as an array \n",
    "\n",
    "    # fourth layer - fully connected with input 25*25*128 and output 1024\n",
    "    W_fc1 = weight_variable([25*25*64, 2048])\n",
    "    b_fc1 = bias_variable([2048])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1, \"0024\")\n",
    "\n",
    "    # a drop layer with probability \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"003\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name=\"0031\")\n",
    "\n",
    "    #     # final layer - reduce to one \"class\" for the linear regression\n",
    "    #     W_fc = weight_variable([1024, 1]) \n",
    "    #     b_fc = bias_variable([1])         \n",
    "    #     y_conv_temp = tf.matmul(h_fc1_drop, W_fc, name=\"0032\") + b_fc\n",
    "    #     y_conv = tf.minimum(y_conv_temp, tf.constant(BREAK_VAL, tf.float32))\n",
    "\n",
    "    #     #     # minimize loss function\n",
    "    #     #     cross_entropy = tf.reduce_mean(\n",
    "    #     #       tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    #     # cost = tf.reduce_sum(tf.pow(y_conv - y_, 2))/(2*BATCHES*BATCH_SIZE) # Mean squared error\n",
    "    #     cost = tf.reduce_mean(tf.square(y_conv_temp - y_), name=\"0033\") # Mean squared error\n",
    "\n",
    "    #     #     # define train step and rate\n",
    "    #     #     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    #     train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost) # Gradient descent\n",
    "\n",
    "    #     # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    #     correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    #     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # final layer - softmax reduction 2 outputs\n",
    "    W_fc2 = weight_variable([2048, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    # minimize loss function\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    # define train step and rate\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # evaluate the prediction and the accuracy on the train test - needed only for printing during the training\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN train\n",
    "def train(train_imgs, train_lbls, output_model, input_model=\"\"):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING:\")\n",
    "    print(\"MODEL:\"+output_model)\n",
    "    print(\"#####################################################################\")\n",
    "\n",
    "    from random import randrange\n",
    "    \n",
    "    # TRAIN Prepare the session\n",
    "\n",
    "    # create a saver object\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "    if input_model != \"\":\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, input_model)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # TRAIN Train the model\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # run the train batches\n",
    "    for i in range(BATCHES):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(BATCH_SIZE):\n",
    "            random_index = randrange(0,len(train_imgs))\n",
    "            image = np.load(train_imgs[random_index]+\".npy\")\n",
    "            x_batch.append(image)\n",
    "            y_batch.append(train_lbls[random_index])\n",
    "\n",
    "        # train\n",
    "        # print(\"step %d\"%(i))\n",
    "        train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "        # print the accuracy thus far\n",
    "        if (i+1)%50 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, output_model)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    # Close the Session when we're done. If un-commented - need to run next bock of restore...\n",
    "    sess.close()   \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"TRAINING ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process - OLD?\n",
    "def pre_process(folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+folder)\n",
    "    print(\"#####################################################################\")\n",
    "    result = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            # Read the image\n",
    "            # image = img.imread(os.path.join(root, file_))\n",
    "            image = np.load(os.path.join(root, file_))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            cubes = VAL_slice_to_static_slices(file_, image)\n",
    "            print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "            result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# RUN pre_process_training - crop image, then tear it randomly to various tears, then per tear create cubes out of the edges, return cube set\n",
    "def pre_process_training(img_name, x_start=X_START, x_end=X_END, y_start=Y_START, y_end=Y_END):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS:\"+img_name)\n",
    "    print(\"#####################################################################\")\n",
    "    short_name = img_name[:img_name.rfind('-D')]\n",
    "    image = read_and_crop(img_name, x_start, x_end, y_start, y_end)\n",
    "    result = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(ROOT_FOLDER+\"fragments/\"):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "\n",
    "    \n",
    "    for col_cut in range(3, 9): # 3...10\n",
    "        for row_cut in range(2, 6): # 2...5\n",
    "            print(\"PRE_PROCESS:::\"+\"TEAR_\"+str(col_cut)+\"X\"+str(row_cut))\n",
    "            pieces = rough_tear_image(image, col_cut, row_cut)\n",
    "            \n",
    "            for piece in pieces:\n",
    "                # print(\"PRE_PROCESS:::\"+\"PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"]))\n",
    "                fragment_name = short_name + \"_TEAR_\"+str(col_cut)+\"X\"+str(row_cut)+\"_PIECE_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "                fragment_file_name = short_name + \"_\"+str(col_cut)+\"X\"+str(row_cut)+\"_\"+str(piece[\"col\"])+\"X\"+str(piece[\"row\"])\n",
    "                # import pdb; pdb.set_trace()\n",
    "                plt.imsave(os.path.join(ROOT_FOLDER+\"fragments/\",fragment_file_name+\".jpg\"), piece[\"cut\"], cmap=plt.cm.gray)\n",
    "                cubes = VAL_slice_TEAR_to_static_slices(fragment_name, piece)\n",
    "                for cube in cubes:\n",
    "                    cube[\"tear\"] = str(col_cut)+\"X\"+str(row_cut)\n",
    "                    cube[\"piece_col\"] = piece[\"col\"]\n",
    "                    cube[\"piece_row\"] = piece[\"row\"]\n",
    "                # print(\"File: %s >>> cubes: %d\"%(file_, len(cubes)))\n",
    "                result.extend(cubes)\n",
    "    \n",
    "    return result\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"PRE_PROCESS ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate1(cubes, model, slice_size, folder, curr_cube):    \n",
    "    # VALIDATE prepare the data sets\n",
    "    test_imgs, test_x_delta, test_y_delta, test_x_file, test_y_file = VAL_build_train_set(cubes, slice_size, folder, curr_cube)\n",
    "    print(\"loaded %d images\"%(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def validate2(folder, model, slice_size):\n",
    "    test_imgs = []\n",
    "    test_x_file = []\n",
    "    test_y_file = []\n",
    "    the_root = \"\"\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        the_root = root\n",
    "        for file_ in files:\n",
    "            test_imgs.append( os.path.join(root, file_) )\n",
    "            test_x_file.append(file_[:file_.rfind('---P')])\n",
    "            test_y_file.append(file_[file_.rfind('---P')+3:])\n",
    "            \n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    v1t = []\n",
    "    count = 0\n",
    "    length = len(test_imgs)\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i])\n",
    "                x_batch.append(image)\n",
    "                count += 1\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        x_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "\n",
    "\n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    A = np.array(test_x_file)\n",
    "    B = np.array(test_y_file)\n",
    "    C = np.array(test_imgs)\n",
    "    match_x_files = A[match_indexes]\n",
    "    match_y_files = B[match_indexes]\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    for matched_img in match_images:\n",
    "        load_img = np.load(matched_img)\n",
    "        plt.imsave(os.path.join(\"/Volumes/250GB/matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file_ in files:\n",
    "            os.remove( os.path.join(root, file_) ) # delete it from the FS\n",
    "                \n",
    "    with open('matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            print(\"MATCH %s === %s\"%(test_x_file[match_index], test_y_file[match_index]))\n",
    "            # print(\"MATCH %s === %s\"%(A[match_index], B[match_index]))\n",
    "            # csvout.writerow([A[match_index], B[match_index]])\n",
    "            csvout.writerow([test_x_file[match_index], test_y_file[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def validate2_for_cross_validation(test_imgs, test_lbls, model, max_samples=0):\n",
    "    print(len(test_imgs))\n",
    "    \n",
    "    # VALIDATE Prepare a test session \n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # start session and initialize variables\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    # VALIDATE Validate the model\n",
    "\n",
    "    count = 0\n",
    "    se = 0\n",
    "    st = 0\n",
    "    v1t = []\n",
    "    v2t = []\n",
    "    v1tt = []\n",
    "    v2tt = []\n",
    "    length = len(test_imgs)\n",
    "    if max_samples != 0:\n",
    "        length = max_samples\n",
    "    batch = 100\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # change the ranges in the loop below - first number is the start point (multiplied by batch size)\n",
    "    # second number is the end point (multiplied by batch size)\n",
    "    # third number is the jump from batch to batch\n",
    "    # use the length about to set the batch length\n",
    "    for start in range(0, length, batch):\n",
    "        for i in range(start, start+batch):\n",
    "            if (i < length):\n",
    "                image = np.load(test_imgs[i]+\".npy\")\n",
    "                x_batch.append(image)\n",
    "                y_batch.append(train_lbls[i])                \n",
    "                \n",
    "        # print the accuracy thus far\n",
    "    #         train_accuracy = accuracy.eval(feed_dict={\n",
    "    #             x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    #         print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "        # print(\"Validating start at #%d end at %d\"%(start*batch,(start+length)*batch))\n",
    "    #         my_prediction=tf.argmax(y_conv,1)\n",
    "    #         v1 = my_prediction.eval(feed_dict={x:x_batch, keep_prob: 1.0})\n",
    "    #         v1t = np.concatenate((v1t, v1), axis=0)\n",
    "\n",
    "        ######## printing the predictions and their normalized values\n",
    "        # print(\"y_conv=\"+str(y_conv.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probability=\"+str(probability.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        # print(\"probabilities=\"+str(probabilities.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})))\n",
    "        \n",
    "        my_prediction=tf.argmax(y_conv,1)\n",
    "        my_target=tf.argmax(y_,1)\n",
    "        v1 = my_prediction.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v2 = my_target.eval(feed_dict={x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        v1t = np.concatenate((v1t, v1), axis=0)\n",
    "        v2t = np.concatenate((v2t, v2), axis=0)\n",
    "\n",
    "        c1 = np.sum(np.absolute(np.subtract(v2, v1)))\n",
    "        c2 = np.sum(np.absolute(v2))\n",
    "        se += c1\n",
    "        st += c2\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        print(\">>> step %d\"%(start+batch))\n",
    "        \n",
    "        count += ((i+1) - start)\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "        print(\"step %d-%d, precision %f, recall %f, f_score %f\"%(start, i, precision, recall, f_score))\n",
    "        # print(\"Accumulated total true = %d\"%(st));\n",
    "        # print(\"Accumulated total error rate = %f\"%(se/count));\n",
    "        # v1tt = np.concatenate((v1tt, v1t), axis=0)\n",
    "        # v2tt = np.concatenate((v2tt, v2t), axis=0)\n",
    "        print(\"=== total %d match %d\"%(count, len(np.nonzero(v1t)[0])))\n",
    "\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(v2t, v1t, average='binary')\n",
    "    print(\"TOTAL %d, precision %f, recall %f, f_score %f\"%(count, precision, recall, f_score))\n",
    "    print(\"TOTAL true = %d\"%(st));\n",
    "    print(\"TOTAL error rate = %f\"%(se/count));\n",
    "    \n",
    "    match_indexes = np.nonzero(v1t)[0]\n",
    "    C = np.array(test_imgs)\n",
    "    match_images = C[match_indexes]\n",
    "    \n",
    "    # for matched_img in match_images:\n",
    "    #    load_img = np.load(matched_img+\".npy\")\n",
    "    #    plt.imsave(os.path.join(ROOT_FOLDER+\"synt_matched/\",matched_img[matched_img.rfind('/')+1:]+\".png\"), load_img, cmap=plt.cm.gray)\n",
    "\n",
    "    with open('synt_all.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for idx, test_img in enumerate(test_imgs):\n",
    "            # print(\"MATCH %s === %s\"%(test_imgs[match_index], train_lbls[match_index]))\n",
    "            match_class = 0\n",
    "            if idx in match_indexes:\n",
    "                match_class = 1\n",
    "            csvout.writerow([test_img, train_lbls[idx], match_class])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    with open('synt_matches.csv', 'a') as csvfile:\n",
    "        csvout = csv.writer(csvfile)\n",
    "        for match_index in match_indexes:\n",
    "            # print(\"MATCH %s === %s\"%(test_imgs[match_index], train_lbls[match_index]))\n",
    "            csvout.writerow([test_imgs[match_index], train_lbls[match_index]])\n",
    "            # plt.imsave(\"match_\"+match_index+\".jpg\", C[match_index])\n",
    "\n",
    "    # Close the Session when we're done.\n",
    "    sess.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def iter_validate(cubes, model, slice_size, folder):\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATING\")\n",
    "    print(\"#####################################################################\")\n",
    "    cubes_len = len(cubes)\n",
    "    batch_size = 100\n",
    "    count = 0\n",
    "    # iterate over the cubes\n",
    "    for curr in cubes:\n",
    "        count += 1\n",
    "        if count < batch_size: ### TEMP LIMITATION\n",
    "            print(\"CUBE:%s\"%(curr[\"file\"]+\"_\"+str(curr[\"top_row\"])+\"_\"+str(curr[\"left_col\"])))\n",
    "            validate1(cubes, model, slice_size, folder, curr)\n",
    "            validate2(folder, model, slice_size)\n",
    "        \n",
    "    print(\"#####################################################################\")\n",
    "    print(\"VALIDATION ENDED\")\n",
    "    print(\"#####################################################################\")\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_all(folder, model, slice_size):\n",
    "    model_tf(slice_size)\n",
    "    cubes_set = pre_process(folder)\n",
    "    validate(cubes_set, model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# HELPER block\n",
    "# image = read_and_crop(\"PX303/FG001/PX303-Fg001-V-C01-R01-D05032015-T112602-ML924__012.jpg\")\n",
    "## image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R01-D08032015-T110900-ML924__012.jpg\", 100, -1, 400, -1)\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C01-R02-D08032015-T105147-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R01-D08032015-T110025-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG004/PX303-Fg004-V-C02-R02-D08032015-T105553-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R01-D08032015-T120605-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C01-R02-D08032015-T115230-ML924__012.jpg\")\n",
    "# image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R01-D08032015-T120158-ML924__012.jpg\")\n",
    "##image = read_and_crop(\"PX303/FG006/PX303-Fg006-V-C02-R02-D08032015-T115704-ML924__012.jpg\", 0, 6200, 0, 4400)\n",
    "##plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_train_from_disk(path):\n",
    "    train_imgs = []\n",
    "    train_lbls = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_ in files:\n",
    "            file_name = os.path.join(root, file_)\n",
    "            file_name = file_name[:file_name.rfind(\".\")]\n",
    "            train_imgs.append(file_name)\n",
    "            train_lbls.append([1,0] if file_.startswith(\"0=\") else [0,1])\n",
    "    return train_imgs, train_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN1 - take 1st large pieces and train on it\n",
    "# cubes_set = pre_process_training(\"PX303-Fg001-V-C01-R01-D05032015-T112520-ML638__006.jpg\")\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats/\", 6, 5)\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RE-RUN1 - take 1st large pieces and train on it\n",
    "# train_imgs, train_lbls = \\\n",
    "#     load_train_from_disk(ROOT_FOLDER + \"train_concats/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "PRE_PROCESS:PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\n",
      "#####################################################################\n",
      "PRE_PROCESS:::TEAR_3X2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESS:::TEAR_3X3\n",
      "PRE_PROCESS:::TEAR_3X4\n",
      "PRE_PROCESS:::TEAR_4X2\n",
      "PRE_PROCESS:::TEAR_4X3\n",
      "PRE_PROCESS:::TEAR_4X4\n",
      "PRE_PROCESS:::TEAR_5X2\n",
      "PRE_PROCESS:::TEAR_5X3\n",
      "PRE_PROCESS:::TEAR_5X4\n",
      "PRE_PROCESS:::TEAR_6X2\n",
      "PRE_PROCESS:::TEAR_6X3\n",
      "PRE_PROCESS:::TEAR_6X4\n",
      "PRE_PROCESS:::TEAR_7X2\n",
      "PRE_PROCESS:::TEAR_7X3\n",
      "PRE_PROCESS:::TEAR_7X4\n",
      "PRE_PROCESS:::TEAR_8X2\n",
      "PRE_PROCESS:::TEAR_8X3\n",
      "PRE_PROCESS:::TEAR_8X4\n",
      "PRE_PROCESS:::TEAR_9X2\n",
      "PRE_PROCESS:::TEAR_9X3\n",
      "PRE_PROCESS:::TEAR_9X4\n",
      "*** MATCHED=21299\n",
      "*** NOT MATCHED=24663\n",
      "*** DISCARDED=4224801\n"
     ]
    }
   ],
   "source": [
    "# RUN2 - take 2nd large pieces and train on it\n",
    "# cubes_set = pre_process_training(\"PX303-Fg004-V-C01-R01-D08032015-T110817-ML638__006.jpg\", 100, -1, 400, -1)\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, \n",
    "#                                             ROOT_FOLDER + \"train_concats2/\", 1, 5)\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\", ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-898022341402>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "#####################################################################\n",
      "TRAINING:\n",
      "MODEL:/home/il239838/files/model_binary/tear_model2.ckpt\n",
      "#####################################################################\n",
      "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model1.ckpt\n",
      "Model restored.\n",
      "step 49, training accuracy 0.84\n",
      "step 99, training accuracy 0.9\n",
      "step 149, training accuracy 0.9\n",
      "step 199, training accuracy 0.86\n",
      "step 249, training accuracy 0.92\n",
      "step 299, training accuracy 0.9\n",
      "step 349, training accuracy 0.9\n",
      "step 399, training accuracy 0.94\n",
      "step 449, training accuracy 0.84\n",
      "step 499, training accuracy 0.88\n",
      "step 549, training accuracy 0.94\n",
      "step 599, training accuracy 0.98\n",
      "step 649, training accuracy 0.94\n",
      "step 699, training accuracy 0.94\n",
      "step 749, training accuracy 0.96\n",
      "step 799, training accuracy 0.94\n",
      "Optimization Finished!\n",
      "step 799, training accuracy 0.94\n",
      "Model saved in file: /home/il239838/files/model_binary/tear_model2.ckpt\n",
      "#####################################################################\n",
      "TRAINING ENDED\n",
      "#####################################################################\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# RE-RUN2 - take 2nd large pieces and train on it\n",
    "# train_imgs, train_lbls = \\\n",
    "#     load_train_from_disk(ROOT_FOLDER + \"train_concats2/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf_deep(250)\n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\", ROOT_FOLDER + \"model_binary/tear_model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL RUN3 - take 3rd large pieces and train on it OR TEST in next block\n",
    "\n",
    "# cubes_set = pre_process_training(\"PX303/FG006/PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "# train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "#     NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats/\")\n",
    "# tf.reset_default_graph()\n",
    "# model_tf(250)    \n",
    "# train(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model3.ckpt\", ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** MATCHED=27179\n",
      "*** NOT MATCHED=41077\n",
      "*** DISCARDED=3663971\n",
      "WARNING:tensorflow:From <ipython-input-6-898022341402>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68256\n",
      "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model2.ckpt\n",
      "Model restored.\n",
      ">>> step 100\n",
      "step 0-99, precision 0.876404, recall 1.000000, f_score 0.934132\n",
      "=== total 100 match 89\n",
      ">>> step 200\n",
      "step 100-199, precision 0.842391, recall 1.000000, f_score 0.914454\n",
      "=== total 200 match 184\n",
      ">>> step 300\n",
      "step 200-299, precision 0.739669, recall 1.000000, f_score 0.850356\n",
      "=== total 300 match 242\n",
      ">>> step 400\n",
      "step 300-399, precision 0.731118, recall 1.000000, f_score 0.844677\n",
      "=== total 400 match 331\n",
      ">>> step 500\n",
      "step 400-499, precision 0.782082, recall 1.000000, f_score 0.877717\n",
      "=== total 500 match 413\n",
      ">>> step 600\n",
      "step 500-599, precision 0.785288, recall 1.000000, f_score 0.879733\n",
      "=== total 600 match 503\n",
      ">>> step 700\n",
      "step 600-699, precision 0.792164, recall 1.000000, f_score 0.884030\n",
      "=== total 700 match 587\n",
      ">>> step 800\n",
      "step 700-799, precision 0.808321, recall 0.996337, f_score 0.892535\n",
      "=== total 800 match 673\n",
      ">>> step 900\n",
      "step 800-899, precision 0.829620, recall 0.995283, f_score 0.904932\n",
      "=== total 900 match 763\n",
      ">>> step 1000\n",
      "step 900-999, precision 0.781690, recall 0.995516, f_score 0.875740\n",
      "=== total 1000 match 852\n",
      ">>> step 1100\n",
      "step 1000-1099, precision 0.783351, recall 0.995929, f_score 0.876941\n",
      "=== total 1100 match 937\n",
      ">>> step 1200\n",
      "step 1100-1199, precision 0.778557, recall 0.979823, f_score 0.867672\n",
      "=== total 1200 match 998\n",
      ">>> step 1300\n",
      "step 1200-1299, precision 0.771536, recall 0.978622, f_score 0.862827\n",
      "=== total 1300 match 1068\n",
      ">>> step 1400\n",
      "step 1300-1399, precision 0.771300, recall 0.967379, f_score 0.858283\n",
      "=== total 1400 match 1115\n",
      ">>> step 1500\n",
      "step 1400-1499, precision 0.782536, recall 0.969823, f_score 0.866171\n",
      "=== total 1500 match 1191\n",
      ">>> step 1600\n",
      "step 1500-1599, precision 0.774042, recall 0.969638, f_score 0.860870\n",
      "=== total 1600 match 1279\n",
      ">>> step 1700\n",
      "step 1600-1699, precision 0.771558, recall 0.970504, f_score 0.859671\n",
      "=== total 1700 match 1322\n",
      ">>> step 1800\n",
      "step 1700-1799, precision 0.776163, recall 0.970909, f_score 0.862682\n",
      "=== total 1800 match 1376\n",
      ">>> step 1900\n",
      "step 1800-1899, precision 0.784477, recall 0.968349, f_score 0.866769\n",
      "=== total 1900 match 1443\n",
      ">>> step 2000\n",
      "step 1900-1999, precision 0.778576, recall 0.969894, f_score 0.863768\n",
      "=== total 2000 match 1531\n",
      ">>> step 2100\n",
      "step 2000-2099, precision 0.767601, recall 0.970843, f_score 0.857342\n",
      "=== total 2100 match 1605\n",
      ">>> step 2200\n",
      "step 2100-2199, precision 0.755744, recall 0.971251, f_score 0.850051\n",
      "=== total 2200 match 1654\n",
      ">>> step 2300\n",
      "step 2200-2299, precision 0.751490, recall 0.971495, f_score 0.847446\n",
      "=== total 2300 match 1678\n",
      ">>> step 2400\n",
      "step 2300-2399, precision 0.746096, recall 0.968468, f_score 0.842862\n",
      "=== total 2400 match 1729\n",
      ">>> step 2500\n",
      "step 2400-2499, precision 0.744763, recall 0.969849, f_score 0.842532\n",
      "=== total 2500 match 1814\n",
      ">>> step 2600\n",
      "step 2500-2599, precision 0.744248, recall 0.970691, f_score 0.842520\n",
      "=== total 2600 match 1869\n",
      ">>> step 2700\n",
      "step 2600-2699, precision 0.752066, recall 0.971963, f_score 0.847991\n",
      "=== total 2700 match 1936\n",
      ">>> step 2800\n",
      "step 2700-2799, precision 0.749500, recall 0.970227, f_score 0.845698\n",
      "=== total 2800 match 2000\n",
      ">>> step 2900\n",
      "step 2800-2899, precision 0.752913, recall 0.971196, f_score 0.848236\n",
      "=== total 2900 match 2060\n",
      ">>> step 3000\n",
      "step 2900-2999, precision 0.750587, recall 0.972019, f_score 0.847071\n",
      "=== total 3000 match 2129\n",
      ">>> step 3100\n",
      "step 3000-3099, precision 0.758152, recall 0.969873, f_score 0.851042\n",
      "=== total 3100 match 2208\n",
      ">>> step 3200\n",
      "step 3100-3199, precision 0.756969, recall 0.970950, f_score 0.850710\n",
      "=== total 3200 match 2296\n",
      ">>> step 3300\n",
      "step 3200-3299, precision 0.755745, recall 0.971554, f_score 0.850168\n",
      "=== total 3300 match 2350\n",
      ">>> step 3400\n",
      "step 3300-3399, precision 0.758378, recall 0.972414, f_score 0.852162\n",
      "=== total 3400 match 2417\n",
      ">>> step 3500\n",
      "step 3400-3499, precision 0.756441, recall 0.963096, f_score 0.847351\n",
      "=== total 3500 match 2484\n",
      ">>> step 3600\n",
      "step 3500-3599, precision 0.758347, recall 0.946898, f_score 0.842198\n",
      "=== total 3600 match 2516\n",
      ">>> step 3700\n",
      "step 3600-3699, precision 0.763229, recall 0.940505, f_score 0.842644\n",
      "=== total 3700 match 2589\n",
      ">>> step 3800\n",
      "step 3700-3799, precision 0.761959, recall 0.941806, f_score 0.842390\n",
      "=== total 3800 match 2655\n",
      ">>> step 3900\n",
      "step 3800-3899, precision 0.757331, recall 0.942518, f_score 0.839837\n",
      "=== total 3900 match 2728\n",
      ">>> step 4000\n",
      "step 3900-3999, precision 0.753602, recall 0.940647, f_score 0.836800\n",
      "=== total 4000 match 2776\n",
      ">>> step 4100\n",
      "step 4000-4099, precision 0.758354, recall 0.941485, f_score 0.840055\n",
      "=== total 4100 match 2843\n",
      ">>> step 4200\n",
      "step 4100-4199, precision 0.757679, recall 0.943076, f_score 0.840273\n",
      "=== total 4200 match 2930\n",
      ">>> step 4300\n",
      "step 4200-4299, precision 0.758129, recall 0.944605, f_score 0.841156\n",
      "=== total 4300 match 3014\n",
      ">>> step 4400\n",
      "step 4300-4399, precision 0.762877, recall 0.946163, f_score 0.844692\n",
      "=== total 4400 match 3087\n",
      ">>> step 4500\n",
      "step 4400-4499, precision 0.760835, recall 0.947223, f_score 0.843860\n",
      "=== total 4500 match 3161\n",
      ">>> step 4600\n",
      "step 4500-4599, precision 0.764341, recall 0.948442, f_score 0.846497\n",
      "=== total 4600 match 3225\n",
      ">>> step 4700\n",
      "step 4600-4699, precision 0.765436, recall 0.948968, f_score 0.847378\n",
      "=== total 4700 match 3304\n",
      ">>> step 4800\n",
      "step 4700-4799, precision 0.758979, recall 0.949499, f_score 0.843616\n",
      "=== total 4800 match 3369\n",
      ">>> step 4900\n",
      "step 4800-4899, precision 0.754856, recall 0.949648, f_score 0.841121\n",
      "=== total 4900 match 3398\n",
      ">>> step 5000\n",
      "step 4900-4999, precision 0.746817, recall 0.949945, f_score 0.836222\n",
      "=== total 5000 match 3456\n",
      ">>> step 5100\n",
      "step 5000-5099, precision 0.747290, recall 0.948932, f_score 0.836126\n",
      "=== total 5100 match 3506\n",
      ">>> step 5200\n",
      "step 5100-5199, precision 0.740949, recall 0.945898, f_score 0.830973\n",
      "=== total 5200 match 3563\n",
      ">>> step 5300\n",
      "step 5200-5299, precision 0.740720, recall 0.946549, f_score 0.831080\n",
      "=== total 5300 match 3610\n",
      ">>> step 5400\n",
      "step 5300-5399, precision 0.737825, recall 0.947533, f_score 0.829632\n",
      "=== total 5400 match 3696\n",
      ">>> step 5500\n",
      "step 5400-5499, precision 0.739973, recall 0.946975, f_score 0.830774\n",
      "=== total 5500 match 3765\n",
      ">>> step 5600\n",
      "step 5500-5599, precision 0.742424, recall 0.947965, f_score 0.832699\n",
      "=== total 5600 match 3828\n",
      ">>> step 5700\n",
      "step 5600-5699, precision 0.748341, recall 0.947641, f_score 0.836281\n",
      "=== total 5700 match 3918\n",
      ">>> step 5800\n",
      "step 5700-5799, precision 0.741171, recall 0.947742, f_score 0.831823\n",
      "=== total 5800 match 3964\n",
      ">>> step 5900\n",
      "step 5800-5899, precision 0.739519, recall 0.948155, f_score 0.830941\n",
      "=== total 5900 match 4031\n",
      ">>> step 6000\n",
      "step 5900-5999, precision 0.739869, recall 0.948957, f_score 0.831470\n",
      "=== total 6000 match 4121\n",
      ">>> step 6100\n",
      "step 6000-6099, precision 0.742673, recall 0.949726, f_score 0.833534\n",
      "=== total 6100 match 4197\n",
      ">>> step 6200\n",
      "step 6100-6199, precision 0.746607, recall 0.950268, f_score 0.836216\n",
      "=== total 6200 match 4274\n",
      ">>> step 6300\n",
      "step 6200-6299, precision 0.751659, recall 0.951608, f_score 0.839898\n",
      "=== total 6300 match 4369\n",
      ">>> step 6400\n",
      "step 6300-6399, precision 0.751067, recall 0.952150, f_score 0.839739\n",
      "=== total 6400 match 4451\n",
      ">>> step 6500\n",
      "step 6400-6499, precision 0.743970, recall 0.952408, f_score 0.835383\n",
      "=== total 6500 match 4519\n",
      ">>> step 6600\n",
      "step 6500-6599, precision 0.744593, recall 0.953020, f_score 0.836011\n",
      "=== total 6600 match 4577\n",
      ">>> step 6700\n",
      "step 6600-6699, precision 0.741914, recall 0.953151, f_score 0.834371\n",
      "=== total 6700 match 4607\n",
      ">>> step 6800\n",
      "step 6700-6799, precision 0.741554, recall 0.952987, f_score 0.834080\n",
      "=== total 6800 match 4647\n",
      ">>> step 6900\n",
      "step 6800-6899, precision 0.738593, recall 0.953220, f_score 0.832292\n",
      "=== total 6900 match 4690\n",
      ">>> step 7000\n",
      "step 6900-6999, precision 0.730914, recall 0.953488, f_score 0.827496\n",
      "=== total 7000 match 4768\n",
      ">>> step 7100\n",
      "step 7000-7099, precision 0.729136, recall 0.952909, f_score 0.826138\n",
      "=== total 7100 match 4829\n",
      ">>> step 7200\n",
      "step 7100-7199, precision 0.727198, recall 0.952228, f_score 0.824637\n",
      "=== total 7200 match 4879\n",
      ">>> step 7300\n",
      "step 7200-7299, precision 0.727143, recall 0.950745, f_score 0.824045\n",
      "=== total 7300 match 4911\n",
      ">>> step 7400\n",
      "step 7300-7399, precision 0.728517, recall 0.948140, f_score 0.823944\n",
      "=== total 7400 match 4969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 7500\n",
      "step 7400-7499, precision 0.723642, recall 0.948195, f_score 0.820838\n",
      "=== total 7500 match 5008\n",
      ">>> step 7600\n",
      "step 7500-7599, precision 0.723205, recall 0.948123, f_score 0.820530\n",
      "=== total 7600 match 5029\n",
      ">>> step 7700\n",
      "step 7600-7699, precision 0.721961, recall 0.948236, f_score 0.819771\n",
      "=== total 7700 match 5100\n",
      ">>> step 7800\n",
      "step 7700-7799, precision 0.719271, recall 0.948606, f_score 0.818172\n",
      "=== total 7800 match 5158\n",
      ">>> step 7900\n",
      "step 7800-7899, precision 0.717333, recall 0.948907, f_score 0.817028\n",
      "=== total 7900 match 5204\n",
      ">>> step 8000\n",
      "step 7900-7999, precision 0.717454, recall 0.948206, f_score 0.816846\n",
      "=== total 8000 match 5231\n",
      ">>> step 8100\n",
      "step 8000-8099, precision 0.718441, recall 0.942159, f_score 0.815230\n",
      "=== total 8100 match 5260\n",
      ">>> step 8200\n",
      "step 8100-8199, precision 0.717507, recall 0.936449, f_score 0.812487\n",
      "=== total 8200 match 5278\n",
      ">>> step 8300\n",
      "step 8200-8299, precision 0.716786, recall 0.929024, f_score 0.809220\n",
      "=== total 8300 match 5314\n",
      ">>> step 8400\n",
      "step 8300-8399, precision 0.715746, recall 0.927711, f_score 0.808060\n",
      "=== total 8400 match 5379\n",
      ">>> step 8500\n",
      "step 8400-8499, precision 0.715834, recall 0.928299, f_score 0.808339\n",
      "=== total 8500 match 5444\n",
      ">>> step 8600\n",
      "step 8500-8599, precision 0.717297, recall 0.929160, f_score 0.809597\n",
      "=== total 8600 match 5504\n",
      ">>> step 8700\n",
      "step 8600-8699, precision 0.716185, recall 0.929371, f_score 0.808968\n",
      "=== total 8700 match 5567\n",
      ">>> step 8800\n",
      "step 8700-8799, precision 0.717935, recall 0.930345, f_score 0.810454\n",
      "=== total 8800 match 5637\n",
      ">>> step 8900\n",
      "step 8800-8899, precision 0.717692, recall 0.931074, f_score 0.810575\n",
      "=== total 8900 match 5703\n",
      ">>> step 9000\n",
      "step 8900-8999, precision 0.719128, recall 0.932078, f_score 0.811872\n",
      "=== total 9000 match 5782\n",
      ">>> step 9100\n",
      "step 9000-9099, precision 0.719101, recall 0.933068, f_score 0.812230\n",
      "=== total 9100 match 5874\n",
      ">>> step 9200\n",
      "step 9100-9199, precision 0.721306, recall 0.933566, f_score 0.813823\n",
      "=== total 9200 match 5942\n",
      ">>> step 9300\n",
      "step 9200-9299, precision 0.721780, recall 0.934451, f_score 0.814461\n",
      "=== total 9300 match 6024\n",
      ">>> step 9400\n",
      "step 9300-9399, precision 0.721948, recall 0.932625, f_score 0.813874\n",
      "=== total 9400 match 6078\n",
      ">>> step 9500\n",
      "step 9400-9499, precision 0.722613, recall 0.925545, f_score 0.811586\n",
      "=== total 9500 match 6107\n",
      ">>> step 9600\n",
      "step 9500-9599, precision 0.723481, recall 0.920207, f_score 0.810071\n",
      "=== total 9600 match 6137\n",
      ">>> step 9700\n",
      "step 9600-9699, precision 0.724827, recall 0.921171, f_score 0.811288\n",
      "=== total 9700 match 6207\n",
      ">>> step 9800\n",
      "step 9700-9799, precision 0.723258, recall 0.921780, f_score 0.810540\n",
      "=== total 9800 match 6273\n",
      ">>> step 9900\n",
      "step 9800-9899, precision 0.717604, recall 0.921843, f_score 0.807002\n",
      "=== total 9900 match 6328\n",
      ">>> step 10000\n",
      "step 9900-9999, precision 0.715920, recall 0.921541, f_score 0.805820\n",
      "=== total 10000 match 6382\n",
      ">>> step 10100\n",
      "step 10000-10099, precision 0.719913, recall 0.922955, f_score 0.808887\n",
      "=== total 10100 match 6473\n",
      ">>> step 10200\n",
      "step 10100-10199, precision 0.719245, recall 0.923919, f_score 0.808835\n",
      "=== total 10200 match 6568\n",
      ">>> step 10300\n",
      "step 10200-10299, precision 0.720175, recall 0.924845, f_score 0.809778\n",
      "=== total 10300 match 6647\n",
      ">>> step 10400\n",
      "step 10300-10399, precision 0.718224, recall 0.925322, f_score 0.808725\n",
      "=== total 10400 match 6711\n",
      ">>> step 10500\n",
      "step 10400-10499, precision 0.717994, recall 0.926003, f_score 0.808839\n",
      "=== total 10500 match 6780\n",
      ">>> step 10600\n",
      "step 10500-10599, precision 0.718787, recall 0.926521, f_score 0.809540\n",
      "=== total 10600 match 6824\n",
      ">>> step 10700\n",
      "step 10600-10699, precision 0.718447, recall 0.927249, f_score 0.809602\n",
      "=== total 10700 match 6901\n",
      ">>> step 10800\n",
      "step 10700-10799, precision 0.719609, recall 0.927843, f_score 0.810566\n",
      "=== total 10800 match 6951\n",
      ">>> step 10900\n",
      "step 10800-10899, precision 0.721433, recall 0.928820, f_score 0.812095\n",
      "=== total 10900 match 7036\n",
      ">>> step 11000\n",
      "step 10900-10999, precision 0.722934, recall 0.929412, f_score 0.813272\n",
      "=== total 11000 match 7103\n",
      ">>> step 11100\n",
      "step 11000-11099, precision 0.720950, recall 0.929420, f_score 0.812018\n",
      "=== total 11100 match 7160\n",
      ">>> step 11200\n",
      "step 11100-11199, precision 0.718993, recall 0.929534, f_score 0.810819\n",
      "=== total 11200 match 7192\n",
      ">>> step 11300\n",
      "step 11200-11299, precision 0.721886, recall 0.930533, f_score 0.813037\n",
      "=== total 11300 match 7274\n",
      ">>> step 11400\n",
      "step 11300-11399, precision 0.724437, recall 0.931636, f_score 0.815075\n",
      "=== total 11400 match 7374\n",
      ">>> step 11500\n",
      "step 11400-11499, precision 0.717770, recall 0.931802, f_score 0.810901\n",
      "=== total 11500 match 7462\n",
      ">>> step 11600\n",
      "step 11500-11599, precision 0.716374, recall 0.932203, f_score 0.810161\n",
      "=== total 11600 match 7524\n",
      ">>> step 11700\n",
      "step 11600-11699, precision 0.714624, recall 0.932658, f_score 0.809212\n",
      "=== total 11700 match 7597\n",
      ">>> step 11800\n",
      "step 11700-11799, precision 0.711725, recall 0.932773, f_score 0.807393\n",
      "=== total 11800 match 7642\n",
      ">>> step 11900\n",
      "step 11800-11899, precision 0.712466, recall 0.933254, f_score 0.808050\n",
      "=== total 11900 match 7693\n",
      ">>> step 12000\n",
      "step 11900-11999, precision 0.711638, recall 0.933367, f_score 0.807559\n",
      "=== total 12000 match 7716\n",
      ">>> step 12100\n",
      "step 12000-12099, precision 0.708323, recall 0.933447, f_score 0.805450\n",
      "=== total 12100 match 7762\n",
      ">>> step 12200\n",
      "step 12100-12199, precision 0.704798, recall 0.933559, f_score 0.803208\n",
      "=== total 12200 match 7815\n",
      ">>> step 12300\n",
      "step 12200-12299, precision 0.706002, recall 0.933870, f_score 0.804104\n",
      "=== total 12300 match 7881\n",
      ">>> step 12400\n",
      "step 12300-12399, precision 0.707262, recall 0.934066, f_score 0.804994\n",
      "=== total 12400 match 7932\n",
      ">>> step 12500\n",
      "step 12400-12499, precision 0.705809, recall 0.934372, f_score 0.804165\n",
      "=== total 12500 match 7988\n",
      ">>> step 12600\n",
      "step 12500-12599, precision 0.702194, recall 0.934675, f_score 0.801925\n",
      "=== total 12600 match 8069\n",
      ">>> step 12700\n",
      "step 12600-12699, precision 0.702709, recall 0.933268, f_score 0.801742\n",
      "=== total 12700 match 8120\n",
      ">>> step 12800\n",
      "step 12700-12799, precision 0.702779, recall 0.933648, f_score 0.801928\n",
      "=== total 12800 match 8169\n",
      ">>> step 12900\n",
      "step 12800-12899, precision 0.703295, recall 0.934181, f_score 0.802461\n",
      "=== total 12900 match 8254\n",
      ">>> step 13000\n",
      "step 12900-12999, precision 0.704264, recall 0.934790, f_score 0.803316\n",
      "=== total 13000 match 8325\n",
      ">>> step 13100\n",
      "step 13000-13099, precision 0.703585, recall 0.935254, f_score 0.803045\n",
      "=== total 13100 match 8397\n",
      ">>> step 13200\n",
      "step 13100-13199, precision 0.705269, recall 0.935742, f_score 0.804321\n",
      "=== total 13200 match 8445\n",
      ">>> step 13300\n",
      "step 13200-13299, precision 0.706289, recall 0.936491, f_score 0.805261\n",
      "=== total 13300 match 8539\n",
      ">>> step 13400\n",
      "step 13300-13399, precision 0.706674, recall 0.936327, f_score 0.805451\n",
      "=== total 13400 match 8615\n",
      ">>> step 13500\n",
      "step 13400-13499, precision 0.707872, recall 0.936765, f_score 0.806390\n",
      "=== total 13500 match 8664\n",
      ">>> step 13600\n",
      "step 13500-13599, precision 0.707857, recall 0.937387, f_score 0.806611\n",
      "=== total 13600 match 8756\n",
      ">>> step 13700\n",
      "step 13600-13699, precision 0.708465, recall 0.937828, f_score 0.807169\n",
      "=== total 13700 match 8836\n",
      ">>> step 13800\n",
      "step 13700-13799, precision 0.709913, recall 0.938482, f_score 0.808350\n",
      "=== total 13800 match 8918\n",
      ">>> step 13900\n",
      "step 13800-13899, precision 0.711735, recall 0.938982, f_score 0.809716\n",
      "=== total 13900 match 9016\n",
      ">>> step 14000\n",
      "step 13900-13999, precision 0.713076, recall 0.939302, f_score 0.810703\n",
      "=== total 14000 match 9093\n",
      ">>> step 14100\n",
      "step 14000-14099, precision 0.713990, recall 0.939768, f_score 0.811467\n",
      "=== total 14100 match 9178\n",
      ">>> step 14200\n",
      "step 14100-14199, precision 0.714626, recall 0.940222, f_score 0.812047\n",
      "=== total 14200 match 9244\n",
      ">>> step 14300\n",
      "step 14200-14299, precision 0.711731, recall 0.940231, f_score 0.810178\n",
      "=== total 14300 match 9283\n",
      ">>> step 14400\n",
      "step 14300-14399, precision 0.709781, recall 0.940324, f_score 0.808948\n",
      "=== total 14400 match 9324\n",
      ">>> step 14500\n",
      "step 14400-14499, precision 0.709516, recall 0.940602, f_score 0.808878\n",
      "=== total 14500 match 9374\n",
      ">>> step 14600\n",
      "step 14500-14599, precision 0.708959, recall 0.940636, f_score 0.808529\n",
      "=== total 14600 match 9387\n",
      ">>> step 14700\n",
      "step 14600-14699, precision 0.708090, recall 0.940686, f_score 0.807982\n",
      "=== total 14700 match 9407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 14800\n",
      "step 14700-14799, precision 0.707565, recall 0.940166, f_score 0.807448\n",
      "=== total 14800 match 9438\n",
      ">>> step 14900\n",
      "step 14800-14899, precision 0.705839, recall 0.940326, f_score 0.806382\n",
      "=== total 14900 match 9488\n",
      ">>> step 15000\n",
      "step 14900-14999, precision 0.704553, recall 0.940359, f_score 0.805554\n",
      "=== total 15000 match 9511\n",
      ">>> step 15100\n",
      "step 15000-15099, precision 0.704569, recall 0.940543, f_score 0.805632\n",
      "=== total 15100 match 9542\n",
      ">>> step 15200\n",
      "step 15100-15199, precision 0.703480, recall 0.940783, f_score 0.805007\n",
      "=== total 15200 match 9598\n",
      ">>> step 15300\n",
      "step 15200-15299, precision 0.702700, recall 0.940907, f_score 0.804542\n",
      "=== total 15300 match 9630\n",
      ">>> step 15400\n",
      "step 15300-15399, precision 0.699948, recall 0.941005, f_score 0.802771\n",
      "=== total 15400 match 9685\n",
      ">>> step 15500\n",
      "step 15400-15499, precision 0.701037, recall 0.941387, f_score 0.803626\n",
      "=== total 15500 match 9737\n",
      ">>> step 15600\n",
      "step 15500-15599, precision 0.700236, recall 0.941404, f_score 0.803105\n",
      "=== total 15600 match 9751\n",
      ">>> step 15700\n",
      "step 15600-15699, precision 0.700010, recall 0.941459, f_score 0.802977\n",
      "=== total 15700 match 9787\n",
      ">>> step 15800\n",
      "step 15700-15799, precision 0.699237, recall 0.941612, f_score 0.802523\n",
      "=== total 15800 match 9825\n",
      ">>> step 15900\n",
      "step 15800-15899, precision 0.697104, recall 0.941922, f_score 0.801229\n",
      "=== total 15900 match 9911\n",
      ">>> step 16000\n",
      "step 15900-15999, precision 0.697460, recall 0.942086, f_score 0.801523\n",
      "=== total 16000 match 9959\n",
      ">>> step 16100\n",
      "step 16000-16099, precision 0.696933, recall 0.942328, f_score 0.801263\n",
      "=== total 16100 match 10011\n",
      ">>> step 16200\n",
      "step 16100-16199, precision 0.696608, recall 0.942403, f_score 0.801075\n",
      "=== total 16200 match 10053\n",
      ">>> step 16300\n",
      "step 16200-16299, precision 0.696399, recall 0.942701, f_score 0.801044\n",
      "=== total 16300 match 10135\n",
      ">>> step 16400\n",
      "step 16300-16399, precision 0.695887, recall 0.942812, f_score 0.800746\n",
      "=== total 16400 match 10187\n",
      ">>> step 16500\n",
      "step 16400-16499, precision 0.696172, recall 0.942971, f_score 0.800991\n",
      "=== total 16500 match 10213\n",
      ">>> step 16600\n",
      "step 16500-16599, precision 0.696106, recall 0.943144, f_score 0.801011\n",
      "=== total 16600 match 10247\n",
      ">>> step 16700\n",
      "step 16600-16699, precision 0.693010, recall 0.943182, f_score 0.798970\n",
      "=== total 16700 match 10300\n",
      ">>> step 16800\n",
      "step 16700-16799, precision 0.692241, recall 0.943443, f_score 0.798553\n",
      "=== total 16800 match 10362\n",
      ">>> step 16900\n",
      "step 16800-16899, precision 0.692478, recall 0.942648, f_score 0.798425\n",
      "=== total 16900 match 10396\n",
      ">>> step 17000\n",
      "step 16900-16999, precision 0.692610, recall 0.941322, f_score 0.798037\n",
      "=== total 17000 match 10446\n",
      ">>> step 17100\n",
      "step 17000-17099, precision 0.693277, recall 0.940658, f_score 0.798241\n",
      "=== total 17100 match 10472\n",
      ">>> step 17200\n",
      "step 17100-17199, precision 0.692803, recall 0.940118, f_score 0.797732\n",
      "=== total 17200 match 10560\n",
      ">>> step 17300\n",
      "step 17200-17299, precision 0.693395, recall 0.940447, f_score 0.798243\n",
      "=== total 17300 match 10613\n",
      ">>> step 17400\n",
      "step 17300-17399, precision 0.694684, recall 0.940923, f_score 0.799268\n",
      "=== total 17400 match 10684\n",
      ">>> step 17500\n",
      "step 17400-17499, precision 0.695846, recall 0.941531, f_score 0.800256\n",
      "=== total 17500 match 10784\n",
      ">>> step 17600\n",
      "step 17500-17599, precision 0.696986, recall 0.942126, f_score 0.801225\n",
      "=== total 17600 match 10884\n",
      ">>> step 17700\n",
      "step 17600-17699, precision 0.697539, recall 0.942481, f_score 0.801718\n",
      "=== total 17700 match 10970\n",
      ">>> step 17800\n",
      "step 17700-17799, precision 0.698739, recall 0.941471, f_score 0.802145\n",
      "=== total 17800 match 11027\n",
      ">>> step 17900\n",
      "step 17800-17899, precision 0.699865, recall 0.941768, f_score 0.802994\n",
      "=== total 17900 match 11115\n",
      ">>> step 18000\n",
      "step 17900-17999, precision 0.701100, recall 0.941961, f_score 0.803876\n",
      "=== total 18000 match 11181\n",
      ">>> step 18100\n",
      "step 18000-18099, precision 0.700587, recall 0.942266, f_score 0.803650\n",
      "=== total 18100 match 11252\n",
      ">>> step 18200\n",
      "step 18100-18199, precision 0.702021, recall 0.942766, f_score 0.804774\n",
      "=== total 18200 match 11333\n",
      ">>> step 18300\n",
      "step 18200-18299, precision 0.702622, recall 0.943143, f_score 0.805307\n",
      "=== total 18300 match 11403\n",
      ">>> step 18400\n",
      "step 18300-18399, precision 0.704899, recall 0.943616, f_score 0.806974\n",
      "=== total 18400 match 11491\n",
      ">>> step 18500\n",
      "step 18400-18499, precision 0.705740, recall 0.944111, f_score 0.807706\n",
      "=== total 18500 match 11585\n",
      ">>> step 18600\n",
      "step 18500-18599, precision 0.708258, recall 0.944749, f_score 0.809587\n",
      "=== total 18600 match 11685\n",
      ">>> step 18700\n",
      "step 18600-18699, precision 0.706342, recall 0.944950, f_score 0.808407\n",
      "=== total 18700 match 11762\n",
      ">>> step 18800\n",
      "step 18700-18799, precision 0.705902, recall 0.945050, f_score 0.808155\n",
      "=== total 18800 match 11792\n",
      ">>> step 18900\n",
      "step 18800-18899, precision 0.703657, recall 0.945218, f_score 0.806743\n",
      "=== total 18900 match 11868\n",
      ">>> step 19000\n",
      "step 18900-18999, precision 0.705041, recall 0.945616, f_score 0.807797\n",
      "=== total 19000 match 11961\n",
      ">>> step 19100\n",
      "step 19000-19099, precision 0.702973, recall 0.945670, f_score 0.806458\n",
      "=== total 19100 match 12009\n",
      ">>> step 19200\n",
      "step 19100-19199, precision 0.701907, recall 0.945810, f_score 0.805807\n",
      "=== total 19200 match 12060\n",
      ">>> step 19300\n",
      "step 19200-19299, precision 0.699638, recall 0.945991, f_score 0.804375\n",
      "=== total 19300 match 12142\n",
      ">>> step 19400\n",
      "step 19300-19399, precision 0.699279, recall 0.946120, f_score 0.804184\n",
      "=== total 19400 match 12204\n",
      ">>> step 19500\n",
      "step 19400-19499, precision 0.697822, recall 0.946251, f_score 0.803267\n",
      "=== total 19500 match 12261\n",
      ">>> step 19600\n",
      "step 19500-19599, precision 0.697773, recall 0.946417, f_score 0.803294\n",
      "=== total 19600 match 12302\n",
      ">>> step 19700\n",
      "step 19600-19699, precision 0.697935, recall 0.946821, f_score 0.803547\n",
      "=== total 19700 match 12398\n",
      ">>> step 19800\n",
      "step 19700-19799, precision 0.697924, recall 0.947139, f_score 0.803655\n",
      "=== total 19800 match 12477\n",
      ">>> step 19900\n",
      "step 19800-19899, precision 0.699362, recall 0.947494, f_score 0.804735\n",
      "=== total 19900 match 12540\n",
      ">>> step 20000\n",
      "step 19900-19999, precision 0.699317, recall 0.947719, f_score 0.804787\n",
      "=== total 20000 match 12598\n",
      ">>> step 20100\n",
      "step 20000-20099, precision 0.698556, recall 0.947132, f_score 0.804070\n",
      "=== total 20100 match 12669\n",
      ">>> step 20200\n",
      "step 20100-20199, precision 0.699662, recall 0.947385, f_score 0.804895\n",
      "=== total 20200 match 12739\n",
      ">>> step 20300\n",
      "step 20200-20299, precision 0.700749, recall 0.947646, f_score 0.805708\n",
      "=== total 20300 match 12812\n",
      ">>> step 20400\n",
      "step 20300-20399, precision 0.700576, recall 0.947811, f_score 0.805652\n",
      "=== total 20400 match 12858\n",
      ">>> step 20500\n",
      "step 20400-20499, precision 0.699815, recall 0.948079, f_score 0.805246\n",
      "=== total 20500 match 12942\n",
      ">>> step 20600\n",
      "step 20500-20599, precision 0.700652, recall 0.948489, f_score 0.805948\n",
      "=== total 20600 match 13035\n",
      ">>> step 20700\n",
      "step 20600-20699, precision 0.701113, recall 0.948808, f_score 0.806368\n",
      "=== total 20700 match 13112\n",
      ">>> step 20800\n",
      "step 20700-20799, precision 0.703353, recall 0.949326, f_score 0.808035\n",
      "=== total 20800 match 13211\n",
      ">>> step 20900\n",
      "step 20800-20899, precision 0.704255, recall 0.949427, f_score 0.808667\n",
      "=== total 20900 match 13302\n",
      ">>> step 21000\n",
      "step 20900-20999, precision 0.706159, recall 0.949890, f_score 0.810089\n",
      "=== total 21000 match 13395\n",
      ">>> step 21100\n",
      "step 21000-21099, precision 0.706197, recall 0.950175, f_score 0.810217\n",
      "=== total 21100 match 13475\n",
      ">>> step 21200\n",
      "step 21100-21199, precision 0.707504, recall 0.950579, f_score 0.811224\n",
      "=== total 21200 match 13566\n",
      ">>> step 21300\n",
      "step 21200-21299, precision 0.709047, recall 0.950918, f_score 0.812361\n",
      "=== total 21300 match 13662\n",
      ">>> step 21400\n",
      "step 21300-21399, precision 0.710144, recall 0.951329, f_score 0.813231\n",
      "=== total 21400 match 13762\n",
      ">>> step 21500\n",
      "step 21400-21499, precision 0.711342, recall 0.951644, f_score 0.814131\n",
      "=== total 21500 match 13833\n",
      ">>> step 21600\n",
      "step 21500-21599, precision 0.712900, recall 0.952038, f_score 0.815296\n",
      "=== total 21600 match 13922\n",
      ">>> step 21700\n",
      "step 21600-21699, precision 0.711874, recall 0.952162, f_score 0.814669\n",
      "=== total 21700 match 13980\n",
      ">>> step 21800\n",
      "step 21700-21799, precision 0.709202, recall 0.952222, f_score 0.812938\n",
      "=== total 21800 match 14051\n",
      ">>> step 21900\n",
      "step 21800-21899, precision 0.708168, recall 0.952249, f_score 0.812268\n",
      "=== total 21900 match 14080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 22000\n",
      "step 21900-21999, precision 0.708602, recall 0.951969, f_score 0.812452\n",
      "=== total 22000 match 14125\n",
      ">>> step 22100\n",
      "step 22000-22099, precision 0.708868, recall 0.951912, f_score 0.812606\n",
      "=== total 22100 match 14186\n",
      ">>> step 22200\n",
      "step 22100-22199, precision 0.707848, recall 0.951949, f_score 0.811949\n",
      "=== total 22200 match 14246\n",
      ">>> step 22300\n",
      "step 22200-22299, precision 0.706935, recall 0.952076, f_score 0.811394\n",
      "=== total 22300 match 14304\n",
      ">>> step 22400\n",
      "step 22300-22399, precision 0.706398, recall 0.952229, f_score 0.811096\n",
      "=== total 22400 match 14363\n",
      ">>> step 22500\n",
      "step 22400-22499, precision 0.706584, recall 0.952185, f_score 0.811202\n",
      "=== total 22500 match 14430\n",
      ">>> step 22600\n",
      "step 22500-22599, precision 0.705139, recall 0.952243, f_score 0.810270\n",
      "=== total 22600 match 14478\n",
      ">>> step 22700\n",
      "step 22600-22699, precision 0.704160, recall 0.952306, f_score 0.809646\n",
      "=== total 22700 match 14518\n",
      ">>> step 22800\n",
      "step 22700-22799, precision 0.702757, recall 0.952306, f_score 0.808718\n",
      "=== total 22800 match 14547\n",
      ">>> step 22900\n",
      "step 22800-22899, precision 0.701807, recall 0.952434, f_score 0.808135\n",
      "=== total 22900 match 14608\n",
      ">>> step 23000\n",
      "step 22900-22999, precision 0.700744, recall 0.952505, f_score 0.807455\n",
      "=== total 23000 match 14653\n",
      ">>> step 23100\n",
      "step 23000-23099, precision 0.699593, recall 0.952491, f_score 0.806685\n",
      "=== total 23100 match 14730\n",
      ">>> step 23200\n",
      "step 23100-23199, precision 0.698953, recall 0.952591, f_score 0.806296\n",
      "=== total 23200 match 14805\n",
      ">>> step 23300\n",
      "step 23200-23299, precision 0.698426, recall 0.952643, f_score 0.805963\n",
      "=== total 23300 match 14862\n",
      ">>> step 23400\n",
      "step 23300-23399, precision 0.698564, recall 0.952773, f_score 0.806102\n",
      "=== total 23400 match 14902\n",
      ">>> step 23500\n",
      "step 23400-23499, precision 0.698497, recall 0.952815, f_score 0.806073\n",
      "=== total 23500 match 14975\n",
      ">>> step 23600\n",
      "step 23500-23599, precision 0.696521, recall 0.952948, f_score 0.804802\n",
      "=== total 23600 match 15062\n",
      ">>> step 23700\n",
      "step 23600-23699, precision 0.694988, recall 0.953033, f_score 0.803808\n",
      "=== total 23700 match 15124\n",
      ">>> step 23800\n",
      "step 23700-23799, precision 0.694883, recall 0.953090, f_score 0.803758\n",
      "=== total 23800 match 15204\n",
      ">>> step 23900\n",
      "step 23800-23899, precision 0.695137, recall 0.953326, f_score 0.804012\n",
      "=== total 23900 match 15279\n",
      ">>> step 24000\n",
      "step 23900-23999, precision 0.696924, recall 0.953728, f_score 0.805350\n",
      "=== total 24000 match 15379\n",
      ">>> step 24100\n",
      "step 24000-24099, precision 0.696018, recall 0.953606, f_score 0.804701\n",
      "=== total 24100 match 15445\n",
      ">>> step 24200\n",
      "step 24100-24199, precision 0.695652, recall 0.953680, f_score 0.804483\n",
      "=== total 24200 match 15479\n",
      ">>> step 24300\n",
      "step 24200-24299, precision 0.694087, recall 0.953727, f_score 0.803452\n",
      "=== total 24300 match 15560\n",
      ">>> step 24400\n",
      "step 24300-24399, precision 0.693386, recall 0.953765, f_score 0.802995\n",
      "=== total 24400 match 15619\n",
      ">>> step 24500\n",
      "step 24400-24499, precision 0.692572, recall 0.953972, f_score 0.802522\n",
      "=== total 24500 match 15711\n",
      ">>> step 24600\n",
      "step 24500-24599, precision 0.693493, recall 0.954188, f_score 0.803217\n",
      "=== total 24600 match 15768\n",
      ">>> step 24700\n",
      "step 24600-24699, precision 0.694362, recall 0.954439, f_score 0.803889\n",
      "=== total 24700 match 15839\n",
      ">>> step 24800\n",
      "step 24700-24799, precision 0.694497, recall 0.954663, f_score 0.804058\n",
      "=== total 24800 match 15918\n",
      ">>> step 24900\n",
      "step 24800-24899, precision 0.695772, recall 0.954994, f_score 0.805030\n",
      "=== total 24900 match 16011\n",
      ">>> step 25000\n",
      "step 24900-24999, precision 0.696025, recall 0.954941, f_score 0.805181\n",
      "=== total 25000 match 16077\n",
      ">>> step 25100\n",
      "step 25000-25099, precision 0.696658, recall 0.955032, f_score 0.805636\n",
      "=== total 25100 match 16127\n",
      ">>> step 25200\n",
      "step 25100-25199, precision 0.697403, recall 0.955306, f_score 0.806232\n",
      "=== total 25200 match 16213\n",
      ">>> step 25300\n",
      "step 25200-25299, precision 0.697954, recall 0.955432, f_score 0.806645\n",
      "=== total 25300 match 16279\n",
      ">>> step 25400\n",
      "step 25300-25399, precision 0.698180, recall 0.955693, f_score 0.806889\n",
      "=== total 25400 match 16374\n",
      ">>> step 25500\n",
      "step 25400-25499, precision 0.698857, recall 0.955835, f_score 0.807391\n",
      "=== total 25500 match 16444\n",
      ">>> step 25600\n",
      "step 25500-25599, precision 0.699673, recall 0.955846, f_score 0.807940\n",
      "=== total 25600 match 16522\n",
      ">>> step 25700\n",
      "step 25600-25699, precision 0.700856, recall 0.955786, f_score 0.808706\n",
      "=== total 25700 match 16594\n",
      ">>> step 25800\n",
      "step 25700-25799, precision 0.701427, recall 0.955793, f_score 0.809089\n",
      "=== total 25800 match 16676\n",
      ">>> step 25900\n",
      "step 25800-25899, precision 0.702472, recall 0.956038, f_score 0.809871\n",
      "=== total 25900 match 16748\n",
      ">>> step 26000\n",
      "step 25900-25999, precision 0.701087, recall 0.956187, f_score 0.809003\n",
      "=== total 26000 match 16841\n",
      ">>> step 26100\n",
      "step 26000-26099, precision 0.698528, recall 0.956223, f_score 0.807310\n",
      "=== total 26100 match 16917\n",
      ">>> step 26200\n",
      "step 26100-26199, precision 0.697223, recall 0.956181, f_score 0.806423\n",
      "=== total 26200 match 16963\n",
      ">>> step 26300\n",
      "step 26200-26299, precision 0.697386, recall 0.956343, f_score 0.806590\n",
      "=== total 26300 match 17025\n",
      ">>> step 26400\n",
      "step 26300-26399, precision 0.696767, recall 0.956350, f_score 0.806178\n",
      "=== total 26400 match 17043\n",
      ">>> step 26500\n",
      "step 26400-26499, precision 0.695683, recall 0.956354, f_score 0.805453\n",
      "=== total 26500 match 17071\n",
      ">>> step 26600\n",
      "step 26500-26599, precision 0.694922, recall 0.956364, f_score 0.804947\n",
      "=== total 26600 match 17094\n",
      ">>> step 26700\n",
      "step 26600-26699, precision 0.694607, recall 0.956438, f_score 0.804761\n",
      "=== total 26700 match 17132\n",
      ">>> step 26800\n",
      "step 26700-26799, precision 0.693702, recall 0.956459, f_score 0.804161\n",
      "=== total 26800 match 17163\n",
      ">>> step 26900\n",
      "step 26800-26899, precision 0.692831, recall 0.956218, f_score 0.803490\n",
      "=== total 26900 match 17212\n",
      ">>> step 27000\n",
      "step 26900-26999, precision 0.692290, recall 0.956278, f_score 0.803147\n",
      "=== total 27000 match 17250\n",
      ">>> step 27100\n",
      "step 27000-27099, precision 0.692481, recall 0.956126, f_score 0.803223\n",
      "=== total 27100 match 17277\n",
      ">>> step 27200\n",
      "step 27100-27199, precision 0.692192, recall 0.956203, f_score 0.803055\n",
      "=== total 27200 match 17316\n",
      ">>> step 27300\n",
      "step 27200-27299, precision 0.692547, recall 0.956228, f_score 0.803303\n",
      "=== total 27300 match 17349\n",
      ">>> step 27400\n",
      "step 27300-27399, precision 0.693213, recall 0.956429, f_score 0.803822\n",
      "=== total 27400 match 17416\n",
      ">>> step 27500\n",
      "step 27400-27499, precision 0.692167, recall 0.956511, f_score 0.803147\n",
      "=== total 27500 match 17477\n",
      ">>> step 27600\n",
      "step 27500-27599, precision 0.691623, recall 0.956515, f_score 0.802782\n",
      "=== total 27600 match 17524\n",
      ">>> step 27700\n",
      "step 27600-27699, precision 0.691108, recall 0.956583, f_score 0.802459\n",
      "=== total 27700 match 17566\n",
      ">>> step 27800\n",
      "step 27700-27799, precision 0.691227, recall 0.956645, f_score 0.802561\n",
      "=== total 27800 match 17589\n",
      ">>> step 27900\n",
      "step 27800-27899, precision 0.691012, recall 0.956740, f_score 0.802450\n",
      "=== total 27900 match 17635\n",
      ">>> step 28000\n",
      "step 27900-27999, precision 0.689149, recall 0.956788, f_score 0.801208\n",
      "=== total 28000 match 17703\n",
      ">>> step 28100\n",
      "step 28000-28099, precision 0.687141, recall 0.956805, f_score 0.799856\n",
      "=== total 28100 match 17762\n",
      ">>> step 28200\n",
      "step 28100-28199, precision 0.686829, recall 0.956849, f_score 0.799660\n",
      "=== total 28200 match 17789\n",
      ">>> step 28300\n",
      "step 28200-28299, precision 0.685706, recall 0.956376, f_score 0.798733\n",
      "=== total 28300 match 17840\n",
      ">>> step 28400\n",
      "step 28300-28399, precision 0.686390, recall 0.956478, f_score 0.799232\n",
      "=== total 28400 match 17898\n",
      ">>> step 28500\n",
      "step 28400-28499, precision 0.687437, recall 0.956458, f_score 0.799935\n",
      "=== total 28500 match 17958\n",
      ">>> step 28600\n",
      "step 28500-28599, precision 0.687323, recall 0.956458, f_score 0.799857\n",
      "=== total 28600 match 17961\n",
      ">>> step 28700\n",
      "step 28600-28699, precision 0.686209, recall 0.956508, f_score 0.799121\n",
      "=== total 28700 match 18012\n",
      ">>> step 28800\n",
      "step 28700-28799, precision 0.684982, recall 0.956572, f_score 0.798310\n",
      "=== total 28800 match 18072\n",
      ">>> step 28900\n",
      "step 28800-28899, precision 0.684830, recall 0.956616, f_score 0.798222\n",
      "=== total 28900 match 18095\n",
      ">>> step 29000\n",
      "step 28900-28999, precision 0.685334, recall 0.956713, f_score 0.798598\n",
      "=== total 29000 match 18124\n",
      ">>> step 29100\n",
      "step 29000-29099, precision 0.683890, recall 0.956826, f_score 0.797656\n",
      "=== total 29100 match 18212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 29200\n",
      "step 29100-29199, precision 0.682907, recall 0.956938, f_score 0.797026\n",
      "=== total 29200 match 18288\n",
      ">>> step 29300\n",
      "step 29200-29299, precision 0.683170, recall 0.957024, f_score 0.797235\n",
      "=== total 29300 match 18319\n",
      ">>> step 29400\n",
      "step 29300-29399, precision 0.683895, recall 0.957161, f_score 0.797776\n",
      "=== total 29400 match 18361\n",
      ">>> step 29500\n",
      "step 29400-29499, precision 0.682625, recall 0.957223, f_score 0.796933\n",
      "=== total 29500 match 18423\n",
      ">>> step 29600\n",
      "step 29500-29599, precision 0.681479, recall 0.957369, f_score 0.796202\n",
      "=== total 29600 match 18520\n",
      ">>> step 29700\n",
      "step 29600-29699, precision 0.679933, recall 0.957492, f_score 0.795188\n",
      "=== total 29700 match 18618\n",
      ">>> step 29800\n",
      "step 29700-29799, precision 0.679513, recall 0.957495, f_score 0.794902\n",
      "=== total 29800 match 18631\n",
      ">>> step 29900\n",
      "step 29800-29899, precision 0.680266, recall 0.957642, f_score 0.795467\n",
      "=== total 29900 match 18678\n",
      ">>> step 30000\n",
      "step 29900-29999, precision 0.680421, recall 0.957341, f_score 0.795469\n",
      "=== total 30000 match 18734\n",
      ">>> step 30100\n",
      "step 30000-30099, precision 0.679016, recall 0.957359, f_score 0.794515\n",
      "=== total 30100 match 18814\n",
      ">>> step 30200\n",
      "step 30100-30199, precision 0.679003, recall 0.957386, f_score 0.794515\n",
      "=== total 30200 match 18860\n",
      ">>> step 30300\n",
      "step 30200-30299, precision 0.679886, recall 0.957551, f_score 0.795176\n",
      "=== total 30300 match 18912\n",
      ">>> step 30400\n",
      "step 30300-30399, precision 0.679577, recall 0.957728, f_score 0.795026\n",
      "=== total 30400 match 19003\n",
      ">>> step 30500\n",
      "step 30400-30499, precision 0.679509, recall 0.957049, f_score 0.794745\n",
      "=== total 30500 match 19052\n",
      ">>> step 30600\n",
      "step 30500-30599, precision 0.679520, recall 0.956772, f_score 0.794657\n",
      "=== total 30600 match 19087\n",
      ">>> step 30700\n",
      "step 30600-30699, precision 0.679749, recall 0.956890, f_score 0.794855\n",
      "=== total 30700 match 19135\n",
      ">>> step 30800\n",
      "step 30700-30799, precision 0.679239, recall 0.956988, f_score 0.794540\n",
      "=== total 30800 match 19195\n",
      ">>> step 30900\n",
      "step 30800-30899, precision 0.679033, recall 0.957142, f_score 0.794452\n",
      "=== total 30900 match 19273\n",
      ">>> step 31000\n",
      "step 30900-30999, precision 0.679146, recall 0.957219, f_score 0.794555\n",
      "=== total 31000 match 19339\n",
      ">>> step 31100\n",
      "step 31000-31099, precision 0.680420, recall 0.957504, f_score 0.795525\n",
      "=== total 31100 match 19438\n",
      ">>> step 31200\n",
      "step 31100-31199, precision 0.679908, recall 0.957623, f_score 0.795216\n",
      "=== total 31200 match 19510\n",
      ">>> step 31300\n",
      "step 31200-31299, precision 0.680868, recall 0.957815, f_score 0.795939\n",
      "=== total 31300 match 19575\n",
      ">>> step 31400\n",
      "step 31300-31399, precision 0.680902, recall 0.957895, f_score 0.795989\n",
      "=== total 31400 match 19646\n",
      ">>> step 31500\n",
      "step 31400-31499, precision 0.681315, recall 0.958042, f_score 0.796323\n",
      "=== total 31500 match 19706\n",
      ">>> step 31600\n",
      "step 31500-31599, precision 0.681634, recall 0.958090, f_score 0.796557\n",
      "=== total 31600 match 19754\n",
      ">>> step 31700\n",
      "step 31600-31699, precision 0.682179, recall 0.958197, f_score 0.796966\n",
      "=== total 31700 match 19791\n",
      ">>> step 31800\n",
      "step 31700-31799, precision 0.682102, recall 0.958280, f_score 0.796942\n",
      "=== total 31800 match 19868\n",
      ">>> step 31900\n",
      "step 31800-31899, precision 0.682701, recall 0.958454, f_score 0.797411\n",
      "=== total 31900 match 19937\n",
      ">>> step 32000\n",
      "step 31900-31999, precision 0.682959, recall 0.958479, f_score 0.797595\n",
      "=== total 32000 match 20010\n",
      ">>> step 32100\n",
      "step 32000-32099, precision 0.683839, recall 0.958691, f_score 0.798269\n",
      "=== total 32100 match 20091\n",
      ">>> step 32200\n",
      "step 32100-32199, precision 0.684323, recall 0.958594, f_score 0.798565\n",
      "=== total 32200 match 20163\n",
      ">>> step 32300\n",
      "step 32200-32299, precision 0.685487, recall 0.958754, f_score 0.799412\n",
      "=== total 32300 match 20244\n",
      ">>> step 32400\n",
      "step 32300-32399, precision 0.685046, recall 0.958828, f_score 0.799138\n",
      "=== total 32400 match 20295\n",
      ">>> step 32500\n",
      "step 32400-32499, precision 0.685139, recall 0.958949, f_score 0.799244\n",
      "=== total 32500 match 20355\n",
      ">>> step 32600\n",
      "step 32500-32599, precision 0.684933, recall 0.959028, f_score 0.799131\n",
      "=== total 32600 match 20402\n",
      ">>> step 32700\n",
      "step 32600-32699, precision 0.685056, recall 0.959191, f_score 0.799271\n",
      "=== total 32700 match 20483\n",
      ">>> step 32800\n",
      "step 32700-32799, precision 0.684933, recall 0.959307, f_score 0.799228\n",
      "=== total 32800 match 20548\n",
      ">>> step 32900\n",
      "step 32800-32899, precision 0.685813, recall 0.959514, f_score 0.799898\n",
      "=== total 32900 match 20631\n",
      ">>> step 33000\n",
      "step 32900-32999, precision 0.684536, recall 0.959547, f_score 0.799041\n",
      "=== total 33000 match 20687\n",
      ">>> step 33100\n",
      "step 33000-33099, precision 0.683202, recall 0.959616, f_score 0.798155\n",
      "=== total 33100 match 20764\n",
      ">>> step 33200\n",
      "step 33100-33199, precision 0.682380, recall 0.959616, f_score 0.797594\n",
      "=== total 33200 match 20789\n",
      ">>> step 33300\n",
      "step 33200-33299, precision 0.681497, recall 0.959651, f_score 0.797003\n",
      "=== total 33300 match 20835\n",
      ">>> step 33400\n",
      "step 33300-33399, precision 0.679975, recall 0.959665, f_score 0.795965\n",
      "=== total 33400 match 20889\n",
      ">>> step 33500\n",
      "step 33400-33499, precision 0.679576, recall 0.959738, f_score 0.795717\n",
      "=== total 33500 match 20941\n",
      ">>> step 33600\n",
      "step 33500-33599, precision 0.679760, recall 0.959863, f_score 0.795886\n",
      "=== total 33600 match 21003\n",
      ">>> step 33700\n",
      "step 33600-33699, precision 0.678846, recall 0.959941, f_score 0.795286\n",
      "=== total 33700 match 21074\n",
      ">>> step 33800\n",
      "step 33700-33799, precision 0.678152, recall 0.959678, f_score 0.794720\n",
      "=== total 33800 match 21128\n",
      ">>> step 33900\n",
      "step 33800-33899, precision 0.679403, recall 0.959928, f_score 0.795663\n",
      "=== total 33900 match 21226\n",
      ">>> step 34000\n",
      "step 33900-33999, precision 0.678038, recall 0.959952, f_score 0.794735\n",
      "=== total 34000 match 21282\n",
      ">>> step 34100\n",
      "step 34000-34099, precision 0.677528, recall 0.959952, f_score 0.794385\n",
      "=== total 34100 match 21298\n",
      ">>> step 34200\n",
      "step 34100-34199, precision 0.677383, recall 0.960016, f_score 0.794307\n",
      "=== total 34200 match 21338\n",
      ">>> step 34300\n",
      "step 34200-34299, precision 0.676537, recall 0.960101, f_score 0.793753\n",
      "=== total 34300 match 21412\n",
      ">>> step 34400\n",
      "step 34300-34399, precision 0.676564, recall 0.960238, f_score 0.793819\n",
      "=== total 34400 match 21488\n",
      ">>> step 34500\n",
      "step 34400-34499, precision 0.676973, recall 0.960353, f_score 0.794140\n",
      "=== total 34500 match 21540\n",
      ">>> step 34600\n",
      "step 34500-34599, precision 0.676360, recall 0.960416, f_score 0.793740\n",
      "=== total 34600 match 21595\n",
      ">>> step 34700\n",
      "step 34600-34699, precision 0.677145, recall 0.960516, f_score 0.794314\n",
      "=== total 34700 match 21663\n",
      ">>> step 34800\n",
      "step 34700-34799, precision 0.677161, recall 0.960573, f_score 0.794344\n",
      "=== total 34800 match 21695\n",
      ">>> step 34900\n",
      "step 34800-34899, precision 0.677327, recall 0.960699, f_score 0.794502\n",
      "=== total 34900 match 21762\n",
      ">>> step 35000\n",
      "step 34900-34999, precision 0.677213, recall 0.960829, f_score 0.794468\n",
      "=== total 35000 match 21841\n",
      ">>> step 35100\n",
      "step 35000-35099, precision 0.676947, recall 0.960886, f_score 0.794304\n",
      "=== total 35100 match 21919\n",
      ">>> step 35200\n",
      "step 35100-35199, precision 0.676836, recall 0.961010, f_score 0.794270\n",
      "=== total 35200 match 21995\n",
      ">>> step 35300\n",
      "step 35200-35299, precision 0.676780, recall 0.961158, f_score 0.794282\n",
      "=== total 35300 match 22084\n",
      ">>> step 35400\n",
      "step 35300-35399, precision 0.677779, recall 0.961354, f_score 0.795037\n",
      "=== total 35400 match 22168\n",
      ">>> step 35500\n",
      "step 35400-35499, precision 0.678841, recall 0.961260, f_score 0.795735\n",
      "=== total 35500 match 22260\n",
      ">>> step 35600\n",
      "step 35500-35599, precision 0.679842, recall 0.961385, f_score 0.796465\n",
      "=== total 35600 match 22339\n",
      ">>> step 35700\n",
      "step 35600-35699, precision 0.681184, recall 0.961613, f_score 0.797464\n",
      "=== total 35700 match 22433\n",
      ">>> step 35800\n",
      "step 35700-35799, precision 0.681691, recall 0.961782, f_score 0.797869\n",
      "=== total 35800 match 22519\n",
      ">>> step 35900\n",
      "step 35800-35899, precision 0.682823, recall 0.961874, f_score 0.798676\n",
      "=== total 35900 match 22612\n",
      ">>> step 36000\n",
      "step 35900-35999, precision 0.683684, recall 0.962065, f_score 0.799331\n",
      "=== total 36000 match 22702\n",
      ">>> step 36100\n",
      "step 36000-36099, precision 0.683441, recall 0.962166, f_score 0.799199\n",
      "=== total 36100 match 22773\n",
      ">>> step 36200\n",
      "step 36100-36199, precision 0.681760, recall 0.962176, f_score 0.798052\n",
      "=== total 36200 match 22835\n",
      ">>> step 36300\n",
      "step 36200-36299, precision 0.681475, recall 0.962253, f_score 0.797883\n",
      "=== total 36300 match 22893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 36400\n",
      "step 36300-36399, precision 0.679916, recall 0.962290, f_score 0.796826\n",
      "=== total 36400 match 22969\n",
      ">>> step 36500\n",
      "step 36400-36499, precision 0.679034, recall 0.962320, f_score 0.796230\n",
      "=== total 36500 match 23018\n",
      ">>> step 36600\n",
      "step 36500-36599, precision 0.679123, recall 0.962403, f_score 0.796320\n",
      "=== total 36600 match 23068\n",
      ">>> step 36700\n",
      "step 36600-36699, precision 0.680059, recall 0.962460, f_score 0.796983\n",
      "=== total 36700 match 23148\n",
      ">>> step 36800\n",
      "step 36700-36799, precision 0.679736, recall 0.962538, f_score 0.796788\n",
      "=== total 36800 match 23209\n",
      ">>> step 36900\n",
      "step 36800-36899, precision 0.678921, recall 0.962611, f_score 0.796252\n",
      "=== total 36900 match 23284\n",
      ">>> step 37000\n",
      "step 36900-36999, precision 0.678310, recall 0.962654, f_score 0.795847\n",
      "=== total 37000 match 23333\n",
      ">>> step 37100\n",
      "step 37000-37099, precision 0.678448, recall 0.962763, f_score 0.795979\n",
      "=== total 37100 match 23399\n",
      ">>> step 37200\n",
      "step 37100-37199, precision 0.677666, recall 0.962810, f_score 0.795456\n",
      "=== total 37200 match 23457\n",
      ">>> step 37300\n",
      "step 37200-37299, precision 0.677765, recall 0.962954, f_score 0.795574\n",
      "=== total 37300 match 23548\n",
      ">>> step 37400\n",
      "step 37300-37399, precision 0.676993, recall 0.962523, f_score 0.794895\n",
      "=== total 37400 match 23597\n",
      ">>> step 37500\n",
      "step 37400-37499, precision 0.676917, recall 0.962586, f_score 0.794864\n",
      "=== total 37500 match 23641\n",
      ">>> step 37600\n",
      "step 37500-37599, precision 0.677947, recall 0.962602, f_score 0.795579\n",
      "=== total 37600 match 23729\n",
      ">>> step 37700\n",
      "step 37600-37699, precision 0.678345, recall 0.962508, f_score 0.795821\n",
      "=== total 37700 match 23805\n",
      ">>> step 37800\n",
      "step 37700-37799, precision 0.678251, recall 0.962577, f_score 0.795780\n",
      "=== total 37800 match 23854\n",
      ">>> step 37900\n",
      "step 37800-37899, precision 0.678038, recall 0.962622, f_score 0.795648\n",
      "=== total 37900 match 23891\n",
      ">>> step 38000\n",
      "step 37900-37999, precision 0.676112, recall 0.962626, f_score 0.794322\n",
      "=== total 38000 match 23962\n",
      ">>> step 38100\n",
      "step 38000-38099, precision 0.675898, recall 0.962637, f_score 0.794178\n",
      "=== total 38100 match 23977\n",
      ">>> step 38200\n",
      "step 38100-38199, precision 0.675738, recall 0.962657, f_score 0.794074\n",
      "=== total 38200 match 23996\n",
      ">>> step 38300\n",
      "step 38200-38299, precision 0.675904, recall 0.962321, f_score 0.794075\n",
      "=== total 38300 match 24070\n",
      ">>> step 38400\n",
      "step 38300-38399, precision 0.676088, recall 0.962421, f_score 0.794236\n",
      "=== total 38400 match 24130\n",
      ">>> step 38500\n",
      "step 38400-38499, precision 0.676368, recall 0.962466, f_score 0.794445\n",
      "=== total 38500 match 24188\n",
      ">>> step 38600\n",
      "step 38500-38599, precision 0.676666, recall 0.962603, f_score 0.794696\n",
      "=== total 38600 match 24269\n",
      ">>> step 38700\n",
      "step 38600-38699, precision 0.676757, recall 0.962723, f_score 0.794800\n",
      "=== total 38700 match 24347\n",
      ">>> step 38800\n",
      "step 38700-38799, precision 0.676463, recall 0.962499, f_score 0.794521\n",
      "=== total 38800 match 24396\n",
      ">>> step 38900\n",
      "step 38800-38899, precision 0.677216, recall 0.962515, f_score 0.795046\n",
      "=== total 38900 match 24456\n",
      ">>> step 39000\n",
      "step 38900-38999, precision 0.677992, recall 0.962633, f_score 0.795621\n",
      "=== total 39000 match 24546\n",
      ">>> step 39100\n",
      "step 39000-39099, precision 0.678136, recall 0.962202, f_score 0.795572\n",
      "=== total 39100 match 24588\n",
      ">>> step 39200\n",
      "step 39100-39199, precision 0.678248, recall 0.962239, f_score 0.795662\n",
      "=== total 39200 match 24609\n",
      ">>> step 39300\n",
      "step 39200-39299, precision 0.677355, recall 0.962324, f_score 0.795077\n",
      "=== total 39300 match 24699\n",
      ">>> step 39400\n",
      "step 39300-39399, precision 0.677117, recall 0.962389, f_score 0.794934\n",
      "=== total 39400 match 24752\n",
      ">>> step 39500\n",
      "step 39400-39499, precision 0.677322, recall 0.962471, f_score 0.795104\n",
      "=== total 39500 match 24839\n",
      ">>> step 39600\n",
      "step 39500-39599, precision 0.677817, recall 0.962589, f_score 0.795485\n",
      "=== total 39600 match 24902\n",
      ">>> step 39700\n",
      "step 39600-39699, precision 0.678119, recall 0.962666, f_score 0.795719\n",
      "=== total 39700 match 24944\n",
      ">>> step 39800\n",
      "step 39700-39799, precision 0.677359, recall 0.962666, f_score 0.795195\n",
      "=== total 39800 match 24972\n",
      ">>> step 39900\n",
      "step 39800-39899, precision 0.677756, recall 0.962784, f_score 0.795509\n",
      "=== total 39900 match 25040\n",
      ">>> step 40000\n",
      "step 39900-39999, precision 0.677969, recall 0.962921, f_score 0.795703\n",
      "=== total 40000 match 25128\n",
      ">>> step 40100\n",
      "step 40000-40099, precision 0.678257, recall 0.963030, f_score 0.795938\n",
      "=== total 40100 match 25194\n",
      ">>> step 40200\n",
      "step 40100-40199, precision 0.677487, recall 0.963084, f_score 0.795427\n",
      "=== total 40200 match 25261\n",
      ">>> step 40300\n",
      "step 40200-40299, precision 0.676330, recall 0.963105, f_score 0.794636\n",
      "=== total 40300 match 25319\n",
      ">>> step 40400\n",
      "step 40300-40399, precision 0.675058, recall 0.963127, f_score 0.793765\n",
      "=== total 40400 match 25383\n",
      ">>> step 40500\n",
      "step 40400-40499, precision 0.675261, recall 0.963220, f_score 0.793937\n",
      "=== total 40500 match 25442\n",
      ">>> step 40600\n",
      "step 40500-40599, precision 0.673078, recall 0.963227, f_score 0.792428\n",
      "=== total 40600 match 25529\n",
      ">>> step 40700\n",
      "step 40600-40699, precision 0.673024, recall 0.963231, f_score 0.792392\n",
      "=== total 40700 match 25534\n",
      ">>> step 40800\n",
      "step 40700-40799, precision 0.672559, recall 0.963258, f_score 0.792078\n",
      "=== total 40800 match 25571\n",
      ">>> step 40900\n",
      "step 40800-40899, precision 0.672609, recall 0.963270, f_score 0.792118\n",
      "=== total 40900 match 25578\n",
      ">>> step 41000\n",
      "step 40900-40999, precision 0.672329, recall 0.963297, f_score 0.791932\n",
      "=== total 41000 match 25608\n",
      ">>> step 41100\n",
      "step 41000-41099, precision 0.671902, recall 0.963346, f_score 0.791652\n",
      "=== total 41100 match 25660\n",
      ">>> step 41200\n",
      "step 41100-41199, precision 0.671379, recall 0.963346, f_score 0.791289\n",
      "=== total 41200 match 25680\n",
      ">>> step 41300\n",
      "step 41200-41299, precision 0.671124, recall 0.963236, f_score 0.791075\n",
      "=== total 41300 match 25727\n",
      ">>> step 41400\n",
      "step 41300-41399, precision 0.671268, recall 0.963115, f_score 0.791134\n",
      "=== total 41400 match 25790\n",
      ">>> step 41500\n",
      "step 41400-41499, precision 0.670549, recall 0.963043, f_score 0.790611\n",
      "=== total 41500 match 25843\n",
      ">>> step 41600\n",
      "step 41500-41599, precision 0.671016, recall 0.962938, f_score 0.790900\n",
      "=== total 41600 match 25904\n",
      ">>> step 41700\n",
      "step 41600-41699, precision 0.671329, recall 0.962486, f_score 0.790965\n",
      "=== total 41700 match 25950\n",
      ">>> step 41800\n",
      "step 41700-41799, precision 0.671276, recall 0.962499, f_score 0.790932\n",
      "=== total 41800 match 25961\n",
      ">>> step 41900\n",
      "step 41800-41899, precision 0.670087, recall 0.962462, f_score 0.790094\n",
      "=== total 41900 match 26019\n",
      ">>> step 42000\n",
      "step 41900-41999, precision 0.669495, recall 0.962462, f_score 0.789682\n",
      "=== total 42000 match 26042\n",
      ">>> step 42100\n",
      "step 42000-42099, precision 0.669033, recall 0.962462, f_score 0.789360\n",
      "=== total 42100 match 26060\n",
      ">>> step 42200\n",
      "step 42100-42199, precision 0.668570, recall 0.962491, f_score 0.789048\n",
      "=== total 42200 match 26099\n",
      ">>> step 42300\n",
      "step 42200-42299, precision 0.666730, recall 0.962491, f_score 0.787765\n",
      "=== total 42300 match 26171\n",
      ">>> step 42400\n",
      "step 42300-42399, precision 0.665777, recall 0.962501, f_score 0.787103\n",
      "=== total 42400 match 26216\n",
      ">>> step 42500\n",
      "step 42400-42499, precision 0.665244, recall 0.962408, f_score 0.786699\n",
      "=== total 42500 match 26246\n",
      ">>> step 42600\n",
      "step 42500-42599, precision 0.665143, recall 0.962367, f_score 0.786615\n",
      "=== total 42600 match 26259\n",
      ">>> step 42700\n",
      "step 42600-42699, precision 0.664463, recall 0.962197, f_score 0.786082\n",
      "=== total 42700 match 26316\n",
      ">>> step 42800\n",
      "step 42700-42799, precision 0.664324, recall 0.962044, f_score 0.785934\n",
      "=== total 42800 match 26326\n",
      ">>> step 42900\n",
      "step 42800-42899, precision 0.664187, recall 0.960799, f_score 0.785423\n",
      "=== total 42900 match 26348\n",
      ">>> step 43000\n",
      "step 42900-42999, precision 0.663862, recall 0.960570, f_score 0.785119\n",
      "=== total 43000 match 26385\n",
      ">>> step 43100\n",
      "step 43000-43099, precision 0.664133, recall 0.960518, f_score 0.785291\n",
      "=== total 43100 match 26448\n",
      ">>> step 43200\n",
      "step 43100-43199, precision 0.663921, recall 0.960275, f_score 0.785062\n",
      "=== total 43200 match 26470\n",
      ">>> step 43300\n",
      "step 43200-43299, precision 0.664265, recall 0.959928, f_score 0.785186\n",
      "=== total 43300 match 26506\n",
      ">>> step 43400\n",
      "step 43300-43399, precision 0.664417, recall 0.959959, f_score 0.785302\n",
      "=== total 43400 match 26521\n",
      ">>> step 43500\n",
      "step 43400-43499, precision 0.663756, recall 0.959991, f_score 0.784851\n",
      "=== total 43500 match 26570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 43600\n",
      "step 43500-43599, precision 0.662113, recall 0.960004, f_score 0.783706\n",
      "=== total 43600 match 26645\n",
      ">>> step 43700\n",
      "step 43600-43699, precision 0.662017, recall 0.960030, f_score 0.783647\n",
      "=== total 43700 match 26667\n",
      ">>> step 43800\n",
      "step 43700-43799, precision 0.661694, recall 0.960030, f_score 0.783421\n",
      "=== total 43800 match 26680\n",
      ">>> step 43900\n",
      "step 43800-43899, precision 0.661699, recall 0.959804, f_score 0.783349\n",
      "=== total 43900 match 26704\n",
      ">>> step 44000\n",
      "step 43900-43999, precision 0.660618, recall 0.959857, f_score 0.782609\n",
      "=== total 44000 match 26784\n",
      ">>> step 44100\n",
      "step 44000-44099, precision 0.660500, recall 0.959575, f_score 0.782432\n",
      "=== total 44100 match 26810\n",
      ">>> step 44200\n",
      "step 44100-44199, precision 0.660630, recall 0.959700, f_score 0.782565\n",
      "=== total 44200 match 26891\n",
      ">>> step 44300\n",
      "step 44200-44299, precision 0.659736, recall 0.959778, f_score 0.781963\n",
      "=== total 44300 match 26982\n",
      ">>> step 44400\n",
      "step 44300-44399, precision 0.660065, recall 0.959739, f_score 0.782181\n",
      "=== total 44400 match 27014\n",
      ">>> step 44500\n",
      "step 44400-44499, precision 0.659967, recall 0.959811, f_score 0.782137\n",
      "=== total 44500 match 27068\n",
      ">>> step 44600\n",
      "step 44500-44599, precision 0.660357, recall 0.959882, f_score 0.782434\n",
      "=== total 44600 match 27102\n",
      ">>> step 44700\n",
      "step 44600-44699, precision 0.659859, recall 0.959955, f_score 0.782108\n",
      "=== total 44700 match 27174\n",
      ">>> step 44800\n",
      "step 44700-44799, precision 0.659621, recall 0.959981, f_score 0.781949\n",
      "=== total 44800 match 27202\n",
      ">>> step 44900\n",
      "step 44800-44899, precision 0.660706, recall 0.959502, f_score 0.782552\n",
      "=== total 44900 match 27289\n",
      ">>> step 45000\n",
      "step 44900-44999, precision 0.660575, recall 0.959364, f_score 0.782414\n",
      "=== total 45000 match 27305\n",
      ">>> step 45100\n",
      "step 45000-45099, precision 0.659735, recall 0.959398, f_score 0.781837\n",
      "=== total 45100 match 27364\n",
      ">>> step 45200\n",
      "step 45100-45199, precision 0.658937, recall 0.959439, f_score 0.781289\n",
      "=== total 45200 match 27426\n",
      ">>> step 45300\n",
      "step 45200-45299, precision 0.658418, recall 0.959540, f_score 0.780958\n",
      "=== total 45300 match 27519\n",
      ">>> step 45400\n",
      "step 45300-45399, precision 0.658441, recall 0.959594, f_score 0.780992\n",
      "=== total 45400 match 27556\n",
      ">>> step 45500\n",
      "step 45400-45499, precision 0.658589, recall 0.959673, f_score 0.781122\n",
      "=== total 45500 match 27606\n",
      ">>> step 45600\n",
      "step 45500-45599, precision 0.658091, recall 0.959739, f_score 0.780793\n",
      "=== total 45600 match 27674\n",
      ">>> step 45700\n",
      "step 45600-45699, precision 0.658350, recall 0.959783, f_score 0.780990\n",
      "=== total 45700 match 27695\n",
      ">>> step 45800\n",
      "step 45700-45799, precision 0.658085, recall 0.959823, f_score 0.780818\n",
      "=== total 45800 match 27735\n",
      ">>> step 45900\n",
      "step 45800-45899, precision 0.657238, recall 0.959849, f_score 0.780229\n",
      "=== total 45900 match 27789\n",
      ">>> step 46000\n",
      "step 45900-45999, precision 0.657031, recall 0.959666, f_score 0.780023\n",
      "=== total 46000 match 27848\n",
      ">>> step 46100\n",
      "step 46000-46099, precision 0.657661, recall 0.959787, f_score 0.780506\n",
      "=== total 46100 match 27908\n",
      ">>> step 46200\n",
      "step 46100-46199, precision 0.657811, recall 0.959929, f_score 0.780659\n",
      "=== total 46200 match 28005\n",
      ">>> step 46300\n",
      "step 46200-46299, precision 0.658106, recall 0.959790, f_score 0.780821\n",
      "=== total 46300 match 28073\n",
      ">>> step 46400\n",
      "step 46300-46399, precision 0.658097, recall 0.959857, f_score 0.780837\n",
      "=== total 46400 match 28122\n",
      ">>> step 46500\n",
      "step 46400-46499, precision 0.658313, recall 0.959940, f_score 0.781016\n",
      "=== total 46500 match 28210\n",
      ">>> step 46600\n",
      "step 46500-46599, precision 0.658051, recall 0.959973, f_score 0.780843\n",
      "=== total 46600 match 28282\n",
      ">>> step 46700\n",
      "step 46600-46699, precision 0.658538, recall 0.959977, f_score 0.781187\n",
      "=== total 46700 match 28337\n",
      ">>> step 46800\n",
      "step 46700-46799, precision 0.659166, recall 0.959899, f_score 0.781603\n",
      "=== total 46800 match 28398\n",
      ">>> step 46900\n",
      "step 46800-46899, precision 0.659416, recall 0.959945, f_score 0.781794\n",
      "=== total 46900 match 28457\n",
      ">>> step 47000\n",
      "step 46900-46999, precision 0.660001, recall 0.960112, f_score 0.782260\n",
      "=== total 47000 match 28556\n",
      ">>> step 47100\n",
      "step 47000-47099, precision 0.660342, recall 0.960217, f_score 0.782535\n",
      "=== total 47100 match 28620\n",
      ">>> step 47200\n",
      "step 47100-47199, precision 0.660452, recall 0.960326, f_score 0.782648\n",
      "=== total 47200 match 28697\n",
      ">>> step 47300\n",
      "step 47200-47299, precision 0.660969, recall 0.960469, f_score 0.783058\n",
      "=== total 47300 match 28782\n",
      ">>> step 47400\n",
      "step 47300-47399, precision 0.661414, recall 0.960614, f_score 0.783418\n",
      "=== total 47400 match 28873\n",
      ">>> step 47500\n",
      "step 47400-47499, precision 0.661762, recall 0.960738, f_score 0.783704\n",
      "=== total 47500 match 28953\n",
      ">>> step 47600\n",
      "step 47500-47599, precision 0.662109, recall 0.960862, f_score 0.783988\n",
      "=== total 47600 match 29033\n",
      ">>> step 47700\n",
      "step 47600-47699, precision 0.662750, recall 0.960996, f_score 0.784483\n",
      "=== total 47700 match 29109\n",
      ">>> step 47800\n",
      "step 47700-47799, precision 0.662139, recall 0.960928, f_score 0.784032\n",
      "=== total 47800 match 29157\n",
      ">>> step 47900\n",
      "step 47800-47899, precision 0.662059, recall 0.961013, f_score 0.784004\n",
      "=== total 47900 match 29227\n",
      ">>> step 48000\n",
      "step 47900-47999, precision 0.662938, recall 0.960993, f_score 0.784613\n",
      "=== total 48000 match 29321\n",
      ">>> step 48100\n",
      "step 48000-48099, precision 0.662583, recall 0.961054, f_score 0.784385\n",
      "=== total 48100 match 29385\n",
      ">>> step 48200\n",
      "step 48100-48199, precision 0.662197, recall 0.961129, f_score 0.784140\n",
      "=== total 48200 match 29461\n",
      ">>> step 48300\n",
      "step 48200-48299, precision 0.662922, recall 0.961287, f_score 0.784700\n",
      "=== total 48300 match 29554\n",
      ">>> step 48400\n",
      "step 48300-48399, precision 0.663585, recall 0.961435, f_score 0.785214\n",
      "=== total 48400 match 29642\n",
      ">>> step 48500\n",
      "step 48400-48499, precision 0.664492, recall 0.961561, f_score 0.785890\n",
      "=== total 48500 match 29740\n",
      ">>> step 48600\n",
      "step 48500-48599, precision 0.665203, recall 0.961719, f_score 0.786440\n",
      "=== total 48600 match 29836\n",
      ">>> step 48700\n",
      "step 48600-48699, precision 0.666210, recall 0.961889, f_score 0.787200\n",
      "=== total 48700 match 29929\n",
      ">>> step 48800\n",
      "step 48700-48799, precision 0.666222, recall 0.961961, f_score 0.787233\n",
      "=== total 48800 match 29987\n",
      ">>> step 48900\n",
      "step 48800-48899, precision 0.665047, recall 0.961979, f_score 0.786418\n",
      "=== total 48900 match 30055\n",
      ">>> step 49000\n",
      "step 48900-48999, precision 0.664960, recall 0.961962, f_score 0.786352\n",
      "=== total 49000 match 30083\n",
      ">>> step 49100\n",
      "step 49000-49099, precision 0.665772, recall 0.961695, f_score 0.786829\n",
      "=== total 49100 match 30168\n",
      ">>> step 49200\n",
      "step 49100-49199, precision 0.665465, recall 0.961627, f_score 0.786592\n",
      "=== total 49200 match 30239\n",
      ">>> step 49300\n",
      "step 49200-49299, precision 0.664820, recall 0.961553, f_score 0.786117\n",
      "=== total 49300 match 30321\n",
      ">>> step 49400\n",
      "step 49300-49399, precision 0.664284, recall 0.961482, f_score 0.785718\n",
      "=== total 49400 match 30362\n",
      ">>> step 49500\n",
      "step 49400-49499, precision 0.664388, recall 0.961202, f_score 0.785698\n",
      "=== total 49500 match 30428\n",
      ">>> step 49600\n",
      "step 49500-49599, precision 0.663521, recall 0.961268, f_score 0.785113\n",
      "=== total 49600 match 30522\n",
      ">>> step 49700\n",
      "step 49600-49699, precision 0.663417, recall 0.961312, f_score 0.785055\n",
      "=== total 49700 match 30563\n",
      ">>> step 49800\n",
      "step 49700-49799, precision 0.662783, recall 0.961369, f_score 0.784630\n",
      "=== total 49800 match 30639\n",
      ">>> step 49900\n",
      "step 49800-49899, precision 0.662438, recall 0.961380, f_score 0.784392\n",
      "=== total 49900 match 30664\n",
      ">>> step 50000\n",
      "step 49900-49999, precision 0.662738, recall 0.961462, f_score 0.784630\n",
      "=== total 50000 match 30718\n",
      ">>> step 50100\n",
      "step 50000-50099, precision 0.662889, recall 0.961513, f_score 0.784752\n",
      "=== total 50100 match 30791\n",
      ">>> step 50200\n",
      "step 50100-50199, precision 0.662504, recall 0.961448, f_score 0.784461\n",
      "=== total 50200 match 30830\n",
      ">>> step 50300\n",
      "step 50200-50299, precision 0.661770, recall 0.961500, f_score 0.783964\n",
      "=== total 50300 match 30908\n",
      ">>> step 50400\n",
      "step 50300-50399, precision 0.661638, recall 0.961553, f_score 0.783888\n",
      "=== total 50400 match 30958\n",
      ">>> step 50500\n",
      "step 50400-50499, precision 0.661819, recall 0.961535, f_score 0.784009\n",
      "=== total 50500 match 31010\n",
      ">>> step 50600\n",
      "step 50500-50599, precision 0.661672, recall 0.961634, f_score 0.783939\n",
      "=== total 50600 match 31100\n",
      ">>> step 50700\n",
      "step 50600-50699, precision 0.662017, recall 0.961757, f_score 0.784222\n",
      "=== total 50700 match 31188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 50800\n",
      "step 50700-50799, precision 0.662649, recall 0.961903, f_score 0.784714\n",
      "=== total 50800 match 31282\n",
      ">>> step 50900\n",
      "step 50800-50899, precision 0.663256, recall 0.962043, f_score 0.785186\n",
      "=== total 50900 match 31374\n",
      ">>> step 51000\n",
      "step 50900-50999, precision 0.663402, recall 0.962133, f_score 0.785318\n",
      "=== total 51000 match 31444\n",
      ">>> step 51100\n",
      "step 51000-51099, precision 0.663196, recall 0.962192, f_score 0.785194\n",
      "=== total 51100 match 31505\n",
      ">>> step 51200\n",
      "step 51100-51199, precision 0.662936, recall 0.962222, f_score 0.785021\n",
      "=== total 51200 match 31543\n",
      ">>> step 51300\n",
      "step 51200-51299, precision 0.663406, recall 0.962301, f_score 0.785377\n",
      "=== total 51300 match 31590\n",
      ">>> step 51400\n",
      "step 51300-51399, precision 0.663389, recall 0.962339, f_score 0.785377\n",
      "=== total 51400 match 31624\n",
      ">>> step 51500\n",
      "step 51400-51499, precision 0.662992, recall 0.962357, f_score 0.785105\n",
      "=== total 51500 match 31658\n",
      ">>> step 51600\n",
      "step 51500-51599, precision 0.663062, recall 0.962426, f_score 0.785177\n",
      "=== total 51600 match 31715\n",
      ">>> step 51700\n",
      "step 51600-51699, precision 0.663763, recall 0.962563, f_score 0.785714\n",
      "=== total 51700 match 31802\n",
      ">>> step 51800\n",
      "step 51700-51799, precision 0.663572, recall 0.962639, f_score 0.785606\n",
      "=== total 51800 match 31879\n",
      ">>> step 51900\n",
      "step 51800-51899, precision 0.663872, recall 0.962748, f_score 0.785852\n",
      "=== total 51900 match 31961\n",
      ">>> step 52000\n",
      "step 51900-51999, precision 0.664117, recall 0.962837, f_score 0.786053\n",
      "=== total 52000 match 32029\n",
      ">>> step 52100\n",
      "step 52000-52099, precision 0.663580, recall 0.962857, f_score 0.785684\n",
      "=== total 52100 match 32073\n",
      ">>> step 52200\n",
      "step 52100-52199, precision 0.662797, recall 0.962878, f_score 0.785141\n",
      "=== total 52200 match 32129\n",
      ">>> step 52300\n",
      "step 52200-52299, precision 0.663042, recall 0.962916, f_score 0.785327\n",
      "=== total 52300 match 32191\n",
      ">>> step 52400\n",
      "step 52300-52399, precision 0.662810, recall 0.962958, f_score 0.785178\n",
      "=== total 52400 match 32240\n",
      ">>> step 52500\n",
      "step 52400-52499, precision 0.662688, recall 0.962408, f_score 0.784909\n",
      "=== total 52500 match 32258\n",
      ">>> step 52600\n",
      "step 52500-52599, precision 0.662250, recall 0.962263, f_score 0.784554\n",
      "=== total 52600 match 32305\n",
      ">>> step 52700\n",
      "step 52600-52699, precision 0.662122, recall 0.962299, f_score 0.784475\n",
      "=== total 52700 match 32343\n",
      ">>> step 52800\n",
      "step 52700-52799, precision 0.662658, recall 0.962422, f_score 0.784892\n",
      "=== total 52800 match 32427\n",
      ">>> step 52900\n",
      "step 52800-52899, precision 0.662560, recall 0.962337, f_score 0.784796\n",
      "=== total 52900 match 32471\n",
      ">>> step 53000\n",
      "step 52900-52999, precision 0.662545, recall 0.962384, f_score 0.784801\n",
      "=== total 53000 match 32514\n",
      ">>> step 53100\n",
      "step 53000-53099, precision 0.662490, recall 0.962438, f_score 0.784780\n",
      "=== total 53100 match 32565\n",
      ">>> step 53200\n",
      "step 53100-53199, precision 0.661987, recall 0.962478, f_score 0.784440\n",
      "=== total 53200 match 32626\n",
      ">>> step 53300\n",
      "step 53200-53299, precision 0.661913, recall 0.962531, f_score 0.784406\n",
      "=== total 53300 match 32678\n",
      ">>> step 53400\n",
      "step 53300-53399, precision 0.662092, recall 0.962583, f_score 0.784549\n",
      "=== total 53400 match 32716\n",
      ">>> step 53500\n",
      "step 53400-53499, precision 0.662149, recall 0.962634, f_score 0.784606\n",
      "=== total 53500 match 32760\n",
      ">>> step 53600\n",
      "step 53500-53599, precision 0.661620, recall 0.962676, f_score 0.784248\n",
      "=== total 53600 match 32824\n",
      ">>> step 53700\n",
      "step 53600-53699, precision 0.661393, recall 0.962710, f_score 0.784100\n",
      "=== total 53700 match 32867\n",
      ">>> step 53800\n",
      "step 53700-53799, precision 0.661897, recall 0.962791, f_score 0.784481\n",
      "=== total 53800 match 32916\n",
      ">>> step 53900\n",
      "step 53800-53899, precision 0.661989, recall 0.962628, f_score 0.784492\n",
      "=== total 53900 match 32996\n",
      ">>> step 54000\n",
      "step 53900-53999, precision 0.661596, recall 0.962684, f_score 0.784234\n",
      "=== total 54000 match 33067\n",
      ">>> step 54100\n",
      "step 54000-54099, precision 0.661403, recall 0.962730, f_score 0.784114\n",
      "=== total 54100 match 33119\n",
      ">>> step 54200\n",
      "step 54100-54199, precision 0.661639, recall 0.962778, f_score 0.784295\n",
      "=== total 54200 match 33151\n",
      ">>> step 54300\n",
      "step 54200-54299, precision 0.661829, recall 0.962856, f_score 0.784455\n",
      "=== total 54300 match 33214\n",
      ">>> step 54400\n",
      "step 54300-54399, precision 0.660994, recall 0.962859, f_score 0.783869\n",
      "=== total 54400 match 33259\n",
      ">>> step 54500\n",
      "step 54400-54499, precision 0.660887, recall 0.962874, f_score 0.783799\n",
      "=== total 54500 match 33278\n",
      ">>> step 54600\n",
      "step 54500-54599, precision 0.660882, recall 0.962568, f_score 0.783694\n",
      "=== total 54600 match 33307\n",
      ">>> step 54700\n",
      "step 54600-54699, precision 0.660794, recall 0.962576, f_score 0.783635\n",
      "=== total 54700 match 33319\n",
      ">>> step 54800\n",
      "step 54700-54799, precision 0.661048, recall 0.962155, f_score 0.783674\n",
      "=== total 54800 match 33344\n",
      ">>> step 54900\n",
      "step 54800-54899, precision 0.660467, recall 0.961711, f_score 0.783118\n",
      "=== total 54900 match 33390\n",
      ">>> step 55000\n",
      "step 54900-54999, precision 0.660253, recall 0.961644, f_score 0.782945\n",
      "=== total 55000 match 33416\n",
      ">>> step 55100\n",
      "step 55000-55099, precision 0.660217, recall 0.961664, f_score 0.782926\n",
      "=== total 55100 match 33436\n",
      ">>> step 55200\n",
      "step 55100-55199, precision 0.660204, recall 0.961707, f_score 0.782932\n",
      "=== total 55200 match 33476\n",
      ">>> step 55300\n",
      "step 55200-55299, precision 0.659875, recall 0.961741, f_score 0.782712\n",
      "=== total 55300 match 33523\n",
      ">>> step 55400\n",
      "step 55300-55399, precision 0.659861, recall 0.961825, f_score 0.782730\n",
      "=== total 55400 match 33601\n",
      ">>> step 55500\n",
      "step 55400-55499, precision 0.659648, recall 0.961839, f_score 0.782584\n",
      "=== total 55500 match 33624\n",
      ">>> step 55600\n",
      "step 55500-55599, precision 0.659939, recall 0.961761, f_score 0.782763\n",
      "=== total 55600 match 33691\n",
      ">>> step 55700\n",
      "step 55600-55699, precision 0.659332, recall 0.961761, f_score 0.782336\n",
      "=== total 55700 match 33722\n",
      ">>> step 55800\n",
      "step 55700-55799, precision 0.658714, recall 0.961794, f_score 0.781912\n",
      "=== total 55800 match 33784\n",
      ">>> step 55900\n",
      "step 55800-55899, precision 0.658860, recall 0.961841, f_score 0.782030\n",
      "=== total 55900 match 33819\n",
      ">>> step 56000\n",
      "step 55900-55999, precision 0.658781, recall 0.961926, f_score 0.782003\n",
      "=== total 56000 match 33902\n",
      ">>> step 56100\n",
      "step 56000-56099, precision 0.658893, recall 0.961995, f_score 0.782104\n",
      "=== total 56100 match 33960\n",
      ">>> step 56200\n",
      "step 56100-56199, precision 0.659727, recall 0.962102, f_score 0.782727\n",
      "=== total 56200 match 34055\n",
      ">>> step 56300\n",
      "step 56200-56299, precision 0.660145, recall 0.962134, f_score 0.783032\n",
      "=== total 56300 match 34141\n",
      ">>> step 56400\n",
      "step 56300-56399, precision 0.660355, recall 0.962152, f_score 0.783186\n",
      "=== total 56400 match 34224\n",
      ">>> step 56500\n",
      "step 56400-56499, precision 0.660614, recall 0.962257, f_score 0.783402\n",
      "=== total 56500 match 34309\n",
      ">>> step 56600\n",
      "step 56500-56599, precision 0.660567, recall 0.961957, f_score 0.783270\n",
      "=== total 56600 match 34375\n",
      ">>> step 56700\n",
      "step 56600-56699, precision 0.660417, recall 0.960638, f_score 0.782727\n",
      "=== total 56700 match 34404\n",
      ">>> step 56800\n",
      "step 56700-56799, precision 0.660703, recall 0.960423, f_score 0.782857\n",
      "=== total 56800 match 34489\n",
      ">>> step 56900\n",
      "step 56800-56899, precision 0.660986, recall 0.960395, f_score 0.783045\n",
      "=== total 56900 match 34559\n",
      ">>> step 57000\n",
      "step 56900-56999, precision 0.661246, recall 0.960478, f_score 0.783256\n",
      "=== total 57000 match 34621\n",
      ">>> step 57100\n",
      "step 57000-57099, precision 0.660865, recall 0.960445, f_score 0.782977\n",
      "=== total 57100 match 34647\n",
      ">>> step 57200\n",
      "step 57100-57199, precision 0.660791, recall 0.960479, f_score 0.782937\n",
      "=== total 57200 match 34719\n",
      ">>> step 57300\n",
      "step 57200-57299, precision 0.661063, recall 0.960583, f_score 0.783162\n",
      "=== total 57300 match 34800\n",
      ">>> step 57400\n",
      "step 57300-57399, precision 0.661373, recall 0.960693, f_score 0.783416\n",
      "=== total 57400 match 34885\n",
      ">>> step 57500\n",
      "step 57400-57499, precision 0.661969, recall 0.960792, f_score 0.783867\n",
      "=== total 57500 match 34982\n",
      ">>> step 57600\n",
      "step 57500-57599, precision 0.662304, recall 0.960905, f_score 0.784140\n",
      "=== total 57600 match 35070\n",
      ">>> step 57700\n",
      "step 57600-57699, precision 0.661439, recall 0.960921, f_score 0.783538\n",
      "=== total 57700 match 35131\n",
      ">>> step 57800\n",
      "step 57700-57799, precision 0.660198, recall 0.960921, f_score 0.782667\n",
      "=== total 57800 match 35197\n",
      ">>> step 57900\n",
      "step 57800-57899, precision 0.659649, recall 0.960939, f_score 0.782287\n",
      "=== total 57900 match 35243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 58000\n",
      "step 57900-57999, precision 0.659775, recall 0.961013, f_score 0.782400\n",
      "=== total 58000 match 35306\n",
      ">>> step 58100\n",
      "step 58000-58099, precision 0.659084, recall 0.961013, f_score 0.781914\n",
      "=== total 58100 match 35343\n",
      ">>> step 58200\n",
      "step 58100-58199, precision 0.658450, recall 0.961013, f_score 0.781468\n",
      "=== total 58200 match 35377\n",
      ">>> step 58300\n",
      "step 58200-58299, precision 0.658357, recall 0.961013, f_score 0.781403\n",
      "=== total 58300 match 35382\n",
      ">>> step 58400\n",
      "step 58300-58399, precision 0.657948, recall 0.961013, f_score 0.781114\n",
      "=== total 58400 match 35404\n",
      ">>> step 58500\n",
      "step 58400-58499, precision 0.657366, recall 0.960668, f_score 0.780590\n",
      "=== total 58500 match 35446\n",
      ">>> step 58600\n",
      "step 58500-58599, precision 0.656385, recall 0.960668, f_score 0.779898\n",
      "=== total 58600 match 35499\n",
      ">>> step 58700\n",
      "step 58600-58699, precision 0.656003, recall 0.960651, f_score 0.779622\n",
      "=== total 58700 match 35541\n",
      ">>> step 58800\n",
      "step 58700-58799, precision 0.655279, recall 0.960349, f_score 0.779012\n",
      "=== total 58800 match 35594\n",
      ">>> step 58900\n",
      "step 58800-58899, precision 0.654553, recall 0.960300, f_score 0.778482\n",
      "=== total 58900 match 35661\n",
      ">>> step 59000\n",
      "step 58900-58999, precision 0.653953, recall 0.960344, f_score 0.778072\n",
      "=== total 59000 match 35735\n",
      ">>> step 59100\n",
      "step 59000-59099, precision 0.653295, recall 0.960344, f_score 0.777606\n",
      "=== total 59100 match 35771\n",
      ">>> step 59200\n",
      "step 59100-59199, precision 0.652887, recall 0.960379, f_score 0.777329\n",
      "=== total 59200 match 35827\n",
      ">>> step 59300\n",
      "step 59200-59299, precision 0.652707, recall 0.959977, f_score 0.777070\n",
      "=== total 59300 match 35866\n",
      ">>> step 59400\n",
      "step 59300-59399, precision 0.652649, recall 0.959874, f_score 0.776994\n",
      "=== total 59400 match 35883\n",
      ">>> step 59500\n",
      "step 59400-59499, precision 0.652553, recall 0.959962, f_score 0.776956\n",
      "=== total 59500 match 35971\n",
      ">>> step 59600\n",
      "step 59500-59599, precision 0.651709, recall 0.959629, f_score 0.776248\n",
      "=== total 59600 match 36036\n",
      ">>> step 59700\n",
      "step 59600-59699, precision 0.651376, recall 0.959435, f_score 0.775948\n",
      "=== total 59700 match 36056\n",
      ">>> step 59800\n",
      "step 59700-59799, precision 0.651115, recall 0.959226, f_score 0.775695\n",
      "=== total 59800 match 36095\n",
      ">>> step 59900\n",
      "step 59800-59899, precision 0.651055, recall 0.959256, f_score 0.775662\n",
      "=== total 59900 match 36126\n",
      ">>> step 60000\n",
      "step 59900-59999, precision 0.651203, recall 0.959291, f_score 0.775779\n",
      "=== total 60000 match 36150\n",
      ">>> step 60100\n",
      "step 60000-60099, precision 0.651219, recall 0.959309, f_score 0.775796\n",
      "=== total 60100 match 36166\n",
      ">>> step 60200\n",
      "step 60100-60199, precision 0.651031, recall 0.959311, f_score 0.775663\n",
      "=== total 60200 match 36178\n",
      ">>> step 60300\n",
      "step 60200-60299, precision 0.651427, recall 0.959382, f_score 0.775967\n",
      "=== total 60300 match 36222\n",
      ">>> step 60400\n",
      "step 60300-60399, precision 0.651127, recall 0.959395, f_score 0.775758\n",
      "=== total 60400 match 36251\n",
      ">>> step 60500\n",
      "step 60400-60499, precision 0.650051, recall 0.959395, f_score 0.774994\n",
      "=== total 60500 match 36311\n",
      ">>> step 60600\n",
      "step 60500-60599, precision 0.649651, recall 0.959049, f_score 0.774597\n",
      "=== total 60600 match 36338\n",
      ">>> step 60700\n",
      "step 60600-60699, precision 0.649049, recall 0.959063, f_score 0.774173\n",
      "=== total 60700 match 36384\n",
      ">>> step 60800\n",
      "step 60700-60799, precision 0.648141, recall 0.959103, f_score 0.773540\n",
      "=== total 60800 match 36472\n",
      ">>> step 60900\n",
      "step 60800-60899, precision 0.648012, recall 0.959169, f_score 0.773470\n",
      "=== total 60900 match 36541\n",
      ">>> step 61000\n",
      "step 60900-60999, precision 0.648067, recall 0.959212, f_score 0.773523\n",
      "=== total 61000 match 36578\n",
      ">>> step 61100\n",
      "step 61000-61099, precision 0.647892, recall 0.959215, f_score 0.773399\n",
      "=== total 61100 match 36591\n",
      ">>> step 61200\n",
      "step 61100-61199, precision 0.648338, recall 0.959241, f_score 0.773725\n",
      "=== total 61200 match 36663\n",
      ">>> step 61300\n",
      "step 61200-61299, precision 0.648424, recall 0.959296, f_score 0.773804\n",
      "=== total 61300 match 36709\n",
      ">>> step 61400\n",
      "step 61300-61399, precision 0.647893, recall 0.959310, f_score 0.773431\n",
      "=== total 61400 match 36753\n",
      ">>> step 61500\n",
      "step 61400-61499, precision 0.648012, recall 0.959340, f_score 0.773525\n",
      "=== total 61500 match 36774\n",
      ">>> step 61600\n",
      "step 61500-61599, precision 0.647514, recall 0.959366, f_score 0.773179\n",
      "=== total 61600 match 36827\n",
      ">>> step 61700\n",
      "step 61600-61699, precision 0.646826, recall 0.959395, f_score 0.772698\n",
      "=== total 61700 match 36894\n",
      ">>> step 61800\n",
      "step 61700-61799, precision 0.646476, recall 0.959413, f_score 0.772454\n",
      "=== total 61800 match 36931\n",
      ">>> step 61900\n",
      "step 61800-61899, precision 0.646446, recall 0.959441, f_score 0.772441\n",
      "=== total 61900 match 36959\n",
      ">>> step 62000\n",
      "step 61900-61999, precision 0.646367, recall 0.959475, f_score 0.772396\n",
      "=== total 62000 match 36996\n",
      ">>> step 62100\n",
      "step 62000-62099, precision 0.646227, recall 0.959475, f_score 0.772296\n",
      "=== total 62100 match 37004\n",
      ">>> step 62200\n",
      "step 62100-62199, precision 0.646035, recall 0.959475, f_score 0.772159\n",
      "=== total 62200 match 37015\n",
      ">>> step 62300\n",
      "step 62200-62299, precision 0.645911, recall 0.959555, f_score 0.772096\n",
      "=== total 62300 match 37098\n",
      ">>> step 62400\n",
      "step 62300-62399, precision 0.645285, recall 0.959555, f_score 0.771648\n",
      "=== total 62400 match 37134\n",
      ">>> step 62500\n",
      "step 62400-62499, precision 0.645548, recall 0.959587, f_score 0.771847\n",
      "=== total 62500 match 37187\n",
      ">>> step 62600\n",
      "step 62500-62599, precision 0.645491, recall 0.959321, f_score 0.771721\n",
      "=== total 62600 match 37229\n",
      ">>> step 62700\n",
      "step 62600-62699, precision 0.645285, recall 0.959242, f_score 0.771548\n",
      "=== total 62700 match 37275\n",
      ">>> step 62800\n",
      "step 62700-62799, precision 0.645243, recall 0.958594, f_score 0.771308\n",
      "=== total 62800 match 37279\n",
      ">>> step 62900\n",
      "step 62800-62899, precision 0.644923, recall 0.958061, f_score 0.770907\n",
      "=== total 62900 match 37299\n",
      ">>> step 63000\n",
      "step 62900-62999, precision 0.644752, recall 0.957074, f_score 0.770465\n",
      "=== total 63000 match 37312\n",
      ">>> step 63100\n",
      "step 63000-63099, precision 0.644756, recall 0.955519, f_score 0.769964\n",
      "=== total 63100 match 37349\n",
      ">>> step 63200\n",
      "step 63100-63199, precision 0.644446, recall 0.955517, f_score 0.769742\n",
      "=== total 63200 match 37398\n",
      ">>> step 63300\n",
      "step 63200-63299, precision 0.644246, recall 0.955487, f_score 0.769589\n",
      "=== total 63300 match 37450\n",
      ">>> step 63400\n",
      "step 63300-63399, precision 0.644099, recall 0.955520, f_score 0.769496\n",
      "=== total 63400 match 37488\n",
      ">>> step 63500\n",
      "step 63400-63499, precision 0.644223, recall 0.955468, f_score 0.769566\n",
      "=== total 63500 match 37501\n",
      ">>> step 63600\n",
      "step 63500-63599, precision 0.644000, recall 0.955141, f_score 0.769302\n",
      "=== total 63600 match 37559\n",
      ">>> step 63700\n",
      "step 63600-63699, precision 0.643710, recall 0.954558, f_score 0.768906\n",
      "=== total 63700 match 37593\n",
      ">>> step 63800\n",
      "step 63700-63799, precision 0.643744, recall 0.954198, f_score 0.768813\n",
      "=== total 63800 match 37605\n",
      ">>> step 63900\n",
      "step 63800-63899, precision 0.643744, recall 0.953763, f_score 0.768672\n",
      "=== total 63900 match 37619\n",
      ">>> step 64000\n",
      "step 63900-63999, precision 0.642891, recall 0.953767, f_score 0.768065\n",
      "=== total 64000 match 37672\n",
      ">>> step 64100\n",
      "step 64000-64099, precision 0.642850, recall 0.953801, f_score 0.768046\n",
      "=== total 64100 match 37704\n",
      ">>> step 64200\n",
      "step 64100-64199, precision 0.642927, recall 0.953836, f_score 0.768113\n",
      "=== total 64200 match 37729\n",
      ">>> step 64300\n",
      "step 64200-64299, precision 0.642553, recall 0.953836, f_score 0.767845\n",
      "=== total 64300 match 37751\n",
      ">>> step 64400\n",
      "step 64300-64399, precision 0.642149, recall 0.953801, f_score 0.767546\n",
      "=== total 64400 match 37809\n",
      ">>> step 64500\n",
      "step 64400-64499, precision 0.641787, recall 0.953823, f_score 0.767294\n",
      "=== total 64500 match 37849\n",
      ">>> step 64600\n",
      "step 64500-64599, precision 0.641760, recall 0.953866, f_score 0.767289\n",
      "=== total 64600 match 37888\n",
      ">>> step 64700\n",
      "step 64600-64699, precision 0.641275, recall 0.953938, f_score 0.766966\n",
      "=== total 64700 match 37979\n",
      ">>> step 64800\n",
      "step 64700-64799, precision 0.641538, recall 0.953703, f_score 0.767078\n",
      "=== total 64800 match 38018\n",
      ">>> step 64900\n",
      "step 64800-64899, precision 0.641738, recall 0.953761, f_score 0.767239\n",
      "=== total 64900 match 38056\n",
      ">>> step 65000\n",
      "step 64900-64999, precision 0.641938, recall 0.953819, f_score 0.767401\n",
      "=== total 65000 match 38094\n",
      ">>> step 65100\n",
      "step 65000-65099, precision 0.642294, recall 0.953858, f_score 0.767668\n",
      "=== total 65100 match 38171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> step 65200\n",
      "step 65100-65199, precision 0.641986, recall 0.953599, f_score 0.767364\n",
      "=== total 65200 match 38222\n",
      ">>> step 65300\n",
      "step 65200-65299, precision 0.641547, recall 0.953513, f_score 0.767022\n",
      "=== total 65300 match 38270\n",
      ">>> step 65400\n",
      "step 65300-65399, precision 0.641738, recall 0.953461, f_score 0.767143\n",
      "=== total 65400 match 38310\n",
      ">>> step 65500\n",
      "step 65400-65499, precision 0.642179, recall 0.953550, f_score 0.767486\n",
      "=== total 65500 match 38360\n",
      ">>> step 65600\n",
      "step 65500-65599, precision 0.641669, recall 0.953580, f_score 0.767131\n",
      "=== total 65600 match 38417\n",
      ">>> step 65700\n",
      "step 65600-65699, precision 0.641427, recall 0.953545, f_score 0.766947\n",
      "=== total 65700 match 38497\n",
      ">>> step 65800\n",
      "step 65700-65799, precision 0.640886, recall 0.953620, f_score 0.766585\n",
      "=== total 65800 match 38595\n",
      ">>> step 65900\n",
      "step 65800-65899, precision 0.641162, recall 0.953662, f_score 0.766796\n",
      "=== total 65900 match 38647\n",
      ">>> step 66000\n",
      "step 65900-65999, precision 0.641567, recall 0.953749, f_score 0.767113\n",
      "=== total 66000 match 38699\n",
      ">>> step 66100\n",
      "step 66000-66099, precision 0.641937, recall 0.953491, f_score 0.767294\n",
      "=== total 66100 match 38739\n",
      ">>> step 66200\n",
      "step 66100-66199, precision 0.641647, recall 0.952354, f_score 0.766719\n",
      "=== total 66200 match 38783\n",
      ">>> step 66300\n",
      "step 66200-66299, precision 0.641723, recall 0.950953, f_score 0.766318\n",
      "=== total 66300 match 38794\n",
      ">>> step 66400\n",
      "step 66300-66399, precision 0.641821, recall 0.949569, f_score 0.765938\n",
      "=== total 66400 match 38813\n",
      ">>> step 66500\n",
      "step 66400-66499, precision 0.642052, recall 0.948967, f_score 0.765906\n",
      "=== total 66500 match 38838\n",
      ">>> step 66600\n",
      "step 66500-66599, precision 0.642211, recall 0.948246, f_score 0.765785\n",
      "=== total 66600 match 38886\n",
      ">>> step 66700\n",
      "step 66600-66699, precision 0.641823, recall 0.948225, f_score 0.765502\n",
      "=== total 66700 match 38950\n",
      ">>> step 66800\n",
      "step 66700-66799, precision 0.641889, recall 0.948201, f_score 0.765541\n",
      "=== total 66800 match 39013\n",
      ">>> step 66900\n",
      "step 66800-66899, precision 0.642062, recall 0.948234, f_score 0.765675\n",
      "=== total 66900 match 39057\n",
      ">>> step 67000\n",
      "step 66900-66999, precision 0.642253, recall 0.948324, f_score 0.765840\n",
      "=== total 67000 match 39117\n",
      ">>> step 67100\n",
      "step 67000-67099, precision 0.642183, recall 0.948400, f_score 0.765815\n",
      "=== total 67100 match 39182\n",
      ">>> step 67200\n",
      "step 67100-67199, precision 0.642235, recall 0.948260, f_score 0.765806\n",
      "=== total 67200 match 39238\n",
      ">>> step 67300\n",
      "step 67200-67299, precision 0.642826, recall 0.948386, f_score 0.766268\n",
      "=== total 67300 match 39303\n",
      ">>> step 67400\n",
      "step 67300-67399, precision 0.642467, recall 0.948437, f_score 0.766030\n",
      "=== total 67400 match 39395\n",
      ">>> step 67500\n",
      "step 67400-67499, precision 0.642743, recall 0.948467, f_score 0.766235\n",
      "=== total 67500 match 39459\n",
      ">>> step 67600\n",
      "step 67500-67599, precision 0.643238, recall 0.948551, f_score 0.766614\n",
      "=== total 67600 match 39525\n",
      ">>> step 67700\n",
      "step 67600-67699, precision 0.643133, recall 0.948639, f_score 0.766568\n",
      "=== total 67700 match 39603\n",
      ">>> step 67800\n",
      "step 67700-67799, precision 0.643129, recall 0.948717, f_score 0.766591\n",
      "=== total 67800 match 39667\n",
      ">>> step 67900\n",
      "step 67800-67899, precision 0.643653, recall 0.948852, f_score 0.767007\n",
      "=== total 67900 match 39745\n",
      ">>> step 68000\n",
      "step 67900-67999, precision 0.644134, recall 0.948984, f_score 0.767392\n",
      "=== total 68000 match 39824\n",
      ">>> step 68100\n",
      "step 68000-68099, precision 0.644551, recall 0.948928, f_score 0.767669\n",
      "=== total 68100 match 39896\n",
      ">>> step 68200\n",
      "step 68100-68199, precision 0.645184, recall 0.948995, f_score 0.768140\n",
      "=== total 68200 match 39970\n",
      ">>> step 68300\n",
      "step 68200-68299, precision 0.645148, recall 0.949005, f_score 0.768117\n",
      "=== total 68300 match 39980\n",
      "TOTAL 68300, precision 0.645148, recall 0.949005, f_score 0.768117\n",
      "TOTAL true = 27179\n",
      "TOTAL error rate = 0.228009\n"
     ]
    }
   ],
   "source": [
    "# TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "# cubes_set = pre_process_training(\"PX303-Fg006-V-C02-R02-D08032015-T115622-ML638__006.jpg\", 0, 6200, 0, 4400)\n",
    "train_imgs, train_lbls, train_x_delta, train_y_delta = \\\n",
    "    NEW_build_train_set_for_binary_labeling(cubes_set, CUBE_SIZE, ROOT_FOLDER + \"train_concats3/\", 1, 5)\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-898022341402>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "41524\n",
      "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model2.ckpt\n",
      "Model restored.\n",
      ">>> step 100\n",
      "step 0-99, precision 0.569231, recall 0.973684, f_score 0.718447\n",
      "=== total 100 match 65\n",
      ">>> step 200\n",
      "step 100-199, precision 0.610294, recall 0.976471, f_score 0.751131\n",
      "=== total 200 match 136\n",
      ">>> step 300\n",
      "step 200-299, precision 0.593909, recall 0.966942, f_score 0.735849\n",
      "=== total 300 match 197\n",
      ">>> step 400\n",
      "step 300-399, precision 0.612167, recall 0.969880, f_score 0.750583\n",
      "=== total 400 match 263\n",
      ">>> step 500\n",
      "step 400-499, precision 0.643963, recall 0.962963, f_score 0.771800\n",
      "=== total 500 match 323\n",
      ">>> step 600\n",
      "step 500-599, precision 0.659740, recall 0.962121, f_score 0.782743\n",
      "=== total 600 match 385\n",
      ">>> step 700\n",
      "step 600-699, precision 0.641758, recall 0.954248, f_score 0.767411\n",
      "=== total 700 match 455\n",
      ">>> step 800\n",
      "step 700-799, precision 0.639535, recall 0.953757, f_score 0.765661\n",
      "=== total 800 match 516\n",
      ">>> step 900\n",
      "step 800-899, precision 0.648973, recall 0.949875, f_score 0.771109\n",
      "=== total 900 match 584\n",
      ">>> step 1000\n",
      "step 900-999, precision 0.660407, recall 0.954751, f_score 0.780759\n",
      "=== total 1000 match 639\n",
      ">>> step 1100\n",
      "step 1000-1099, precision 0.664789, recall 0.957404, f_score 0.784705\n",
      "=== total 1100 match 710\n",
      ">>> step 1200\n",
      "step 1100-1199, precision 0.661017, recall 0.954802, f_score 0.781202\n",
      "=== total 1200 match 767\n",
      ">>> step 1300\n",
      "step 1200-1299, precision 0.665054, recall 0.956522, f_score 0.784593\n",
      "=== total 1300 match 827\n",
      ">>> step 1400\n",
      "step 1300-1399, precision 0.663677, recall 0.957929, f_score 0.784106\n",
      "=== total 1400 match 892\n",
      ">>> step 1500\n",
      "step 1400-1499, precision 0.661811, recall 0.956391, f_score 0.782288\n",
      "=== total 1500 match 961\n",
      ">>> step 1600\n",
      "step 1500-1599, precision 0.662439, recall 0.957687, f_score 0.783160\n",
      "=== total 1600 match 1025\n",
      ">>> step 1700\n",
      "step 1600-1699, precision 0.659341, recall 0.957447, f_score 0.780911\n",
      "=== total 1700 match 1092\n",
      ">>> step 1800\n",
      "step 1700-1799, precision 0.658834, recall 0.958228, f_score 0.780815\n",
      "=== total 1800 match 1149\n",
      ">>> step 1900\n",
      "step 1800-1899, precision 0.659768, recall 0.959085, f_score 0.781756\n",
      "=== total 1900 match 1208\n",
      ">>> step 2000\n",
      "step 1900-1999, precision 0.664848, recall 0.960586, f_score 0.785813\n",
      "=== total 2000 match 1283\n",
      ">>> step 2100\n",
      "step 2000-2099, precision 0.664425, recall 0.959008, f_score 0.784989\n",
      "=== total 2100 match 1338\n",
      ">>> step 2200\n",
      "step 2100-2199, precision 0.662633, recall 0.959794, f_score 0.784000\n",
      "=== total 2200 match 1405\n",
      ">>> step 2300\n",
      "step 2200-2299, precision 0.660494, recall 0.960120, f_score 0.782609\n",
      "=== total 2300 match 1458\n",
      ">>> step 2400\n",
      "step 2300-2399, precision 0.660750, recall 0.958055, f_score 0.782101\n",
      "=== total 2400 match 1521\n",
      ">>> step 2500\n",
      "step 2400-2499, precision 0.662696, recall 0.958296, f_score 0.783543\n",
      "=== total 2500 match 1595\n",
      ">>> step 2600\n",
      "step 2500-2599, precision 0.664858, recall 0.958297, f_score 0.785053\n",
      "=== total 2600 match 1659\n",
      ">>> step 2700\n",
      "step 2600-2699, precision 0.668605, recall 0.958333, f_score 0.787671\n",
      "=== total 2700 match 1720\n",
      ">>> step 2800\n",
      "step 2700-2799, precision 0.669101, recall 0.957395, f_score 0.787698\n",
      "=== total 2800 match 1780\n",
      ">>> step 2900\n",
      "step 2800-2899, precision 0.670288, recall 0.958075, f_score 0.788750\n",
      "=== total 2900 match 1841\n",
      ">>> step 3000\n",
      "step 2900-2999, precision 0.669811, recall 0.957303, f_score 0.788159\n",
      "=== total 3000 match 1908\n",
      ">>> step 3100\n",
      "step 3000-3099, precision 0.668871, recall 0.955669, f_score 0.786954\n",
      "=== total 3100 match 1966\n",
      ">>> step 3200\n",
      "step 3100-3199, precision 0.668312, recall 0.953521, f_score 0.785839\n",
      "=== total 3200 match 2026\n",
      ">>> step 3300\n",
      "step 3200-3299, precision 0.670172, recall 0.953093, f_score 0.786977\n",
      "=== total 3300 match 2092\n",
      ">>> step 3400\n",
      "step 3300-3399, precision 0.667441, recall 0.954183, f_score 0.785461\n",
      "=== total 3400 match 2153\n",
      ">>> step 3500\n",
      "step 3400-3499, precision 0.667722, recall 0.952289, f_score 0.785012\n",
      "=== total 3500 match 2212\n",
      ">>> step 3600\n",
      "step 3500-3599, precision 0.668426, recall 0.952978, f_score 0.785733\n",
      "=== total 3600 match 2274\n",
      ">>> step 3700\n",
      "step 3600-3699, precision 0.669231, recall 0.953135, f_score 0.786342\n",
      "=== total 3700 match 2340\n",
      ">>> step 3800\n",
      "step 3700-3799, precision 0.669858, recall 0.953199, f_score 0.786797\n",
      "=== total 3800 match 2402\n",
      ">>> step 3900\n",
      "step 3800-3899, precision 0.670321, recall 0.952683, f_score 0.786940\n",
      "=== total 3900 match 2463\n",
      ">>> step 4000\n",
      "step 3900-3999, precision 0.669968, recall 0.953213, f_score 0.786878\n",
      "=== total 4000 match 2524\n",
      ">>> step 4100\n",
      "step 4000-4099, precision 0.668854, recall 0.953770, f_score 0.786298\n",
      "=== total 4100 match 2591\n",
      ">>> step 4200\n",
      "step 4100-4199, precision 0.667296, recall 0.952047, f_score 0.784636\n",
      "=== total 4200 match 2648\n",
      ">>> step 4300\n",
      "step 4200-4299, precision 0.666790, recall 0.953109, f_score 0.784645\n",
      "=== total 4300 match 2713\n",
      ">>> step 4400\n",
      "step 4300-4399, precision 0.667390, recall 0.953512, f_score 0.785198\n",
      "=== total 4400 match 2766\n",
      ">>> step 4500\n",
      "step 4400-4499, precision 0.668662, recall 0.953792, f_score 0.786173\n",
      "=== total 4500 match 2840\n",
      ">>> step 4600\n",
      "step 4500-4599, precision 0.668956, recall 0.954902, f_score 0.786753\n",
      "=== total 4600 match 2912\n",
      ">>> step 4700\n",
      "step 4600-4699, precision 0.667900, recall 0.953868, f_score 0.785672\n",
      "=== total 4700 match 2972\n",
      ">>> step 4800\n",
      "step 4700-4799, precision 0.667651, recall 0.954951, f_score 0.785866\n",
      "=== total 4800 match 3048\n",
      ">>> step 4900\n",
      "step 4800-4899, precision 0.668167, recall 0.955842, f_score 0.786525\n",
      "=== total 4900 match 3110\n",
      ">>> step 5000\n",
      "step 4900-4999, precision 0.668768, recall 0.956718, f_score 0.787238\n",
      "=== total 5000 match 3173\n",
      ">>> step 5100\n",
      "step 5000-5099, precision 0.669648, recall 0.956388, f_score 0.787736\n",
      "=== total 5100 match 3242\n",
      ">>> step 5200\n",
      "step 5100-5199, precision 0.669287, recall 0.955959, f_score 0.787340\n",
      "=== total 5200 match 3308\n",
      ">>> step 5300\n",
      "step 5200-5299, precision 0.666864, recall 0.956706, f_score 0.785914\n",
      "=== total 5300 match 3380\n",
      ">>> step 5400\n",
      "step 5300-5399, precision 0.668406, recall 0.956846, f_score 0.787031\n",
      "=== total 5400 match 3450\n",
      ">>> step 5500\n",
      "step 5400-5499, precision 0.668466, recall 0.956504, f_score 0.786957\n",
      "=== total 5500 match 3520\n",
      ">>> step 5600\n",
      "step 5500-5599, precision 0.668618, recall 0.956539, f_score 0.787073\n",
      "=== total 5600 match 3588\n",
      ">>> step 5700\n",
      "step 5600-5699, precision 0.667670, recall 0.956096, f_score 0.786267\n",
      "=== total 5700 match 3653\n",
      ">>> step 5800\n",
      "step 5700-5799, precision 0.667026, recall 0.955650, f_score 0.785669\n",
      "=== total 5800 match 3715\n",
      ">>> step 5900\n",
      "step 5800-5899, precision 0.666314, recall 0.956027, f_score 0.785303\n",
      "=== total 5900 match 3785\n",
      ">>> step 6000\n",
      "step 5900-5999, precision 0.666320, recall 0.956392, f_score 0.785430\n",
      "=== total 6000 match 3851\n",
      ">>> step 6100\n",
      "step 6000-6099, precision 0.666581, recall 0.954945, f_score 0.785123\n",
      "=== total 6100 match 3911\n",
      ">>> step 6200\n",
      "step 6100-6199, precision 0.664824, recall 0.954890, f_score 0.783884\n",
      "=== total 6200 match 3980\n",
      ">>> step 6300\n",
      "step 6200-6299, precision 0.665843, recall 0.955658, f_score 0.784851\n",
      "=== total 6300 match 4046\n",
      ">>> step 6400\n",
      "step 6300-6399, precision 0.667315, recall 0.955795, f_score 0.785919\n",
      "=== total 6400 match 4115\n",
      ">>> step 6500\n",
      "step 6400-6499, precision 0.666347, recall 0.956014, f_score 0.785321\n",
      "=== total 6500 match 4175\n",
      ">>> step 6600\n",
      "step 6500-6599, precision 0.667218, recall 0.956654, f_score 0.786142\n",
      "=== total 6600 match 4234\n",
      ">>> step 6700\n",
      "step 6600-6699, precision 0.665736, recall 0.956522, f_score 0.785067\n",
      "=== total 6700 match 4296\n",
      ">>> step 6800\n",
      "step 6700-6799, precision 0.664451, recall 0.956135, f_score 0.784043\n",
      "=== total 6800 match 4363\n",
      ">>> step 6900\n",
      "step 6800-6899, precision 0.663277, recall 0.956338, f_score 0.783293\n",
      "=== total 6900 match 4425\n",
      ">>> step 7000\n",
      "step 6900-6999, precision 0.664813, recall 0.956786, f_score 0.784514\n",
      "=== total 7000 match 4496\n",
      ">>> step 7100\n",
      "step 7000-7099, precision 0.663738, recall 0.956329, f_score 0.783612\n",
      "=== total 7100 match 4553\n",
      ">>> step 7200\n",
      "step 7100-7199, precision 0.664211, recall 0.956034, f_score 0.783843\n",
      "=== total 7200 match 4616\n",
      ">>> step 7300\n",
      "step 7200-7299, precision 0.664457, recall 0.955706, f_score 0.783903\n",
      "=== total 7300 match 4676\n",
      ">>> step 7400\n",
      "step 7300-7399, precision 0.663087, recall 0.956271, f_score 0.783139\n",
      "=== total 7400 match 4749\n",
      ">>> step 7500\n",
      "step 7400-7499, precision 0.663066, recall 0.956835, f_score 0.783313\n",
      "=== total 7500 match 4814\n",
      ">>> step 7600\n",
      "step 7500-7599, precision 0.662700, recall 0.956754, f_score 0.783030\n",
      "=== total 7600 match 4874\n",
      ">>> step 7700\n",
      "step 7600-7699, precision 0.664438, recall 0.956814, f_score 0.784262\n",
      "=== total 7700 match 4935\n",
      ">>> step 7800\n",
      "step 7700-7799, precision 0.663931, recall 0.957287, f_score 0.784068\n",
      "=== total 7800 match 4996\n",
      ">>> step 7900\n",
      "step 7800-7899, precision 0.662720, recall 0.957241, f_score 0.783207\n",
      "=== total 7900 match 5067\n",
      ">>> step 8000\n",
      "step 7900-7999, precision 0.662178, recall 0.956853, f_score 0.782699\n",
      "=== total 8000 match 5124\n",
      ">>> step 8100\n",
      "step 8000-8099, precision 0.663136, recall 0.956655, f_score 0.783301\n",
      "=== total 8100 match 5192\n",
      ">>> step 8200\n",
      "step 8100-8199, precision 0.664382, recall 0.956951, f_score 0.784270\n",
      "=== total 8200 match 5253\n",
      ">>> step 8300\n",
      "step 8200-8299, precision 0.664159, recall 0.957441, f_score 0.784279\n",
      "=== total 8300 match 5318\n",
      ">>> step 8400\n",
      "step 8300-8399, precision 0.664377, recall 0.957976, f_score 0.784610\n",
      "=== total 8400 match 5387\n",
      ">>> step 8500\n",
      "step 8400-8499, precision 0.663733, recall 0.956913, f_score 0.783804\n",
      "=== total 8500 match 5454\n",
      ">>> step 8600\n",
      "step 8500-8599, precision 0.663893, recall 0.957225, f_score 0.784021\n",
      "=== total 8600 match 5528\n",
      ">>> step 8700\n",
      "step 8600-8699, precision 0.663571, recall 0.957239, f_score 0.783801\n",
      "=== total 8700 match 5600\n",
      ">>> step 8800\n",
      "step 8700-8799, precision 0.663959, recall 0.957474, f_score 0.784150\n",
      "=== total 8800 match 5663\n",
      ">>> step 8900\n",
      "step 8800-8899, precision 0.664803, recall 0.957003, f_score 0.784580\n",
      "=== total 8900 match 5725\n",
      ">>> step 9000\n",
      "step 8900-8999, precision 0.665631, recall 0.957061, f_score 0.785176\n",
      "=== total 9000 match 5793\n",
      ">>> step 9100\n",
      "step 9000-9099, precision 0.663993, recall 0.956960, f_score 0.784002\n",
      "=== total 9100 match 5860\n",
      ">>> step 9200\n",
      "step 9100-9199, precision 0.664358, recall 0.956934, f_score 0.784247\n",
      "=== total 9200 match 5920\n",
      ">>> step 9300\n",
      "step 9200-9299, precision 0.664775, recall 0.956981, f_score 0.784553\n",
      "=== total 9300 match 5990\n",
      ">>> step 9400\n",
      "step 9300-9399, precision 0.664189, recall 0.957153, f_score 0.784203\n",
      "=== total 9400 match 6054\n",
      ">>> step 9500\n",
      "step 9400-9499, precision 0.663945, recall 0.957156, f_score 0.784034\n",
      "=== total 9500 match 6124\n",
      ">>> step 9600\n",
      "step 9500-9599, precision 0.663921, recall 0.956937, f_score 0.783944\n",
      "=== total 9600 match 6192\n",
      ">>> step 9700\n",
      "step 9600-9699, precision 0.663841, recall 0.957113, f_score 0.783947\n",
      "=== total 9700 match 6253\n",
      ">>> step 9800\n",
      "step 9700-9799, precision 0.663713, recall 0.956879, f_score 0.783779\n",
      "=== total 9800 match 6319\n",
      ">>> step 9900\n",
      "step 9800-9899, precision 0.662488, recall 0.956778, f_score 0.782890\n",
      "=== total 9900 match 6382\n",
      ">>> step 10000\n",
      "step 9900-9999, precision 0.662838, recall 0.956756, f_score 0.783127\n",
      "=== total 10000 match 6442\n",
      ">>> step 10100\n",
      "step 10000-10099, precision 0.663134, recall 0.956782, f_score 0.783342\n",
      "=== total 10100 match 6510\n",
      ">>> step 10200\n",
      "step 10100-10199, precision 0.663214, recall 0.956713, f_score 0.783375\n",
      "=== total 10200 match 6565\n",
      ">>> step 10300\n",
      "step 10200-10299, precision 0.661631, recall 0.956332, f_score 0.782143\n",
      "=== total 10300 match 6620\n",
      ">>> step 10400\n",
      "step 10300-10399, precision 0.661730, recall 0.956531, f_score 0.782278\n",
      "=== total 10400 match 6684\n",
      ">>> step 10500\n",
      "step 10400-10499, precision 0.661527, recall 0.956484, f_score 0.782121\n",
      "=== total 10500 match 6745\n",
      ">>> step 10600\n",
      "step 10500-10599, precision 0.661622, recall 0.956457, f_score 0.782178\n",
      "=== total 10600 match 6806\n",
      ">>> step 10700\n",
      "step 10600-10699, precision 0.661326, recall 0.956394, f_score 0.781950\n",
      "=== total 10700 match 6865\n",
      ">>> step 10800\n",
      "step 10700-10799, precision 0.661516, recall 0.956367, f_score 0.782074\n",
      "=== total 10800 match 6925\n",
      ">>> step 10900\n",
      "step 10800-10899, precision 0.661184, recall 0.956549, f_score 0.781903\n",
      "=== total 10900 match 6992\n",
      ">>> step 11000\n",
      "step 10900-10999, precision 0.659864, recall 0.956254, f_score 0.780881\n",
      "=== total 11000 match 7056\n",
      ">>> step 11100\n",
      "step 11000-11099, precision 0.660303, recall 0.956486, f_score 0.781266\n",
      "=== total 11100 match 7124\n",
      ">>> step 11200\n",
      "step 11100-11199, precision 0.659805, recall 0.956259, f_score 0.780841\n",
      "=== total 11200 match 7190\n",
      ">>> step 11300\n",
      "step 11200-11299, precision 0.660091, recall 0.956044, f_score 0.780969\n",
      "=== total 11300 match 7249\n",
      ">>> step 11400\n",
      "step 11300-11399, precision 0.659464, recall 0.955987, f_score 0.780511\n",
      "=== total 11400 match 7312\n",
      ">>> step 11500\n",
      "step 11400-11499, precision 0.659704, recall 0.955975, f_score 0.780676\n",
      "=== total 11500 match 7373\n",
      ">>> step 11600\n",
      "step 11500-11599, precision 0.659635, recall 0.956361, f_score 0.780755\n",
      "=== total 11600 match 7442\n",
      ">>> step 11700\n",
      "step 11600-11699, precision 0.659291, recall 0.956118, f_score 0.780434\n",
      "=== total 11700 match 7502\n",
      ">>> step 11800\n",
      "step 11700-11799, precision 0.658911, recall 0.956255, f_score 0.780213\n",
      "=== total 11800 match 7564\n",
      ">>> step 11900\n",
      "step 11800-11899, precision 0.658808, recall 0.956274, f_score 0.780147\n",
      "=== total 11900 match 7635\n",
      ">>> step 12000\n",
      "step 11900-11999, precision 0.658305, recall 0.956021, f_score 0.779711\n",
      "=== total 12000 match 7694\n",
      ">>> step 12100\n",
      "step 12000-12099, precision 0.658376, recall 0.955668, f_score 0.779643\n",
      "=== total 12100 match 7760\n",
      ">>> step 12200\n",
      "step 12100-12199, precision 0.658353, recall 0.955626, f_score 0.779612\n",
      "=== total 12200 match 7818\n",
      ">>> step 12300\n",
      "step 12200-12299, precision 0.657955, recall 0.955416, f_score 0.779264\n",
      "=== total 12300 match 7882\n",
      ">>> step 12400\n",
      "step 12300-12399, precision 0.657355, recall 0.955377, f_score 0.778830\n",
      "=== total 12400 match 7947\n",
      ">>> step 12500\n",
      "step 12400-12499, precision 0.656761, recall 0.955669, f_score 0.778510\n",
      "=== total 12500 match 8009\n",
      ">>> step 12600\n",
      "step 12500-12599, precision 0.657118, recall 0.955636, f_score 0.778749\n",
      "=== total 12600 match 8064\n",
      ">>> step 12700\n",
      "step 12600-12699, precision 0.656858, recall 0.955579, f_score 0.778548\n",
      "=== total 12700 match 8122\n",
      ">>> step 12800\n",
      "step 12700-12799, precision 0.656769, recall 0.955386, f_score 0.778421\n",
      "=== total 12800 match 8184\n",
      ">>> step 12900\n",
      "step 12800-12899, precision 0.656364, recall 0.955532, f_score 0.778185\n",
      "=== total 12900 match 8250\n",
      ">>> step 13000\n",
      "step 12900-12999, precision 0.656573, recall 0.955579, f_score 0.778348\n",
      "=== total 13000 match 8322\n",
      ">>> step 13100\n",
      "step 13000-13099, precision 0.655933, recall 0.955359, f_score 0.777825\n",
      "=== total 13100 match 8385\n",
      ">>> step 13200\n",
      "step 13100-13199, precision 0.656176, recall 0.955548, f_score 0.778058\n",
      "=== total 13200 match 8452\n",
      ">>> step 13300\n",
      "step 13200-13299, precision 0.656529, recall 0.955563, f_score 0.778311\n",
      "=== total 13300 match 8516\n",
      ">>> step 13400\n",
      "step 13300-13399, precision 0.657426, recall 0.955477, f_score 0.778913\n",
      "=== total 13400 match 8585\n",
      ">>> step 13500\n",
      "step 13400-13499, precision 0.657956, recall 0.955339, f_score 0.779239\n",
      "=== total 13500 match 8648\n",
      ">>> step 13600\n",
      "step 13500-13599, precision 0.657218, recall 0.955296, f_score 0.778707\n",
      "=== total 13600 match 8714\n",
      ">>> step 13700\n",
      "step 13600-13699, precision 0.657286, recall 0.955132, f_score 0.778700\n",
      "=== total 13700 match 8777\n",
      ">>> step 13800\n",
      "step 13700-13799, precision 0.657844, recall 0.954695, f_score 0.778946\n",
      "=== total 13800 match 8841\n",
      ">>> step 13900\n",
      "step 13800-13899, precision 0.657980, recall 0.954849, f_score 0.779093\n",
      "=== total 13900 match 8903\n",
      ">>> step 14000\n",
      "step 13900-13999, precision 0.658827, recall 0.955236, f_score 0.779815\n",
      "=== total 14000 match 8972\n",
      ">>> step 14100\n",
      "step 14000-14099, precision 0.658628, recall 0.955238, f_score 0.779677\n",
      "=== total 14100 match 9040\n",
      ">>> step 14200\n",
      "step 14100-14199, precision 0.658681, recall 0.955371, f_score 0.779758\n",
      "=== total 14200 match 9100\n",
      ">>> step 14300\n",
      "step 14200-14299, precision 0.658585, recall 0.955021, f_score 0.779573\n",
      "=== total 14300 match 9156\n",
      ">>> step 14400\n",
      "step 14300-14399, precision 0.658388, recall 0.955010, f_score 0.779433\n",
      "=== total 14400 match 9221\n",
      ">>> step 14500\n",
      "step 14400-14499, precision 0.658340, recall 0.955014, f_score 0.779400\n",
      "=== total 14500 match 9287\n",
      ">>> step 14600\n",
      "step 14500-14599, precision 0.658069, recall 0.954517, f_score 0.779045\n",
      "=== total 14600 match 9344\n",
      ">>> step 14700\n",
      "step 14600-14699, precision 0.658303, recall 0.954819, f_score 0.779309\n",
      "=== total 14700 match 9406\n",
      ">>> step 14800\n",
      "step 14700-14799, precision 0.658186, recall 0.954824, f_score 0.779229\n",
      "=== total 14800 match 9473\n",
      ">>> step 14900\n",
      "step 14800-14899, precision 0.658350, recall 0.954843, f_score 0.779350\n",
      "=== total 14900 match 9539\n",
      ">>> step 15000\n",
      "step 14900-14999, precision 0.657684, recall 0.954401, f_score 0.778736\n",
      "=== total 15000 match 9611\n",
      ">>> step 15100\n",
      "step 15000-15099, precision 0.657810, recall 0.954402, f_score 0.778825\n",
      "=== total 15100 match 9673\n",
      ">>> step 15200\n",
      "step 15100-15199, precision 0.658308, recall 0.954410, f_score 0.779177\n",
      "=== total 15200 match 9731\n",
      ">>> step 15300\n",
      "step 15200-15299, precision 0.658569, recall 0.954438, f_score 0.779368\n",
      "=== total 15300 match 9797\n",
      ">>> step 15400\n",
      "step 15300-15399, precision 0.658962, recall 0.954479, f_score 0.779657\n",
      "=== total 15400 match 9864\n",
      ">>> step 15500\n",
      "step 15400-15499, precision 0.659109, recall 0.954751, f_score 0.779851\n",
      "=== total 15500 match 9924\n",
      ">>> step 15600\n",
      "step 15500-15599, precision 0.658786, recall 0.954842, f_score 0.779655\n",
      "=== total 15600 match 9982\n",
      ">>> step 15700\n",
      "step 15600-15699, precision 0.657942, recall 0.954795, f_score 0.779048\n",
      "=== total 15700 match 10048\n",
      ">>> step 15800\n",
      "step 15700-15799, precision 0.658387, recall 0.954519, f_score 0.779268\n",
      "=== total 15800 match 10105\n",
      ">>> step 15900\n",
      "step 15800-15899, precision 0.658438, recall 0.954383, f_score 0.779259\n",
      "=== total 15900 match 10168\n",
      ">>> step 16000\n",
      "step 15900-15999, precision 0.658234, recall 0.954533, f_score 0.779165\n",
      "=== total 16000 match 10238\n",
      ">>> step 16100\n",
      "step 16000-16099, precision 0.658352, recall 0.954814, f_score 0.779342\n",
      "=== total 16100 match 10303\n",
      ">>> step 16200\n",
      "step 16100-16199, precision 0.658271, recall 0.954921, f_score 0.779320\n",
      "=== total 16200 match 10362\n",
      ">>> step 16300\n",
      "step 16200-16299, precision 0.657933, recall 0.954735, f_score 0.779022\n",
      "=== total 16300 match 10419\n",
      ">>> step 16400\n",
      "step 16300-16399, precision 0.657814, recall 0.954665, f_score 0.778915\n",
      "=== total 16400 match 10468\n",
      ">>> step 16500\n",
      "step 16400-16499, precision 0.657330, recall 0.954238, f_score 0.778434\n",
      "=== total 16500 match 10532\n",
      ">>> step 16600\n",
      "step 16500-16599, precision 0.657763, recall 0.954527, f_score 0.778833\n",
      "=== total 16600 match 10595\n",
      ">>> step 16700\n",
      "step 16600-16699, precision 0.658287, recall 0.954440, f_score 0.779172\n",
      "=== total 16700 match 10661\n",
      ">>> step 16800\n",
      "step 16700-16799, precision 0.658427, recall 0.954306, f_score 0.779225\n",
      "=== total 16800 match 10721\n",
      ">>> step 16900\n",
      "step 16800-16899, precision 0.658100, recall 0.954386, f_score 0.779023\n",
      "=== total 16900 match 10778\n",
      ">>> step 17000\n",
      "step 16900-16999, precision 0.657688, recall 0.954612, f_score 0.778809\n",
      "=== total 17000 match 10841\n",
      ">>> step 17100\n",
      "step 17000-17099, precision 0.657101, recall 0.954709, f_score 0.778429\n",
      "=== total 17100 match 10907\n",
      ">>> step 17200\n",
      "step 17100-17199, precision 0.656763, recall 0.954690, f_score 0.778186\n",
      "=== total 17200 match 10972\n",
      ">>> step 17300\n",
      "step 17200-17299, precision 0.656848, recall 0.954557, f_score 0.778201\n",
      "=== total 17300 match 11033\n",
      ">>> step 17400\n",
      "step 17300-17399, precision 0.657377, recall 0.954432, f_score 0.778531\n",
      "=== total 17400 match 11088\n",
      ">>> step 17500\n",
      "step 17400-17499, precision 0.657673, recall 0.954717, f_score 0.778833\n",
      "=== total 17500 match 11156\n",
      ">>> step 17600\n",
      "step 17500-17599, precision 0.657369, recall 0.954581, f_score 0.778575\n",
      "=== total 17600 match 11222\n",
      ">>> step 17700\n",
      "step 17600-17699, precision 0.657636, recall 0.954633, f_score 0.778780\n",
      "=== total 17700 match 11295\n",
      ">>> step 17800\n",
      "step 17700-17799, precision 0.657392, recall 0.954365, f_score 0.778519\n",
      "=== total 17800 match 11357\n",
      ">>> step 17900\n",
      "step 17800-17899, precision 0.657245, recall 0.954505, f_score 0.778463\n",
      "=== total 17900 match 11428\n",
      ">>> step 18000\n",
      "step 17900-17999, precision 0.657150, recall 0.954488, f_score 0.778391\n",
      "=== total 18000 match 11489\n",
      ">>> step 18100\n",
      "step 18000-18099, precision 0.657460, recall 0.954494, f_score 0.778610\n",
      "=== total 18100 match 11549\n",
      ">>> step 18200\n",
      "step 18100-18199, precision 0.657820, recall 0.954489, f_score 0.778860\n",
      "=== total 18200 match 11605\n",
      ">>> step 18300\n",
      "step 18200-18299, precision 0.658415, recall 0.954551, f_score 0.779298\n",
      "=== total 18300 match 11675\n",
      ">>> step 18400\n",
      "step 18300-18399, precision 0.658456, recall 0.954529, f_score 0.779319\n",
      "=== total 18400 match 11732\n",
      ">>> step 18500\n",
      "step 18400-18499, precision 0.658301, recall 0.954277, f_score 0.779127\n",
      "=== total 18500 match 11794\n",
      ">>> step 18600\n",
      "step 18500-18599, precision 0.658372, recall 0.954507, f_score 0.779253\n",
      "=== total 18600 match 11855\n",
      ">>> step 18700\n",
      "step 18600-18699, precision 0.658473, recall 0.954634, f_score 0.779366\n",
      "=== total 18700 match 11920\n",
      ">>> step 18800\n",
      "step 18700-18799, precision 0.657879, recall 0.954722, f_score 0.778980\n",
      "=== total 18800 match 11987\n",
      ">>> step 18900\n",
      "step 18800-18899, precision 0.657869, recall 0.954825, f_score 0.779006\n",
      "=== total 18900 match 12048\n",
      ">>> step 19000\n",
      "step 18900-18999, precision 0.657641, recall 0.954927, f_score 0.778880\n",
      "=== total 19000 match 12113\n",
      ">>> step 19100\n",
      "step 19000-19099, precision 0.657547, recall 0.954784, f_score 0.778767\n",
      "=== total 19100 match 12171\n",
      ">>> step 19200\n",
      "step 19100-19199, precision 0.657813, recall 0.954351, f_score 0.778810\n",
      "=== total 19200 match 12236\n",
      ">>> step 19300\n",
      "step 19200-19299, precision 0.658318, recall 0.954379, f_score 0.779173\n",
      "=== total 19300 match 12298\n",
      ">>> step 19400\n",
      "step 19300-19399, precision 0.658414, recall 0.954492, f_score 0.779278\n",
      "=== total 19400 match 12360\n",
      ">>> step 19500\n",
      "step 19400-19499, precision 0.658376, recall 0.954487, f_score 0.779249\n",
      "=== total 19500 match 12423\n",
      ">>> step 19600\n",
      "step 19500-19599, precision 0.658181, recall 0.954604, f_score 0.779152\n",
      "=== total 19600 match 12492\n",
      ">>> step 19700\n",
      "step 19600-19699, precision 0.658278, recall 0.954614, f_score 0.779223\n",
      "=== total 19700 match 12557\n",
      ">>> step 19800\n",
      "step 19700-19799, precision 0.658452, recall 0.954509, f_score 0.779310\n",
      "=== total 19800 match 12619\n",
      ">>> step 19900\n",
      "step 19800-19899, precision 0.658627, recall 0.954753, f_score 0.779514\n",
      "=== total 19900 match 12687\n",
      ">>> step 20000\n",
      "step 19900-19999, precision 0.658850, recall 0.954768, f_score 0.779675\n",
      "=== total 20000 match 12751\n",
      ">>> step 20100\n",
      "step 20000-20099, precision 0.658733, recall 0.954756, f_score 0.779589\n",
      "=== total 20100 match 12814\n",
      ">>> step 20200\n",
      "step 20100-20199, precision 0.658876, recall 0.954658, f_score 0.779656\n",
      "=== total 20200 match 12878\n",
      ">>> step 20300\n",
      "step 20200-20299, precision 0.658919, recall 0.954688, f_score 0.779697\n",
      "=== total 20300 match 12950\n",
      ">>> step 20400\n",
      "step 20300-20399, precision 0.658673, recall 0.954662, f_score 0.779516\n",
      "=== total 20400 match 13011\n",
      ">>> step 20500\n",
      "step 20400-20499, precision 0.659042, recall 0.954465, f_score 0.779709\n",
      "=== total 20500 match 13072\n",
      ">>> step 20600\n",
      "step 20500-20599, precision 0.659153, recall 0.954460, f_score 0.779785\n",
      "=== total 20600 match 13132\n",
      ">>> step 20700\n",
      "step 20600-20699, precision 0.658688, recall 0.954451, f_score 0.779456\n",
      "=== total 20700 match 13202\n",
      ">>> step 20800\n",
      "step 20700-20799, precision 0.658625, recall 0.954337, f_score 0.779374\n",
      "=== total 20800 match 13264\n",
      ">>> step 20900\n",
      "step 20800-20899, precision 0.658615, recall 0.954555, f_score 0.779440\n",
      "=== total 20900 match 13331\n",
      ">>> step 21000\n",
      "step 20900-20999, precision 0.658608, recall 0.954781, f_score 0.779510\n",
      "=== total 21000 match 13401\n",
      ">>> step 21100\n",
      "step 21000-21099, precision 0.658647, recall 0.954687, f_score 0.779506\n",
      "=== total 21100 match 13467\n",
      ">>> step 21200\n",
      "step 21100-21199, precision 0.658582, recall 0.954658, f_score 0.779450\n",
      "=== total 21200 match 13523\n",
      ">>> step 21300\n",
      "step 21200-21299, precision 0.659014, recall 0.954899, f_score 0.779834\n",
      "=== total 21300 match 13590\n",
      ">>> step 21400\n",
      "step 21300-21399, precision 0.658976, recall 0.954893, f_score 0.779805\n",
      "=== total 21400 match 13653\n",
      ">>> step 21500\n",
      "step 21400-21499, precision 0.658647, recall 0.954666, f_score 0.779499\n",
      "=== total 21500 match 13716\n",
      ">>> step 21600\n",
      "step 21500-21599, precision 0.658830, recall 0.954584, f_score 0.779600\n",
      "=== total 21600 match 13782\n",
      ">>> step 21700\n",
      "step 21600-21699, precision 0.658526, recall 0.954346, f_score 0.779307\n",
      "=== total 21700 match 13840\n",
      ">>> step 21800\n",
      "step 21700-21799, precision 0.658563, recall 0.954346, f_score 0.779334\n",
      "=== total 21800 match 13903\n",
      ">>> step 21900\n",
      "step 21800-21899, precision 0.658262, recall 0.954309, f_score 0.779111\n",
      "=== total 21900 match 13961\n",
      ">>> step 22000\n",
      "step 21900-21999, precision 0.658161, recall 0.954517, f_score 0.779109\n",
      "=== total 22000 match 14030\n",
      ">>> step 22100\n",
      "step 22000-22099, precision 0.658691, recall 0.954424, f_score 0.779449\n",
      "=== total 22100 match 14084\n",
      ">>> step 22200\n",
      "step 22100-22199, precision 0.658940, recall 0.954155, f_score 0.779533\n",
      "=== total 22200 match 14150\n",
      ">>> step 22300\n",
      "step 22200-22299, precision 0.659256, recall 0.954272, f_score 0.779794\n",
      "=== total 22300 match 14213\n",
      ">>> step 22400\n",
      "step 22300-22399, precision 0.659406, recall 0.954287, f_score 0.779904\n",
      "=== total 22400 match 14278\n",
      ">>> step 22500\n",
      "step 22400-22499, precision 0.659389, recall 0.954270, f_score 0.779886\n",
      "=== total 22500 match 14336\n",
      ">>> step 22600\n",
      "step 22500-22599, precision 0.659140, recall 0.954331, f_score 0.779732\n",
      "=== total 22600 match 14393\n",
      ">>> step 22700\n",
      "step 22600-22699, precision 0.658869, recall 0.954386, f_score 0.779561\n",
      "=== total 22700 match 14449\n",
      ">>> step 22800\n",
      "step 22700-22799, precision 0.658716, recall 0.954550, f_score 0.779509\n",
      "=== total 22800 match 14507\n",
      ">>> step 22900\n",
      "step 22800-22899, precision 0.658572, recall 0.954654, f_score 0.779442\n",
      "=== total 22900 match 14577\n",
      ">>> step 23000\n",
      "step 22900-22999, precision 0.658337, recall 0.954748, f_score 0.779309\n",
      "=== total 23000 match 14646\n",
      ">>> step 23100\n",
      "step 23000-23099, precision 0.658258, recall 0.954922, f_score 0.779312\n",
      "=== total 23100 match 14707\n",
      ">>> step 23200\n",
      "step 23100-23199, precision 0.658588, recall 0.954943, f_score 0.779550\n",
      "=== total 23200 match 14771\n",
      ">>> step 23300\n",
      "step 23200-23299, precision 0.658398, recall 0.954559, f_score 0.779289\n",
      "=== total 23300 match 14836\n",
      ">>> step 23400\n",
      "step 23300-23399, precision 0.658522, recall 0.954559, f_score 0.779376\n",
      "=== total 23400 match 14897\n",
      ">>> step 23500\n",
      "step 23400-23499, precision 0.658065, recall 0.954338, f_score 0.778982\n",
      "=== total 23500 match 14959\n",
      ">>> step 23600\n",
      "step 23500-23599, precision 0.658214, recall 0.954445, f_score 0.779122\n",
      "=== total 23600 match 15024\n",
      ">>> step 23700\n",
      "step 23600-23699, precision 0.657876, recall 0.954248, f_score 0.778819\n",
      "=== total 23700 match 15091\n",
      ">>> step 23800\n",
      "step 23700-23799, precision 0.658310, recall 0.954285, f_score 0.779136\n",
      "=== total 23800 match 15157\n",
      ">>> step 23900\n",
      "step 23800-23899, precision 0.658298, recall 0.953905, f_score 0.779001\n",
      "=== total 23900 match 15215\n",
      ">>> step 24000\n",
      "step 23900-23999, precision 0.658241, recall 0.953972, f_score 0.778983\n",
      "=== total 24000 match 15271\n",
      ">>> step 24100\n",
      "step 24000-24099, precision 0.658432, recall 0.953824, f_score 0.779068\n",
      "=== total 24100 match 15341\n",
      ">>> step 24200\n",
      "step 24100-24199, precision 0.658443, recall 0.953913, f_score 0.779105\n",
      "=== total 24200 match 15403\n",
      ">>> step 24300\n",
      "step 24200-24299, precision 0.658801, recall 0.953946, f_score 0.779367\n",
      "=== total 24300 match 15469\n",
      ">>> step 24400\n",
      "step 24300-24399, precision 0.658725, recall 0.953846, f_score 0.779280\n",
      "=== total 24400 match 15530\n",
      ">>> step 24500\n",
      "step 24400-24499, precision 0.658864, recall 0.953853, f_score 0.779379\n",
      "=== total 24500 match 15592\n",
      ">>> step 24600\n",
      "step 24500-24599, precision 0.658642, recall 0.953680, f_score 0.779167\n",
      "=== total 24600 match 15661\n",
      ">>> step 24700\n",
      "step 24600-24699, precision 0.658695, recall 0.953503, f_score 0.779145\n",
      "=== total 24700 match 15722\n",
      ">>> step 24800\n",
      "step 24700-24799, precision 0.658702, recall 0.953486, f_score 0.779144\n",
      "=== total 24800 match 15778\n",
      ">>> step 24900\n",
      "step 24800-24899, precision 0.658443, recall 0.953490, f_score 0.778964\n",
      "=== total 24900 match 15848\n",
      ">>> step 25000\n",
      "step 24900-24999, precision 0.658497, recall 0.953586, f_score 0.779033\n",
      "=== total 25000 match 15912\n",
      ">>> step 25100\n",
      "step 25000-25099, precision 0.658613, recall 0.953598, f_score 0.779119\n",
      "=== total 25100 match 15976\n",
      ">>> step 25200\n",
      "step 25100-25199, precision 0.658436, recall 0.953499, f_score 0.778962\n",
      "=== total 25200 match 16038\n",
      ">>> step 25300\n",
      "step 25200-25299, precision 0.658924, recall 0.953451, f_score 0.779288\n",
      "=== total 25300 match 16102\n",
      ">>> step 25400\n",
      "step 25300-25399, precision 0.658977, recall 0.953638, f_score 0.779387\n",
      "=== total 25400 match 16169\n",
      ">>> step 25500\n",
      "step 25400-25499, precision 0.659067, recall 0.953638, f_score 0.779450\n",
      "=== total 25500 match 16229\n",
      ">>> step 25600\n",
      "step 25500-25599, precision 0.658975, recall 0.953557, f_score 0.779358\n",
      "=== total 25600 match 16295\n",
      ">>> step 25700\n",
      "step 25600-25699, precision 0.658945, recall 0.953312, f_score 0.779256\n",
      "=== total 25700 match 16361\n",
      ">>> step 25800\n",
      "step 25700-25799, precision 0.659156, recall 0.953320, f_score 0.779406\n",
      "=== total 25800 match 16421\n",
      ">>> step 25900\n",
      "step 25800-25899, precision 0.659425, recall 0.953492, f_score 0.779651\n",
      "=== total 25900 match 16478\n",
      ">>> step 26000\n",
      "step 25900-25999, precision 0.659510, recall 0.953484, f_score 0.779708\n",
      "=== total 26000 match 16535\n",
      ">>> step 26100\n",
      "step 26000-26099, precision 0.659437, recall 0.953567, f_score 0.779685\n",
      "=== total 26100 match 16599\n",
      ">>> step 26200\n",
      "step 26100-26199, precision 0.659325, recall 0.953650, f_score 0.779635\n",
      "=== total 26200 match 16664\n",
      ">>> step 26300\n",
      "step 26200-26299, precision 0.659153, recall 0.953802, f_score 0.779565\n",
      "=== total 26300 match 16726\n",
      ">>> step 26400\n",
      "step 26300-26399, precision 0.659080, recall 0.953867, f_score 0.779536\n",
      "=== total 26400 match 16784\n",
      ">>> step 26500\n",
      "step 26400-26499, precision 0.659265, recall 0.953952, f_score 0.779693\n",
      "=== total 26500 match 16843\n",
      ">>> step 26600\n",
      "step 26500-26599, precision 0.659335, recall 0.954059, f_score 0.779778\n",
      "=== total 26600 match 16914\n",
      ">>> step 26700\n",
      "step 26600-26699, precision 0.659559, recall 0.954158, f_score 0.779968\n",
      "=== total 26700 match 16978\n",
      ">>> step 26800\n",
      "step 26700-26799, precision 0.659348, recall 0.954128, f_score 0.779810\n",
      "=== total 26800 match 17035\n",
      ">>> step 26900\n",
      "step 26800-26899, precision 0.659490, recall 0.953880, f_score 0.779826\n",
      "=== total 26900 match 17092\n",
      ">>> step 27000\n",
      "step 26900-26999, precision 0.659321, recall 0.953947, f_score 0.779731\n",
      "=== total 27000 match 17154\n",
      ">>> step 27100\n",
      "step 27000-27099, precision 0.659115, recall 0.954087, f_score 0.779633\n",
      "=== total 27100 match 17214\n",
      ">>> step 27200\n",
      "step 27100-27199, precision 0.659222, recall 0.954191, f_score 0.779743\n",
      "=== total 27200 match 17284\n",
      ">>> step 27300\n",
      "step 27200-27299, precision 0.658961, recall 0.954094, f_score 0.779528\n",
      "=== total 27300 match 17347\n",
      ">>> step 27400\n",
      "step 27300-27399, precision 0.659220, recall 0.954039, f_score 0.779691\n",
      "=== total 27400 match 17413\n",
      ">>> step 27500\n",
      "step 27400-27499, precision 0.659244, recall 0.953868, f_score 0.779651\n",
      "=== total 27500 match 17470\n",
      ">>> step 27600\n",
      "step 27500-27599, precision 0.659139, recall 0.953866, f_score 0.779576\n",
      "=== total 27600 match 17535\n",
      ">>> step 27700\n",
      "step 27600-27699, precision 0.659208, recall 0.954056, f_score 0.779688\n",
      "=== total 27700 match 17609\n",
      ">>> step 27800\n",
      "step 27700-27799, precision 0.659235, recall 0.954214, f_score 0.779760\n",
      "=== total 27800 match 17672\n",
      ">>> step 27900\n",
      "step 27800-27899, precision 0.659165, recall 0.953807, f_score 0.779575\n",
      "=== total 27900 match 17730\n",
      ">>> step 28000\n",
      "step 27900-27999, precision 0.658667, recall 0.953617, f_score 0.779163\n",
      "=== total 28000 match 17792\n",
      ">>> step 28100\n",
      "step 28000-28099, precision 0.658509, recall 0.953536, f_score 0.779025\n",
      "=== total 28100 match 17857\n",
      ">>> step 28200\n",
      "step 28100-28199, precision 0.658686, recall 0.953624, f_score 0.779179\n",
      "=== total 28200 match 17919\n",
      ">>> step 28300\n",
      "step 28200-28299, precision 0.658640, recall 0.953696, f_score 0.779170\n",
      "=== total 28300 match 17981\n",
      ">>> step 28400\n",
      "step 28300-28399, precision 0.658724, recall 0.953710, f_score 0.779234\n",
      "=== total 28400 match 18047\n",
      ">>> step 28500\n",
      "step 28400-28499, precision 0.658531, recall 0.953622, f_score 0.779070\n",
      "=== total 28500 match 18110\n",
      ">>> step 28600\n",
      "step 28500-28599, precision 0.658870, recall 0.953789, f_score 0.779362\n",
      "=== total 28600 match 18169\n",
      ">>> step 28700\n",
      "step 28600-28699, precision 0.659002, recall 0.953928, f_score 0.779501\n",
      "=== total 28700 match 18223\n",
      ">>> step 28800\n",
      "step 28700-28799, precision 0.659101, recall 0.953930, f_score 0.779571\n",
      "=== total 28800 match 18284\n",
      ">>> step 28900\n",
      "step 28800-28899, precision 0.658873, recall 0.953993, f_score 0.779433\n",
      "=== total 28900 match 18348\n",
      ">>> step 29000\n",
      "step 28900-28999, precision 0.658775, recall 0.953999, f_score 0.779366\n",
      "=== total 29000 match 18416\n",
      ">>> step 29100\n",
      "step 29000-29099, precision 0.658799, recall 0.954136, f_score 0.779429\n",
      "=== total 29100 match 18473\n",
      ">>> step 29200\n",
      "step 29100-29199, precision 0.658825, recall 0.954130, f_score 0.779445\n",
      "=== total 29200 match 18533\n",
      ">>> step 29300\n",
      "step 29200-29299, precision 0.658977, recall 0.954291, f_score 0.779605\n",
      "=== total 29300 match 18597\n",
      ">>> step 29400\n",
      "step 29300-29399, precision 0.658699, recall 0.954341, f_score 0.779427\n",
      "=== total 29400 match 18658\n",
      ">>> step 29500\n",
      "step 29400-29499, precision 0.658796, recall 0.954415, f_score 0.779520\n",
      "=== total 29500 match 18719\n",
      ">>> step 29600\n",
      "step 29500-29599, precision 0.658576, recall 0.954482, f_score 0.779388\n",
      "=== total 29600 match 18786\n",
      ">>> step 29700\n",
      "step 29600-29699, precision 0.658104, recall 0.954507, f_score 0.779066\n",
      "=== total 29700 match 18842\n",
      ">>> step 29800\n",
      "step 29700-29799, precision 0.657937, recall 0.954556, f_score 0.778965\n",
      "=== total 29800 match 18900\n",
      ">>> step 29900\n",
      "step 29800-29899, precision 0.658108, recall 0.954650, f_score 0.779116\n",
      "=== total 29900 match 18968\n",
      ">>> step 30000\n",
      "step 29900-29999, precision 0.657820, recall 0.954549, f_score 0.778881\n",
      "=== total 30000 match 19028\n",
      ">>> step 30100\n",
      "step 30000-30099, precision 0.657849, recall 0.954694, f_score 0.778949\n",
      "=== total 30100 match 19091\n",
      ">>> step 30200\n",
      "step 30100-30199, precision 0.657654, recall 0.954690, f_score 0.778811\n",
      "=== total 30200 match 19159\n",
      ">>> step 30300\n",
      "step 30200-30299, precision 0.657736, recall 0.954693, f_score 0.778870\n",
      "=== total 30300 match 19222\n",
      ">>> step 30400\n",
      "step 30300-30399, precision 0.657816, recall 0.954474, f_score 0.778852\n",
      "=== total 30400 match 19282\n",
      ">>> step 30500\n",
      "step 30400-30499, precision 0.657514, recall 0.954436, f_score 0.778628\n",
      "=== total 30500 match 19338\n",
      ">>> step 30600\n",
      "step 30500-30599, precision 0.657561, recall 0.954369, f_score 0.778639\n",
      "=== total 30600 match 19402\n",
      ">>> step 30700\n",
      "step 30600-30699, precision 0.657523, recall 0.954512, f_score 0.778660\n",
      "=== total 30700 match 19467\n",
      ">>> step 30800\n",
      "step 30700-30799, precision 0.657503, recall 0.954657, f_score 0.778694\n",
      "=== total 30800 match 19533\n",
      ">>> step 30900\n",
      "step 30800-30899, precision 0.657379, recall 0.954788, f_score 0.778651\n",
      "=== total 30900 match 19596\n",
      ">>> step 31000\n",
      "step 30900-30999, precision 0.657462, recall 0.954804, f_score 0.778715\n",
      "=== total 31000 match 19665\n",
      ">>> step 31100\n",
      "step 31000-31099, precision 0.657121, recall 0.954783, f_score 0.778468\n",
      "=== total 31100 match 19730\n",
      ">>> step 31200\n",
      "step 31100-31199, precision 0.657165, recall 0.954696, f_score 0.778470\n",
      "=== total 31200 match 19785\n",
      ">>> step 31300\n",
      "step 31200-31299, precision 0.657248, recall 0.954712, f_score 0.778534\n",
      "=== total 31300 match 19854\n",
      ">>> step 31400\n",
      "step 31300-31399, precision 0.657042, recall 0.954622, f_score 0.778359\n",
      "=== total 31400 match 19915\n",
      ">>> step 31500\n",
      "step 31400-31499, precision 0.657306, recall 0.954635, f_score 0.778549\n",
      "=== total 31500 match 19977\n",
      ">>> step 31600\n",
      "step 31500-31599, precision 0.657206, recall 0.954503, f_score 0.778434\n",
      "=== total 31600 match 20047\n",
      ">>> step 31700\n",
      "step 31600-31699, precision 0.657271, recall 0.954585, f_score 0.778507\n",
      "=== total 31700 match 20115\n",
      ">>> step 31800\n",
      "step 31700-31799, precision 0.657299, recall 0.954647, f_score 0.778548\n",
      "=== total 31800 match 20175\n",
      ">>> step 31900\n",
      "step 31800-31899, precision 0.657117, recall 0.954718, f_score 0.778444\n",
      "=== total 31900 match 20246\n",
      ">>> step 32000\n",
      "step 31900-31999, precision 0.657244, recall 0.954649, f_score 0.778510\n",
      "=== total 32000 match 20306\n",
      ">>> step 32100\n",
      "step 32000-32099, precision 0.657275, recall 0.954724, f_score 0.778556\n",
      "=== total 32100 match 20372\n",
      ">>> step 32200\n",
      "step 32100-32199, precision 0.657274, recall 0.954736, f_score 0.778560\n",
      "=== total 32200 match 20442\n",
      ">>> step 32300\n",
      "step 32200-32299, precision 0.657349, recall 0.954793, f_score 0.778632\n",
      "=== total 32300 match 20499\n",
      ">>> step 32400\n",
      "step 32300-32399, precision 0.657411, recall 0.954799, f_score 0.778677\n",
      "=== total 32400 match 20564\n",
      ">>> step 32500\n",
      "step 32400-32499, precision 0.657265, recall 0.954801, f_score 0.778575\n",
      "=== total 32500 match 20634\n",
      ">>> step 32600\n",
      "step 32500-32599, precision 0.657038, recall 0.954660, f_score 0.778369\n",
      "=== total 32600 match 20702\n",
      ">>> step 32700\n",
      "step 32600-32699, precision 0.657086, recall 0.954609, f_score 0.778386\n",
      "=== total 32700 match 20772\n",
      ">>> step 32800\n",
      "step 32700-32799, precision 0.657387, recall 0.954691, f_score 0.778624\n",
      "=== total 32800 match 20834\n",
      ">>> step 32900\n",
      "step 32800-32899, precision 0.656889, recall 0.954656, f_score 0.778263\n",
      "=== total 32900 match 20897\n",
      ">>> step 33000\n",
      "step 32900-32999, precision 0.656870, recall 0.954520, f_score 0.778205\n",
      "=== total 33000 match 20960\n",
      ">>> step 33100\n",
      "step 33000-33099, precision 0.657044, recall 0.954536, f_score 0.778332\n",
      "=== total 33100 match 21026\n",
      ">>> step 33200\n",
      "step 33100-33199, precision 0.657037, recall 0.954451, f_score 0.778299\n",
      "=== total 33200 match 21081\n",
      ">>> step 33300\n",
      "step 33200-33299, precision 0.657271, recall 0.954599, f_score 0.778512\n",
      "=== total 33300 match 21145\n",
      ">>> step 33400\n",
      "step 33300-33399, precision 0.657191, recall 0.954601, f_score 0.778457\n",
      "=== total 33400 match 21213\n",
      ">>> step 33500\n",
      "step 33400-33499, precision 0.657566, recall 0.954567, f_score 0.778708\n",
      "=== total 33500 match 21280\n",
      ">>> step 33600\n",
      "step 33500-33599, precision 0.657794, recall 0.954570, f_score 0.778869\n",
      "=== total 33600 match 21338\n",
      ">>> step 33700\n",
      "step 33600-33699, precision 0.657847, recall 0.954293, f_score 0.778814\n",
      "=== total 33700 match 21391\n",
      ">>> step 33800\n",
      "step 33700-33799, precision 0.657994, recall 0.954158, f_score 0.778872\n",
      "=== total 33800 match 21447\n",
      ">>> step 33900\n",
      "step 33800-33899, precision 0.658021, recall 0.954166, f_score 0.778894\n",
      "=== total 33900 match 21513\n",
      ">>> step 34000\n",
      "step 33900-33999, precision 0.658265, recall 0.954197, f_score 0.779076\n",
      "=== total 34000 match 21584\n",
      ">>> step 34100\n",
      "step 34000-34099, precision 0.657998, recall 0.954183, f_score 0.778883\n",
      "=== total 34100 match 21649\n",
      ">>> step 34200\n",
      "step 34100-34199, precision 0.657934, recall 0.954190, f_score 0.778841\n",
      "=== total 34200 match 21718\n",
      ">>> step 34300\n",
      "step 34200-34299, precision 0.658052, recall 0.954204, f_score 0.778928\n",
      "=== total 34300 match 21784\n",
      ">>> step 34400\n",
      "step 34300-34399, precision 0.657862, recall 0.954060, f_score 0.778747\n",
      "=== total 34400 match 21845\n",
      ">>> step 34500\n",
      "step 34400-34499, precision 0.658205, recall 0.954007, f_score 0.778970\n",
      "=== total 34500 match 21902\n",
      ">>> step 34600\n",
      "step 34500-34599, precision 0.657640, recall 0.954107, f_score 0.778607\n",
      "=== total 34600 match 21971\n",
      ">>> step 34700\n",
      "step 34600-34699, precision 0.657788, recall 0.954252, f_score 0.778760\n",
      "=== total 34700 match 22039\n",
      ">>> step 34800\n",
      "step 34700-34799, precision 0.657766, recall 0.954050, f_score 0.778677\n",
      "=== total 34800 match 22096\n",
      ">>> step 34900\n",
      "step 34800-34899, precision 0.657776, recall 0.954049, f_score 0.778683\n",
      "=== total 34900 match 22158\n",
      ">>> step 35000\n",
      "step 34900-34999, precision 0.657862, recall 0.953869, f_score 0.778683\n",
      "=== total 35000 match 22222\n",
      ">>> step 35100\n",
      "step 35000-35099, precision 0.657857, recall 0.953871, f_score 0.778681\n",
      "=== total 35100 match 22286\n",
      ">>> step 35200\n",
      "step 35100-35199, precision 0.657857, recall 0.953897, f_score 0.778689\n",
      "=== total 35200 match 22362\n",
      ">>> step 35300\n",
      "step 35200-35299, precision 0.657777, recall 0.953828, f_score 0.778611\n",
      "=== total 35300 match 22424\n",
      ">>> step 35400\n",
      "step 35300-35399, precision 0.657519, recall 0.953742, f_score 0.778401\n",
      "=== total 35400 match 22483\n",
      ">>> step 35500\n",
      "step 35400-35499, precision 0.657354, recall 0.953803, f_score 0.778306\n",
      "=== total 35500 match 22551\n",
      ">>> step 35600\n",
      "step 35500-35599, precision 0.657499, recall 0.953939, f_score 0.778453\n",
      "=== total 35600 match 22616\n",
      ">>> step 35700\n",
      "step 35600-35699, precision 0.657448, recall 0.953981, f_score 0.778431\n",
      "=== total 35700 match 22671\n",
      ">>> step 35800\n",
      "step 35700-35799, precision 0.657461, recall 0.953991, f_score 0.778443\n",
      "=== total 35800 match 22739\n",
      ">>> step 35900\n",
      "step 35800-35899, precision 0.657587, recall 0.953869, f_score 0.778490\n",
      "=== total 35900 match 22797\n",
      ">>> step 36000\n",
      "step 35900-35999, precision 0.657830, recall 0.953825, f_score 0.778647\n",
      "=== total 36000 match 22860\n",
      ">>> step 36100\n",
      "step 36000-36099, precision 0.657998, recall 0.953706, f_score 0.778725\n",
      "=== total 36100 match 22918\n",
      ">>> step 36200\n",
      "step 36100-36199, precision 0.657981, recall 0.953717, f_score 0.778716\n",
      "=== total 36200 match 22987\n",
      ">>> step 36300\n",
      "step 36200-36299, precision 0.658050, recall 0.953674, f_score 0.778750\n",
      "=== total 36300 match 23056\n",
      ">>> step 36400\n",
      "step 36300-36399, precision 0.658133, recall 0.953808, f_score 0.778853\n",
      "=== total 36400 match 23123\n",
      ">>> step 36500\n",
      "step 36400-36499, precision 0.658173, recall 0.953884, f_score 0.778906\n",
      "=== total 36500 match 23193\n",
      ">>> step 36600\n",
      "step 36500-36599, precision 0.658339, recall 0.953956, f_score 0.779047\n",
      "=== total 36600 match 23257\n",
      ">>> step 36700\n",
      "step 36600-36699, precision 0.658389, recall 0.954012, f_score 0.779100\n",
      "=== total 36700 match 23316\n",
      ">>> step 36800\n",
      "step 36700-36799, precision 0.658509, recall 0.953774, f_score 0.779105\n",
      "=== total 36800 match 23374\n",
      ">>> step 36900\n",
      "step 36800-36899, precision 0.658660, recall 0.953849, f_score 0.779236\n",
      "=== total 36900 match 23440\n",
      ">>> step 37000\n",
      "step 36900-36999, precision 0.658609, recall 0.953895, f_score 0.779216\n",
      "=== total 37000 match 23498\n",
      ">>> step 37100\n",
      "step 37000-37099, precision 0.658758, recall 0.953906, f_score 0.779323\n",
      "=== total 37100 match 23561\n",
      ">>> step 37200\n",
      "step 37100-37199, precision 0.658539, recall 0.953946, f_score 0.779183\n",
      "=== total 37200 match 23622\n",
      ">>> step 37300\n",
      "step 37200-37299, precision 0.658561, recall 0.953895, f_score 0.779182\n",
      "=== total 37300 match 23688\n",
      ">>> step 37400\n",
      "step 37300-37399, precision 0.658639, recall 0.953844, f_score 0.779219\n",
      "=== total 37400 match 23752\n",
      ">>> step 37500\n",
      "step 37400-37499, precision 0.658705, recall 0.953744, f_score 0.779232\n",
      "=== total 37500 match 23821\n",
      ">>> step 37600\n",
      "step 37500-37599, precision 0.658505, recall 0.953746, f_score 0.779093\n",
      "=== total 37600 match 23892\n",
      ">>> step 37700\n",
      "step 37600-37699, precision 0.658639, recall 0.953823, f_score 0.779212\n",
      "=== total 37700 match 23960\n",
      ">>> step 37800\n",
      "step 37700-37799, precision 0.658687, recall 0.953877, f_score 0.779264\n",
      "=== total 37800 match 24019\n",
      ">>> step 37900\n",
      "step 37800-37899, precision 0.658345, recall 0.953954, f_score 0.779050\n",
      "=== total 37900 match 24074\n",
      ">>> step 38000\n",
      "step 37900-37999, precision 0.658368, recall 0.953904, f_score 0.779050\n",
      "=== total 38000 match 24140\n",
      ">>> step 38100\n",
      "step 38000-38099, precision 0.658224, recall 0.953952, f_score 0.778965\n",
      "=== total 38100 match 24203\n",
      ">>> step 38200\n",
      "step 38100-38199, precision 0.658066, recall 0.953877, f_score 0.778829\n",
      "=== total 38200 match 24262\n",
      ">>> step 38300\n",
      "step 38200-38299, precision 0.658213, recall 0.953779, f_score 0.778899\n",
      "=== total 38300 match 24328\n",
      ">>> step 38400\n",
      "step 38300-38399, precision 0.658427, recall 0.953854, f_score 0.779074\n",
      "=== total 38400 match 24393\n",
      ">>> step 38500\n",
      "step 38400-38499, precision 0.658666, recall 0.953816, f_score 0.779229\n",
      "=== total 38500 match 24457\n",
      ">>> step 38600\n",
      "step 38500-38599, precision 0.658228, recall 0.953863, f_score 0.778938\n",
      "=== total 38600 match 24531\n",
      ">>> step 38700\n",
      "step 38600-38699, precision 0.658414, recall 0.953881, f_score 0.779074\n",
      "=== total 38700 match 24597\n",
      ">>> step 38800\n",
      "step 38700-38799, precision 0.658341, recall 0.953824, f_score 0.779004\n",
      "=== total 38800 match 24662\n",
      ">>> step 38900\n",
      "step 38800-38899, precision 0.658498, recall 0.953843, f_score 0.779120\n",
      "=== total 38900 match 24729\n",
      ">>> step 39000\n",
      "step 38900-38999, precision 0.658201, recall 0.953794, f_score 0.778896\n",
      "=== total 39000 match 24807\n",
      ">>> step 39100\n",
      "step 39000-39099, precision 0.658007, recall 0.953886, f_score 0.778791\n",
      "=== total 39100 match 24866\n",
      ">>> step 39200\n",
      "step 39100-39199, precision 0.658122, recall 0.953835, f_score 0.778854\n",
      "=== total 39200 match 24927\n",
      ">>> step 39300\n",
      "step 39200-39299, precision 0.658075, recall 0.953768, f_score 0.778799\n",
      "=== total 39300 match 24985\n",
      ">>> step 39400\n",
      "step 39300-39399, precision 0.657987, recall 0.953748, f_score 0.778731\n",
      "=== total 39400 match 25040\n",
      ">>> step 39500\n",
      "step 39400-39499, precision 0.657731, recall 0.953627, f_score 0.778511\n",
      "=== total 39500 match 25106\n",
      ">>> step 39600\n",
      "step 39500-39599, precision 0.657714, recall 0.953626, f_score 0.778499\n",
      "=== total 39600 match 25169\n",
      ">>> step 39700\n",
      "step 39600-39699, precision 0.657633, recall 0.953634, f_score 0.778445\n",
      "=== total 39700 match 25239\n",
      ">>> step 39800\n",
      "step 39700-39799, precision 0.657668, recall 0.953743, f_score 0.778506\n",
      "=== total 39800 match 25300\n",
      ">>> step 39900\n",
      "step 39800-39899, precision 0.657429, recall 0.953844, f_score 0.778372\n",
      "=== total 39900 match 25367\n",
      ">>> step 40000\n",
      "step 39900-39999, precision 0.657582, recall 0.953905, f_score 0.778499\n",
      "=== total 40000 match 25428\n",
      ">>> step 40100\n",
      "step 40000-40099, precision 0.657972, recall 0.953938, f_score 0.778784\n",
      "=== total 40100 match 25495\n",
      ">>> step 40200\n",
      "step 40100-40199, precision 0.657654, recall 0.954022, f_score 0.778588\n",
      "=== total 40200 match 25556\n",
      ">>> step 40300\n",
      "step 40200-40299, precision 0.657533, recall 0.954015, f_score 0.778502\n",
      "=== total 40300 match 25620\n",
      ">>> step 40400\n",
      "step 40300-40399, precision 0.657490, recall 0.954063, f_score 0.778487\n",
      "=== total 40400 match 25681\n",
      ">>> step 40500\n",
      "step 40400-40499, precision 0.657539, recall 0.953961, f_score 0.778488\n",
      "=== total 40500 match 25746\n",
      ">>> step 40600\n",
      "step 40500-40599, precision 0.657163, recall 0.953891, f_score 0.778201\n",
      "=== total 40600 match 25814\n",
      ">>> step 40700\n",
      "step 40600-40699, precision 0.657378, recall 0.953901, f_score 0.778355\n",
      "=== total 40700 match 25874\n",
      ">>> step 40800\n",
      "step 40700-40799, precision 0.657415, recall 0.954017, f_score 0.778419\n",
      "=== total 40800 match 25941\n",
      ">>> step 40900\n",
      "step 40800-40899, precision 0.657503, recall 0.954084, f_score 0.778504\n",
      "=== total 40900 match 26009\n",
      ">>> step 41000\n",
      "step 40900-40999, precision 0.657280, recall 0.954057, f_score 0.778338\n",
      "=== total 41000 match 26065\n",
      ">>> step 41100\n",
      "step 41000-41099, precision 0.657265, recall 0.954061, f_score 0.778329\n",
      "=== total 41100 match 26131\n",
      ">>> step 41200\n",
      "step 41100-41199, precision 0.657236, recall 0.954157, f_score 0.778340\n",
      "=== total 41200 match 26190\n",
      ">>> step 41300\n",
      "step 41200-41299, precision 0.657321, recall 0.954214, f_score 0.778419\n",
      "=== total 41300 match 26252\n",
      ">>> step 41400\n",
      "step 41300-41399, precision 0.657395, recall 0.954220, f_score 0.778473\n",
      "=== total 41400 match 26316\n",
      ">>> step 41500\n",
      "step 41400-41499, precision 0.657496, recall 0.954293, f_score 0.778568\n",
      "=== total 41500 match 26388\n",
      ">>> step 41600\n",
      "step 41500-41599, precision 0.657551, recall 0.954266, f_score 0.778598\n",
      "=== total 41600 match 26401\n",
      "TOTAL 41600, precision 0.657551, recall 0.954266, f_score 0.778598\n",
      "TOTAL true = 18192\n",
      "TOTAL error rate = 0.237332\n"
     ]
    }
   ],
   "source": [
    "# RE-TEST3 - take 1 piece and cross- validate on this (uncomment all for full test run)\n",
    "train_imgs, train_lbls = \\\n",
    "    load_train_from_disk(ROOT_FOLDER + \"train_concats3/\")\n",
    "tf.reset_default_graph()\n",
    "model_tf_deep(250, 1)    \n",
    "validate2_for_cross_validation(train_imgs, train_lbls, ROOT_FOLDER + \"model_binary/tear_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(x[1] == 1 for x in train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.78\n",
    "step 99, training accuracy 0.84\n",
    "step 149, training accuracy 0.9\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.84\n",
    "step 349, training accuracy 0.92\n",
    "step 399, training accuracy 0.86\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.86\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "step 49, training accuracy 0.84\n",
    "step 99, training accuracy 0.94\n",
    "step 149, training accuracy 0.86\n",
    "step 199, training accuracy 0.92\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.96\n",
    "step 399, training accuracy 0.96\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.96\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "WARNING:tensorflow:From /home/il239838/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
    "Instructions for updating:\n",
    "Use `tf.global_variables_initializer` instead.\n",
    "step 49, training accuracy 0.82\n",
    "step 99, training accuracy 0.92\n",
    "step 149, training accuracy 0.72\n",
    "step 199, training accuracy 0.8\n",
    "step 249, training accuracy 0.88\n",
    "step 299, training accuracy 0.88\n",
    "step 349, training accuracy 0.94\n",
    "step 399, training accuracy 0.84\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.84\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.76\n",
    "step 99, training accuracy 0.82\n",
    "step 149, training accuracy 0.96\n",
    "step 199, training accuracy 0.86\n",
    "step 249, training accuracy 0.76\n",
    "step 299, training accuracy 0.82\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.88\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.88\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.88\n",
    "step 149, training accuracy 0.88\n",
    "step 199, training accuracy 0.88\n",
    "step 249, training accuracy 0.92\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.86\n",
    "step 399, training accuracy 0.94\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.94\n",
    "Model saved in file: /Users/il239838/Downloads/private/Thesis/Papyrus/model_binary_new_X6/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    "deeper network on GCP\n",
    "#####################################################################\n",
    "TRAINING:\n",
    "MODEL:/home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "INFO:tensorflow:Restoring parameters from /home/il239838/files/model_binary/tear_model1.ckpt\n",
    "Model restored.\n",
    "step 49, training accuracy 0.88\n",
    "step 99, training accuracy 0.96\n",
    "step 149, training accuracy 0.84\n",
    "step 199, training accuracy 0.84\n",
    "step 249, training accuracy 0.84\n",
    "step 299, training accuracy 0.92\n",
    "step 349, training accuracy 0.98\n",
    "step 399, training accuracy 0.9\n",
    "Optimization Finished!\n",
    "step 399, training accuracy 0.9\n",
    "Model saved in file: /home/il239838/files/model_binary/tear_model2.ckpt\n",
    "#####################################################################\n",
    "TRAINING ENDED\n",
    "#####################################################################\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    ">>> step 100\n",
    "step 0-99, precision 0.695652, recall 0.640000, f_score 0.666667\n",
    ">>> step 200\n",
    "step 100-199, precision 0.512500, recall 0.732143, f_score 0.602941\n",
    ">>> step 300\n",
    "step 200-299, precision 0.350427, recall 0.732143, f_score 0.473988\n",
    ">>> step 400\n",
    "step 300-399, precision 0.408805, recall 0.677083, f_score 0.509804\n",
    ">>> step 500\n",
    "step 400-499, precision 0.366834, recall 0.651786, f_score 0.469453\n",
    ">>> step 600\n",
    "step 500-599, precision 0.349282, recall 0.651786, f_score 0.454829\n",
    ">>> step 700\n",
    "step 600-699, precision 0.376471, recall 0.662069, f_score 0.480000\n",
    ">>> step 800\n",
    "step 700-799, precision 0.383142, recall 0.595238, f_score 0.466200\n",
    ">>> step 900\n",
    "step 800-899, precision 0.394649, recall 0.617801, f_score 0.481633\n",
    ">>> step 1000\n",
    "step 900-999, precision 0.407821, recall 0.651786, f_score 0.501718\n",
    ">>> step 1100\n",
    "step 1000-1099, precision 0.404432, recall 0.651786, f_score 0.499145\n",
    ">>> step 1200\n",
    "step 1100-1199, precision 0.423559, recall 0.657588, f_score 0.515244\n",
    ">>> step 1300\n",
    "step 1200-1299, precision 0.408983, recall 0.662835, f_score 0.505848\n",
    ">>> step 1400\n",
    "step 1300-1399, precision 0.405640, recall 0.667857, f_score 0.504723\n",
    ">>> step 1500\n",
    "step 1400-1499, precision 0.415020, recall 0.670927, f_score 0.512821\n",
    ">>> step 1600\n",
    "step 1500-1599, precision 0.402852, recall 0.672619, f_score 0.503902\n",
    ">>> step 1700\n",
    "step 1600-1699, precision 0.401681, recall 0.647696, f_score 0.495851\n",
    ">>> step 1800\n",
    "step 1700-1799, precision 0.387987, recall 0.647696, f_score 0.485279\n",
    ">>> step 1900\n",
    "step 1800-1899, precision 0.385484, recall 0.647696, f_score 0.483316\n",
    ">>> step 2000\n",
    "step 1900-1999, precision 0.388802, recall 0.647668, f_score 0.485909\n",
    ">>> step 2100\n",
    "step 2000-2099, precision 0.379161, recall 0.650124, f_score 0.478976\n",
    ">>> step 2200\n",
    "step 2100-2199, precision 0.374286, recall 0.650124, f_score 0.475068\n",
    ">>> step 2300\n",
    "step 2200-2299, precision 0.381868, recall 0.651054, f_score 0.481385\n",
    ">>> step 2400\n",
    "step 2300-2399, precision 0.392622, recall 0.659292, f_score 0.492155\n",
    ">>> step 2500\n",
    "step 2400-2499, precision 0.393604, recall 0.665281, f_score 0.494590\n",
    ">>> step 2600\n",
    "step 2500-2599, precision 0.392601, recall 0.664646, f_score 0.493623\n",
    ">>> step 2700\n",
    "step 2600-2699, precision 0.389810, recall 0.664646, f_score 0.491412\n",
    ">>> step 2800\n",
    "step 2700-2799, precision 0.384884, recall 0.655446, f_score 0.484982\n",
    ">>> step 2900\n",
    "step 2800-2899, precision 0.390572, recall 0.664122, f_score 0.491873\n",
    ">>> step 3000\n",
    "step 2900-2999, precision 0.401064, recall 0.679279, f_score 0.504348\n",
    ">>> step 3100\n",
    "step 3000-3099, precision 0.418534, recall 0.695431, f_score 0.522568\n",
    ">>> step 3200\n",
    "step 3100-3199, precision 0.430129, recall 0.704545, f_score 0.534154\n",
    ">>> step 3300\n",
    "step 3200-3299, precision 0.446036, recall 0.716258, f_score 0.549735\n",
    ">>> step 3400\n",
    "step 3300-3399, precision 0.448563, recall 0.722388, f_score 0.553459\n",
    ">>> step 3500\n",
    "step 3400-3499, precision 0.455204, recall 0.719599, f_score 0.557650\n",
    ">>> step 3600\n",
    "step 3500-3599, precision 0.456560, recall 0.718271, f_score 0.558266\n",
    ">>> step 3700\n",
    "step 3600-3699, precision 0.457841, recall 0.724000, f_score 0.560950\n",
    ">>> step 3800\n",
    "step 3700-3799, precision 0.455285, recall 0.720721, f_score 0.558047\n",
    ">>> step 3900\n",
    "step 3800-3899, precision 0.459969, recall 0.724351, f_score 0.562650\n",
    ">>> step 4000\n",
    "step 3900-3999, precision 0.445289, recall 0.724351, f_score 0.551529\n",
    ">>> step 4100\n",
    "step 4000-4099, precision 0.442598, recall 0.724351, f_score 0.549461\n",
    ">>> step 4200\n",
    "step 4100-4199, precision 0.445596, recall 0.725301, f_score 0.552040\n",
    ">>> step 4300\n",
    "step 4200-4299, precision 0.429793, recall 0.724760, f_score 0.539597\n",
    ">>> step 4400\n",
    "step 4300-4399, precision 0.426450, recall 0.724760, f_score 0.536955\n",
    ">>> step 4500\n",
    "step 4400-4499, precision 0.427491, recall 0.727485, f_score 0.538528\n",
    ">>> step 4600\n",
    "step 4500-4599, precision 0.427310, recall 0.723820, f_score 0.537377\n",
    ">>> step 4700\n",
    "step 4600-4699, precision 0.422999, recall 0.723820, f_score 0.533956\n",
    ">>> step 4800\n",
    "step 4700-4799, precision 0.419893, recall 0.723820, f_score 0.531474\n",
    ">>> step 4900\n",
    "step 4800-4899, precision 0.423740, recall 0.723669, f_score 0.534504\n",
    ">>> step 5000\n",
    "step 4900-4999, precision 0.428387, recall 0.725683, f_score 0.538742\n",
    ">>> step 5100\n",
    "step 5000-5099, precision 0.417348, recall 0.725683, f_score 0.529928\n",
    ">>> step 5200\n",
    "step 5100-5199, precision 0.414482, recall 0.725683, f_score 0.527612\n",
    ">>> step 5300\n",
    "step 5200-5299, precision 0.412935, recall 0.725683, f_score 0.526358\n",
    ">>> step 5400\n",
    "step 5300-5399, precision 0.418093, recall 0.720759, f_score 0.529207\n",
    ">>> step 5500\n",
    "step 5400-5499, precision 0.409934, recall 0.720294, f_score 0.522502\n",
    ">>> step 5600\n",
    "step 5500-5599, precision 0.409280, recall 0.719665, f_score 0.521805\n",
    ">>> step 5700\n",
    "step 5600-5699, precision 0.411176, recall 0.716189, f_score 0.522422\n",
    ">>> step 5800\n",
    "step 5700-5799, precision 0.413056, recall 0.707921, f_score 0.521707\n",
    ">>> step 5900\n",
    "step 5800-5899, precision 0.408427, recall 0.701061, f_score 0.516152\n",
    ">>> step 6000\n",
    "step 5900-5999, precision 0.404113, recall 0.701061, f_score 0.512694\n",
    ">>> step 6100\n",
    "step 6000-6099, precision 0.399565, recall 0.700382, f_score 0.508839\n",
    ">>> step 6200\n",
    "step 6100-6199, precision 0.398913, recall 0.700382, f_score 0.508310\n",
    ">>> step 6300\n",
    "step 6200-6299, precision 0.402681, recall 0.704503, f_score 0.512453\n",
    ">>> step 6400\n",
    "step 6300-6399, precision 0.404178, recall 0.709441, f_score 0.514970\n",
    ">>> step 6500\n",
    "step 6400-6499, precision 0.402955, recall 0.713255, f_score 0.514974\n",
    ">>> step 6600\n",
    "step 6500-6599, precision 0.402532, recall 0.713645, f_score 0.514730\n",
    ">>> step 6700\n",
    "step 6600-6699, precision 0.413011, recall 0.712585, f_score 0.522933\n",
    ">>> step 6800\n",
    "step 6700-6799, precision 0.416506, recall 0.713813, f_score 0.526059\n",
    ">>> step 6900\n",
    "step 6800-6899, precision 0.421002, recall 0.712439, f_score 0.529253\n",
    ">>> step 7000\n",
    "step 6900-6999, precision 0.428773, recall 0.709176, f_score 0.534427\n",
    ">>> step 7100\n",
    "step 7000-7099, precision 0.430497, recall 0.704718, f_score 0.534488\n",
    ">>> step 7200\n",
    "step 7100-7199, precision 0.437929, recall 0.695273, f_score 0.537381\n",
    ">>> step 7300\n",
    "step 7200-7299, precision 0.440254, recall 0.693133, f_score 0.538483\n",
    ">>> step 7400\n",
    "step 7300-7399, precision 0.441348, recall 0.690577, f_score 0.538525\n",
    ">>> step 7500\n",
    "step 7400-7499, precision 0.439821, recall 0.688375, f_score 0.536719\n",
    ">>> step 7600\n",
    "step 7500-7599, precision 0.440693, recall 0.683196, f_score 0.535782\n",
    ">>> step 7700\n",
    "step 7600-7699, precision 0.440618, recall 0.676152, f_score 0.533547\n",
    ">>> step 7800\n",
    "step 7700-7799, precision 0.435618, recall 0.676152, f_score 0.529865\n",
    ">>> step 7900\n",
    "step 7800-7899, precision 0.431701, recall 0.674044, f_score 0.526316\n",
    ">>> step 8000\n",
    "step 7900-7999, precision 0.429482, recall 0.668651, f_score 0.523021\n",
    ">>> step 8100\n",
    "step 8000-8099, precision 0.428027, recall 0.667768, f_score 0.521672\n",
    ">>> step 8200\n",
    "step 8100-8199, precision 0.429717, recall 0.664057, f_score 0.521784\n",
    ">>> step 8300\n",
    "step 8200-8299, precision 0.428451, recall 0.664057, f_score 0.520849\n",
    ">>> step 8400\n",
    "step 8300-8399, precision 0.429589, recall 0.660438, f_score 0.520569\n",
    ">>> step 8500\n",
    "step 8400-8499, precision 0.428870, recall 0.660438, f_score 0.520041\n",
    ">>> step 8600\n",
    "step 8500-8599, precision 0.427618, recall 0.660438, f_score 0.519119\n",
    ">>> step 8700\n",
    "step 8600-8699, precision 0.424528, recall 0.655478, f_score 0.515310\n",
    ">>> step 8800\n",
    "step 8700-8799, precision 0.421074, recall 0.655478, f_score 0.512757\n",
    ">>> step 8900\n",
    "step 8800-8899, precision 0.420032, recall 0.650814, f_score 0.510555\n",
    ">>> step 9000\n",
    "step 8900-8999, precision 0.415667, recall 0.650814, f_score 0.507317\n",
    ">>> step 9100\n",
    "step 9000-9099, precision 0.414201, recall 0.646154, f_score 0.504808\n",
    ">>> step 9200\n",
    "step 9100-9199, precision 0.414510, recall 0.642944, f_score 0.504053\n",
    ">>> step 9300\n",
    "step 9200-9299, precision 0.413793, recall 0.639138, f_score 0.502352\n",
    ">>> step 9400\n",
    "step 9300-9399, precision 0.408101, recall 0.639138, f_score 0.498134\n",
    ">>> step 9500\n",
    "step 9400-9499, precision 0.407789, recall 0.639138, f_score 0.497902\n",
    ">>> step 9600\n",
    "step 9500-9599, precision 0.408248, recall 0.639218, f_score 0.498268\n",
    ">>> step 9700\n",
    "step 9600-9699, precision 0.406097, recall 0.639218, f_score 0.496663\n",
    ">>> step 9800\n",
    "step 9700-9799, precision 0.408482, recall 0.639487, f_score 0.498524\n",
    ">>> step 9900\n",
    "step 9800-9899, precision 0.407298, recall 0.639098, f_score 0.497524\n",
    ">>> step 10000\n",
    "step 9900-9999, precision 0.406250, recall 0.639098, f_score 0.496741\n",
    ">>> step 10100\n",
    "step 10000-10099, precision 0.407407, recall 0.636312, f_score 0.496758\n",
    ">>> step 10200\n",
    "step 10100-10199, precision 0.408892, recall 0.636415, f_score 0.497892\n",
    ">>> step 10300\n",
    "step 10200-10299, precision 0.407852, recall 0.636415, f_score 0.497120\n",
    ">>> step 10400\n",
    "step 10300-10399, precision 0.408796, recall 0.637079, f_score 0.498024\n",
    ">>> step 10500\n",
    "step 10400-10499, precision 0.406452, recall 0.637079, f_score 0.496280\n",
    ">>> step 10600\n",
    "step 10500-10599, precision 0.405540, recall 0.637989, f_score 0.495875\n",
    ">>> step 10700\n",
    "step 10600-10699, precision 0.403583, recall 0.636918, f_score 0.494087\n",
    ">>> step 10800\n",
    "step 10700-10799, precision 0.401748, recall 0.636918, f_score 0.492710\n",
    ">>> step 10900\n",
    "step 10800-10899, precision 0.402490, recall 0.636066, f_score 0.493011\n",
    ">>> step 11000\n",
    "step 10900-10999, precision 0.400619, recall 0.633496, f_score 0.490836\n",
    ">>> step 11100\n",
    "step 11000-11099, precision 0.396798, recall 0.633496, f_score 0.487958\n",
    ">>> step 11200\n",
    "step 11100-11199, precision 0.396944, recall 0.629510, f_score 0.486880\n",
    ">>> step 11300\n",
    "step 11200-11299, precision 0.397928, recall 0.628496, f_score 0.487316\n",
    ">>> step 11400\n",
    "step 11300-11399, precision 0.398474, recall 0.625521, f_score 0.486826\n",
    ">>> step 11500\n",
    "step 11400-11499, precision 0.397233, recall 0.620690, f_score 0.484435\n",
    ">>> step 11600\n",
    "step 11500-11599, precision 0.396189, recall 0.620690, f_score 0.483658\n",
    ">>> step 11700\n",
    "step 11600-11699, precision 0.397906, recall 0.620725, f_score 0.484945\n",
    ">>> step 11800\n",
    "step 11700-11799, precision 0.397856, recall 0.618999, f_score 0.484381\n",
    ">>> step 11900\n",
    "step 11800-11899, precision 0.396184, recall 0.618999, f_score 0.483139\n",
    ">>> step 12000\n",
    "step 11900-11999, precision 0.394779, recall 0.618999, f_score 0.482094\n",
    ">>> step 12100\n",
    "step 12000-12099, precision 0.397894, recall 0.619473, f_score 0.484554\n",
    ">>> step 12200\n",
    "step 12100-12199, precision 0.402704, recall 0.621242, f_score 0.488652\n",
    ">>> step 12300\n",
    "step 12200-12299, precision 0.408271, recall 0.618464, f_score 0.491852\n",
    ">>> step 12400\n",
    "step 12300-12399, precision 0.411149, recall 0.615207, f_score 0.492893\n",
    ">>> step 12500\n",
    "step 12400-12499, precision 0.414962, recall 0.612991, f_score 0.494902\n",
    ">>> step 12600\n",
    "step 12500-12599, precision 0.416262, recall 0.606275, f_score 0.493614\n",
    ">>> step 12700\n",
    "step 12600-12699, precision 0.419520, recall 0.603456, f_score 0.494951\n",
    ">>> step 12800\n",
    "step 12700-12799, precision 0.418013, recall 0.603456, f_score 0.493901\n",
    ">>> step 12900\n",
    "step 12800-12899, precision 0.417014, recall 0.600943, f_score 0.492362\n",
    ">>> step 13000\n",
    "step 12900-12999, precision 0.412596, recall 0.600943, f_score 0.489269\n",
    ">>> step 13100\n",
    "step 13000-13099, precision 0.410480, recall 0.599745, f_score 0.487383\n",
    ">>> step 13200\n",
    "step 13100-13199, precision 0.408577, recall 0.599745, f_score 0.486039\n",
    ">>> step 13300\n",
    "step 13200-13299, precision 0.407397, recall 0.599745, f_score 0.485203\n",
    ">>> step 13400\n",
    "step 13300-13399, precision 0.406178, recall 0.599409, f_score 0.484228\n",
    ">>> step 13500\n",
    "step 13400-13499, precision 0.401925, recall 0.599409, f_score 0.481193\n",
    ">>> step 13600\n",
    "step 13500-13599, precision 0.401584, recall 0.599409, f_score 0.480948\n",
    ">>> step 13700\n",
    "step 13600-13699, precision 0.400391, recall 0.600335, f_score 0.480389\n",
    ">>> step 13800\n",
    "step 13700-13799, precision 0.399224, recall 0.598753, f_score 0.479042\n",
    ">>> step 13900\n",
    "step 13800-13899, precision 0.395553, recall 0.598919, f_score 0.476442\n",
    ">>> step 14000\n",
    "step 13900-13999, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14100\n",
    "step 14000-14099, precision 0.393915, recall 0.598432, f_score 0.475098\n",
    ">>> step 14200\n",
    "step 14100-14199, precision 0.392741, recall 0.598432, f_score 0.474244\n",
    ">>> step 14300\n",
    "step 14200-14299, precision 0.393562, recall 0.597536, f_score 0.474560\n",
    ">>> step 14400\n",
    "step 14300-14399, precision 0.390675, recall 0.594374, f_score 0.471463\n",
    ">>> step 14500\n",
    "step 14400-14499, precision 0.389944, recall 0.592924, f_score 0.470474\n",
    ">>> step 14600\n",
    "step 14500-14599, precision 0.386943, recall 0.592924, f_score 0.468283\n",
    ">>> step 14700\n",
    "step 14600-14699, precision 0.386532, recall 0.592924, f_score 0.467983\n",
    ">>> step 14800\n",
    "step 14700-14799, precision 0.387326, recall 0.595152, f_score 0.469258\n",
    ">>> step 14900\n",
    "step 14800-14899, precision 0.386903, recall 0.595343, f_score 0.469007\n",
    ">>> step 15000\n",
    "step 14900-14999, precision 0.388658, recall 0.595694, f_score 0.470403\n",
    ">>> step 15100\n",
    "step 15000-15099, precision 0.387264, recall 0.595304, f_score 0.469260\n",
    ">>> step 15200\n",
    "step 15100-15199, precision 0.386923, recall 0.596679, f_score 0.469435\n",
    ">>> step 15300\n",
    "step 15200-15299, precision 0.387583, recall 0.597950, f_score 0.470315\n",
    ">>> step 15400\n",
    "step 15300-15399, precision 0.384228, recall 0.599214, f_score 0.468222\n",
    ">>> step 15500\n",
    "step 15400-15499, precision 0.383070, recall 0.599214, f_score 0.467361\n",
    ">>> step 15600\n",
    "step 15500-15599, precision 0.384500, recall 0.599143, f_score 0.468403\n",
    ">>> step 15700\n",
    "step 15600-15699, precision 0.386607, recall 0.601239, f_score 0.470606\n",
    ">>> step 15800\n",
    "step 15700-15799, precision 0.382607, recall 0.601239, f_score 0.467630\n",
    ">>> step 15900\n",
    "step 15800-15899, precision 0.382626, recall 0.603309, f_score 0.468269\n",
    ">>> step 16000\n",
    "step 15900-15999, precision 0.383248, recall 0.603296, f_score 0.468731\n",
    ">>> step 16100\n",
    "step 16000-16099, precision 0.382317, recall 0.603296, f_score 0.468034\n",
    ">>> step 16200\n",
    "step 16100-16199, precision 0.381946, recall 0.603296, f_score 0.467756\n",
    ">>> step 16300\n",
    "step 16200-16299, precision 0.379460, recall 0.603296, f_score 0.465887\n",
    ">>> step 16400\n",
    "step 16300-16399, precision 0.382472, recall 0.604669, f_score 0.468563\n",
    ">>> step 16500\n",
    "step 16400-16499, precision 0.385124, recall 0.605644, f_score 0.470843\n",
    ">>> step 16600\n",
    "step 16500-16599, precision 0.386226, recall 0.609174, f_score 0.472732\n",
    ">>> step 16700\n",
    "step 16600-16699, precision 0.385509, recall 0.609174, f_score 0.472195\n",
    ">>> step 16800\n",
    "step 16700-16799, precision 0.387558, recall 0.609641, f_score 0.473870\n",
    ">>> step 16900\n",
    "step 16800-16899, precision 0.389587, recall 0.611689, f_score 0.476004\n",
    ">>> step 17000\n",
    "step 16900-16999, precision 0.392534, recall 0.612858, f_score 0.478555\n",
    ">>> step 17100\n",
    "step 17000-17099, precision 0.393763, recall 0.611711, f_score 0.479115\n",
    ">>> step 17200\n",
    "step 17100-17199, precision 0.392442, recall 0.611711, f_score 0.478136\n",
    ">>> step 17300\n",
    "step 17200-17299, precision 0.394895, recall 0.614296, f_score 0.480746\n",
    ">>> step 17400\n",
    "step 17300-17399, precision 0.397101, recall 0.616644, f_score 0.483100\n",
    ">>> step 17500\n",
    "step 17400-17499, precision 0.401132, recall 0.617085, f_score 0.486208\n",
    ">>> step 17600\n",
    "step 17500-17599, precision 0.399220, recall 0.617085, f_score 0.484801\n",
    ">>> step 17700\n",
    "step 17600-17699, precision 0.400259, recall 0.617539, f_score 0.485707\n",
    ">>> step 17800\n",
    "step 17700-17799, precision 0.402402, recall 0.618734, f_score 0.487653\n",
    ">>> step 17900\n",
    "step 17800-17899, precision 0.403036, recall 0.617830, f_score 0.487836\n",
    ">>> step 18000\n",
    "step 17900-17999, precision 0.404838, recall 0.618076, f_score 0.489231\n",
    ">>> step 18100\n",
    "step 18000-18099, precision 0.407634, recall 0.618758, f_score 0.491482\n",
    ">>> step 18200\n",
    "step 18100-18199, precision 0.406473, recall 0.618682, f_score 0.490614\n",
    ">>> step 18300\n",
    "step 18200-18299, precision 0.407004, recall 0.619139, f_score 0.491144\n",
    ">>> step 18400\n",
    "step 18300-18399, precision 0.406982, recall 0.619472, f_score 0.491232\n",
    ">>> step 18500\n",
    "step 18400-18499, precision 0.405737, recall 0.619093, f_score 0.490206\n",
    ">>> step 18600\n",
    "step 18500-18599, precision 0.404076, recall 0.620025, f_score 0.489282\n",
    ">>> step 18700\n",
    "step 18600-18699, precision 0.401760, recall 0.620025, f_score 0.487581\n",
    ">>> step 18800\n",
    "step 18700-18799, precision 0.399308, recall 0.620025, f_score 0.485771\n",
    ">>> step 18900\n",
    "step 18800-18899, precision 0.398062, recall 0.620321, f_score 0.484938\n",
    ">>> step 19000\n",
    "step 18900-18999, precision 0.396462, recall 0.620321, f_score 0.483748\n",
    ">>> step 19100\n",
    "step 19000-19099, precision 0.396994, recall 0.620614, f_score 0.484234\n",
    ">>> step 19200\n",
    "step 19100-19199, precision 0.395224, recall 0.620431, f_score 0.482859\n",
    ">>> step 19300\n",
    "step 19200-19299, precision 0.393585, recall 0.620281, f_score 0.481589\n",
    ">>> step 19400\n",
    "step 19300-19399, precision 0.393508, recall 0.620281, f_score 0.481531\n",
    ">>> step 19500\n",
    "step 19400-19499, precision 0.393243, recall 0.621160, f_score 0.481597\n",
    ">>> step 19600\n",
    "step 19500-19599, precision 0.392549, recall 0.621160, f_score 0.481077\n",
    ">>> step 19700\n",
    "step 19600-19699, precision 0.392088, recall 0.621160, f_score 0.480730\n",
    ">>> step 19800\n",
    "step 19700-19799, precision 0.391704, recall 0.621160, f_score 0.480442\n",
    ">>> step 19900\n",
    "step 19800-19899, precision 0.391016, recall 0.621160, f_score 0.479923\n",
    ">>> step 20000\n",
    "step 19900-19999, precision 0.390358, recall 0.620519, f_score 0.479236\n",
    ">>> step 20100\n",
    "step 20000-20099, precision 0.390504, recall 0.620764, f_score 0.479419\n",
    ">>> step 20200\n",
    "step 20100-20199, precision 0.386887, recall 0.621114, f_score 0.476787\n",
    ">>> step 20300\n",
    "step 20200-20299, precision 0.384308, recall 0.621114, f_score 0.474824\n",
    ">>> step 20400\n",
    "step 20300-20399, precision 0.384161, recall 0.621114, f_score 0.474712\n",
    ">>> step 20500\n",
    "step 20400-20499, precision 0.383577, recall 0.621114, f_score 0.474266\n",
    ">>> step 20600\n",
    "step 20500-20599, precision 0.383660, recall 0.622861, f_score 0.474837\n",
    ">>> step 20700\n",
    "step 20600-20699, precision 0.383569, recall 0.624886, f_score 0.475355\n",
    ">>> step 20800\n",
    "step 20700-20799, precision 0.382785, recall 0.624886, f_score 0.474752\n",
    ">>> step 20900\n",
    "step 20800-20899, precision 0.384644, recall 0.627704, f_score 0.476995\n",
    ">>> step 21000\n",
    "step 20900-20999, precision 0.383092, recall 0.627704, f_score 0.475800\n",
    ">>> step 21100\n",
    "step 21000-21099, precision 0.383464, recall 0.626977, f_score 0.475878\n",
    ">>> step 21200\n",
    "step 21100-21199, precision 0.383045, recall 0.626977, f_score 0.475555\n",
    ">>> step 21300\n",
    "step 21200-21299, precision 0.383430, recall 0.626852, f_score 0.475816\n",
    ">>> step 21400\n",
    "step 21300-21399, precision 0.384685, recall 0.628119, f_score 0.477146\n",
    ">>> step 21500\n",
    "step 21400-21499, precision 0.383581, recall 0.628119, f_score 0.476296\n",
    ">>> step 21600\n",
    "step 21500-21599, precision 0.384506, recall 0.629446, f_score 0.477391\n",
    ">>> step 21700\n",
    "step 21600-21699, precision 0.382462, recall 0.629446, f_score 0.475813\n",
    ">>> step 21800\n",
    "step 21700-21799, precision 0.382716, recall 0.628439, f_score 0.475721\n",
    ">>> step 21900\n",
    "step 21800-21899, precision 0.380835, recall 0.628439, f_score 0.474265\n",
    ">>> step 22000\n",
    "step 21900-21999, precision 0.380235, recall 0.628439, f_score 0.473799\n",
    ">>> step 22100\n",
    "step 22000-22099, precision 0.380420, recall 0.629084, f_score 0.474126\n",
    ">>> step 22200\n",
    "step 22100-22199, precision 0.378838, recall 0.629758, f_score 0.473086\n",
    ">>> step 22300\n",
    "step 22200-22299, precision 0.378196, recall 0.628481, f_score 0.472225\n",
    ">>> step 22400\n",
    "step 22300-22399, precision 0.377388, recall 0.627647, f_score 0.471359\n",
    ">>> step 22500\n",
    "step 22400-22499, precision 0.377378, recall 0.626815, f_score 0.471117\n",
    ">>> step 22600\n",
    "step 22500-22599, precision 0.377937, recall 0.628183, f_score 0.471939\n",
    ">>> step 22700\n",
    "step 22600-22699, precision 0.376335, recall 0.628183, f_score 0.470688\n",
    ">>> step 22800\n",
    "step 22700-22799, precision 0.376016, recall 0.628183, f_score 0.470439\n",
    ">>> step 22900\n",
    "step 22800-22899, precision 0.375968, recall 0.629473, f_score 0.470762\n",
    ">>> step 23000\n",
    "step 22900-22999, precision 0.375210, recall 0.629473, f_score 0.470167\n",
    ">>> step 23100\n",
    "step 23000-23099, precision 0.374708, recall 0.630191, f_score 0.469973\n",
    ">>> step 23200\n",
    "step 23100-23199, precision 0.376144, recall 0.631564, f_score 0.471484\n",
    ">>> step 23300\n",
    "step 23200-23299, precision 0.376630, recall 0.632834, f_score 0.472219\n",
    ">>> step 23400\n",
    "step 23300-23399, precision 0.375021, recall 0.632834, f_score 0.470952\n",
    ">>> step 23500\n",
    "step 23400-23499, precision 0.374286, recall 0.633527, f_score 0.470564\n",
    ">>> step 23600\n",
    "step 23500-23599, precision 0.374068, recall 0.634763, f_score 0.470732\n",
    ">>> step 23700\n",
    "step 23600-23699, precision 0.373814, recall 0.635494, f_score 0.470731\n",
    ">>> step 23800\n",
    "step 23700-23799, precision 0.372496, recall 0.635494, f_score 0.469685\n",
    ">>> step 23900\n",
    "step 23800-23899, precision 0.372906, recall 0.635918, f_score 0.470127\n",
    ">>> step 24000\n",
    "step 23900-23999, precision 0.372976, recall 0.634865, f_score 0.469894\n",
    ">>> step 24100\n",
    "step 24000-24099, precision 0.373713, recall 0.634750, f_score 0.470448\n",
    ">>> step 24200\n",
    "step 24100-24199, precision 0.373123, recall 0.634750, f_score 0.469979\n",
    ">>> step 24300\n",
    "step 24200-24299, precision 0.374528, recall 0.635369, f_score 0.471263\n",
    ">>> step 24400\n",
    "step 24300-24399, precision 0.374765, recall 0.635397, f_score 0.471458"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
